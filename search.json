[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lista de datasets",
    "section": "",
    "text": "Un dataset es un tabla de datos plana con observaciones por filas y variables por columnas. Los datasets son recursos educativos para universitarios muy útiles para practicar técnicas multivariantes o de series temporales. Así por ejemplo, sin necesidad de acudir a los microdatos, se pueden obtener datasets muy interesantes cruzando estadísticas de población, empleo y contabilidad nacional por Comunidades Autónomas.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Título\n        \n         \n          Tema\n        \n         \n          Número de observaciones\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    \n      \n        \n          Análisis Discriminante: laboral\n          Microdatos relativos a la encuesta de estructura salarial de las personas en 2018. Salario bruto, Edad y Antigüedad en el trabajo y días de vacaciones al año del encuestado con el fin de clasificar los estudios (o bien tienen pocos estudios o bien muchos) de una persona en función de las anteriores variables.\n          Técnica estadística: Análisis Discriminante\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta Anual de Estructura Salarial\n            \n            \n          \n          \n          Número de observaciones: 99782\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Análisis Discriminante: salud\n          Microdatos relativos a la encuesta de Salud a personas en 2017. Altura, Peso y Edad del encuestado con el fin de clasificar el sexo de una persona en función de las anteriores variables.\n          Técnica estadística: Análisis Discriminante\n          Temas: Salud\n          Operaciones:\n          \n            \n              Encuesta Nacional de Salud (ENSE)\n            \n            \n          \n          \n          Número de observaciones: 23089\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Cluster Jerárquico: VarCovid\n          Este dataset presenta un conjunto de datos de las Tasas de variación de fallecidos en 2020 respecto al año anterior. Destacar que que 2020 fue el año del COVID y 1Ola, 2Ola, 3Ola corresponden a las semanas en las que se dieron dichas olas.\n          Técnica estadística: Cluster Jerárquico\n          Temas: Salud\n          Operaciones:\n          \n            \n              Estimación de Defunciones Semanales\n            \n            \n          \n          \n          Número de observaciones: 20\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster Jerárquico: VarCovid_provincias\n          Datos por provincias del exceso de mortalidad durante las cinco primeras olas de COVID-19 en comparación con las personas fallecidas en el mismo periodo de los años 2017, 2018 y 2019, años previos a la pandemia, una estadística con carácter experimental.\n          Técnica estadística: Cluster Jerárquico\n          Temas: Salud\n          Operaciones:\n          \n            \n              Estimación de Defunciones Semanales\n            \n            \n          \n          \n          Número de observaciones: 53\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster Jerárquico: desigualdad_CCAA\n          Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18.\n          Técnica estadística: Cluster Jerárquico\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta Anual de Estructura Salarial (INE)\n            \n            \n          \n          \n          Número de observaciones: 17\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster Jerárquico: ecv_cluster\n          Datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021.\n          Técnica estadística: Cluster Jerárquico\n          Temas: Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Encuesta de Condiciones de Vida (ECV) (INE)\n            \n            \n          \n          \n          Número de observaciones: 19\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster K-Means: VarCovid\n          Este dataset presenta un conjunto de datos de las Tasas de variación de fallecidos en 2020 respecto al año anterior. Destacar que que 2020 fue el año del COVID y 1Ola, 2Ola, 3Ola corresponden a las semanas en las que se dieron dichas olas.\n          Técnica estadística: Cluster K-Means\n          Temas: Salud\n          Operaciones:\n          \n            \n              Estimación de Defunciones Semanales\n            \n            \n          \n          \n          Número de observaciones: 20\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster K-Means: desigualdad_CCAA\n          Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18.\n          Técnica estadística: Cluster K-Means\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta Anual de Estructura Salarial (INE)\n            \n            \n          \n          \n          Número de observaciones: 17\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster K-Means: ecv_cluster\n          Datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021.\n          Técnica estadística: Cluster K-Means\n          Temas: Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Encuesta de Condiciones de Vida (ECV) (INE)\n            \n            \n          \n          \n          Número de observaciones: 19\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Cluster K-Means: salario_kmeans\n          Este dataset presenta un conjunto de datos sobre el salario medio de los trabajadores y la población en las comunidades autónomas junto a Ceuta y Melilla, relativos al año 2018.\n          Técnica estadística: Cluster K-Means\n          Temas: Mercado laboral y salarios, Demografía y población\n          Operaciones:\n          \n            \n              Cifras Oficiales de Población de los Municipios Españoles\n            \n            ,\n          \n            \n              Encuesta Anual de Estructura Salarial\n            \n            \n          \n          \n          Número de observaciones: 18\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - Análisis Factorial: ECV_factorial\n          Este dataset presenta un conjunto de microdatos  relativos a la ECV donde se puntua el nivel de satisfacción en determinados ámbitos de la vida (laboral, económico, relaciones personales..). El objetivo es llevar a cabo un análisis factorial\n          Técnica estadística: Reducción Dimensionalidad - Análisis Factorial\n          Temas: Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Encuesta de Condiciones de Vida (ECV) (INE)\n            \n            \n          \n          \n          Número de observaciones: 26883\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - Análisis Factorial: pisa_factorial\n          Este dataset presenta un conjunto de microdatos  relativos a la prueba PISA  que trata de medir la existencia de variables socioeconómicas, metacognitivas, motivacionales e, incluso, emocionales, que pueden tener impacto sobre el rendimiento académico. El objetivo es llevar a cabo un análisis factorial.\n          Técnica estadística: Reducción Dimensionalidad - Análisis Factorial\n          Temas: Educación y formación\n          Operaciones:\n          \n            \n              Ministerio Educación\n            \n            \n          \n          \n          Número de observaciones: 35943\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - PCA: IMCV_pca\n          Datos por Comunidades Autónomas de las nueve dimensiones relativos a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV), una estadística con carácter experimental. Datos correspondientes al año 2020.\n          Técnica estadística: Reducción Dimensionalidad - PCA\n          Temas: Estadística Experimental\n          Operaciones:\n          \n            \n              Estadística experimental\n            \n            \n          \n          \n          Número de observaciones: 20\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - PCA: MercadoHipotecas\n          Este dataset presenta los datos de estudio del mercado de adquisición de viviendas en propiedad en todas las comunidades autónomas durante el ejercicio 2021, relacionando las transmisiones inmobiliarias con las hipotecas constituidas, los índices de precio de vivienda y otras variables económicas y sociodemográficas. \nOrientado a escalamiento multidimensional, técnicas de clusterización y análisis de componentes principales.\n          Técnica estadística: Reducción Dimensionalidad - PCA\n          Temas: Construcción y vivienda. Demografía y población. Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Censos de Población y Viviendas 2021  INE\n            \n            ,\n          \n            \n              Indicadores Demográficos Básicos\n            \n            ,\n          \n            \n              Tablas de mortalidad\n            \n            ,\n          \n            \n              Encuesta de Población Activa (EPA).\n            \n            ,\n          \n            \n              Estadística de Transmisiones de Derechos de la Propiedad\n            \n            ,\n          \n            \n              Estadística de Hipotecas\n            \n            ,\n          \n            \n              Índice de Precios de la Vivienda (IPV)\n            \n            \n          \n          \n          Número de observaciones: 19\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - PCA: Situacion_sanitaria\n          Datos para las Comunidades Autónomas, abordando sobre aspectos demográficos y de salud. Contiene variables como la tasa de natalidad y mortalidad, el índice de envejecimiento, tasas de profesionales de la salud (médicos y enfermeros) por cada 100,000 habitantes, el porcentaje de inaccesibilidad a medicamentos recetados por razones económicas y la tasa media de morbilidad hospitalaria debido a enfermedades.\n          Técnica estadística: Reducción Dimensionalidad - PCA\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Indicadores demográficos básicos\n            \n            ,\n          \n            \n              Estadística de profesionales sanitarios colegiados\n            \n            ,\n          \n            \n              Encuesta europea de salud en España\n            \n            \n          \n          \n          Número de observaciones: 17\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Reducción Dimensionalidad - PCA: provincias_variado\n          El conjunto de datos abarca información diversa sobre variables socioeconómicas en provincias y ciudades autónomas. Contiene datos como número de explotaciones agrícolas, superficies agrícolas, indicadores de empleo, ejecuciones hipotecarias, empresas por sector, PIB per cápita y datos demográficos. \nEste conjunto de variables puede ser útil para aplicar técnicas de reducir la dimensionalidad y entender las relaciones entre las diferentes variables.\n          Técnica estadística: Reducción Dimensionalidad - PCA\n          Temas: Agricultura, ganadería, selvicultura y caza. Construcción y vivienda. Demografía y población. Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Encuesta de Población Activa\n            \n            ,\n          \n            \n              Estadística sobre Ejecuciones Hipotecarias. (INE)\n            \n            ,\n          \n            \n              Explotación Estadística del Directorio Central de Empresas\n            \n            ,\n          \n            \n              Contabilidad Regional de España\n            \n            ,\n          \n            \n              Estadística de migraciones\n            \n            ,\n          \n            \n              Estadística del Padrón Continuo\n            \n            ,\n          \n            \n              Censo Agrario\n            \n            ,\n          \n            \n              Estadística de prestaciones por desempleo. (Ministerio de Trabajo y Economía Social)\n            \n            ,\n          \n            \n              Estadística de apoyo a la creación de empleo.(Ministerio de Trabajo y Economía Social)\n            \n            \n          \n          \n          Número de observaciones: 52\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Regresión Lineal: IMCV_reg\n          Datos por Comunidades Autónomas de ocho de las nueve dimensiones relativas a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV), una estadística con carácter experimental. Datos correspondientes al año 2020 y 2021 .\n          Técnica estadística: Regresión Lineal\n          Temas: Estadística Experimental\n          Operaciones:\n          \n            \n              Estadística experimental\n            \n            \n          \n          \n          Número de observaciones: 38\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Regresión Lineal: Sucidios\n          Este dataset presenta la tasa de suicidios que ocurre en España con distintas variables demográficas y económicas que pueden llegar a influir o no,  por Comunidades Autónomas.\n          Técnica estadística: Regresión Lineal\n          Temas: Salud\n          Operaciones:\n          \n            \n              Cifras Oficiales de Población de los Municipios Españoles: Revisión del Padrón Municipal\n            \n            ,\n          \n            \n              Encuesta de Población Activa (EPA)\n            \n            ,\n          \n            \n              Índice de Precios de Consumo\n            \n            ,\n          \n            \n              Estadística de Condenados: Menores (INE)\n            \n            ,\n          \n            \n              Contabilidad Regional de España\n            \n            ,\n          \n            \n              Estadísticas sobre Recogida y Tratamiento de Residuos\n            \n            ,\n          \n            \n              Estadística de Sociedades Mercantiles\n            \n            ,\n          \n            \n              Estadística de Defunciones según la Causa de Muerte\n            \n            ,\n          \n            \n              Estadística de Movimientos Turísticos en Frontera (FRONTUR)\n            \n            ,\n          \n            \n              Estadística de Violencia Doméstica y Violencia de Género\n            \n            ,\n          \n            \n              Estadística de Defunciones según la Causa de Muerte\n            \n            ,\n          \n            \n              Estadística del Procedimiento Concursal\n            \n            ,\n          \n            \n              (Ficha plan) Estadística sobre Actividades en ID\n            \n            ,\n          \n            \n              Encuesta de Presupuestos Familiares (EPF)\n            \n            ,\n          \n            \n              Estadísticas sobre las Actividades de Protección Medioambiental\n            \n            \n          \n          \n          Número de observaciones: 17\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Regresión Lineal: matrimonios_reg\n          Dataset con número de matrimonios en las ciudades españolas (de entre 50.000 y 300.000 habitantes), número total de habitantes, número de mujeres y número de nacimientos que ha habido ese año. Datos correspondientes a 2022.\n          Técnica estadística: Regresión Lineal\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Cifras Oficiales de Población de los Municipios Españoles: Revisión del Padrón Municipal\n            \n            ,\n          \n            \n              MNP: Estadística de Matrimonios\n            \n            ,\n          \n            \n              MNP: Estadística de Nacimientos\n            \n            \n          \n          \n          Número de observaciones: 133\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Regresión Lineal: salud_reg\n          Microdatos relativos a la encuesta de Salud a personas en 2017. Altura, Peso, Edad Sexo e IMC del encuestado con el fin de hacer una regresión del Peso en función del resto de variables y explicar por qué es interesante este estudio.\n          Técnica estadística: Regresión Lineal\n          Temas: Salud\n          Operaciones:\n          \n            \n              Encuesta Nacional de Salud (ENSE)\n            \n            \n          \n          \n          Número de observaciones: 22019\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Regresión Logística: ECV_microdatos\n          Microdatos extraidos de la  Encuesta de Condiciones de Vida (ECV) del año 2019. El objetivo es  estimar si la calidad de vida de una persona ha mejorado respecto a su infancia , de acuerdo con su año de nacimiento y su nivel de estudios.\n          Técnica estadística: Regresión Logística\n          Temas: Nivel, calidad y condiciones de vida\n          Operaciones:\n          \n            \n              Encuesta de Condiciones de Vida (ECV)\n            \n            \n          \n          \n          Número de observaciones: 17463\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Regresión Logística: Partos\n          Datos relativos a partos en 2022 en la Comunidad Autónoma de Navarra. El objetivo es clasificar los partos en si hubo o no una cesárea en función de la edad de la madre, las complicaciones en el parto y las semanas de gestación.\n          Técnica estadística: Regresión Logística\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Estadística de nacimientos. Movimiento natural de la población\n            \n            \n          \n          \n          Número de observaciones: 2809\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Regresión Logística: laboral\n          Microdatos relativos a la encuesta de estructura salarial de las personas en 2018. Salario bruto, Edad y Antigüedad en el trabajo y días de vacaciones al año del encuestado con el fin de clasificar los estudios (o bien tienen pocos estudios o bien muchos) de una persona en función de las anteriores variables.\n          Técnica estadística: Regresión Logística\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta Anual de Estructura Salarial\n            \n            \n          \n          \n          Número de observaciones: 99782\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Regresión Logística: salud\n          Microdatos relativos a la encuesta de Salud a personas en 2017. Altura, Peso y Edad del encuestado con el fin de clasificar el sexo de una persona en función de las anteriores variables.\n          Técnica estadística: Regresión Logística\n          Temas: Salud\n          Operaciones:\n          \n            \n              Encuesta Nacional de Salud (ENSE)\n            \n            \n          \n          \n          Número de observaciones: 23089\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n              \n                Preprocesado de datos\n              \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - ARIMA: Paro\n          Datos relativos al número de parados de España desde 2013 hasta 2024 con carácter trimestral.\n          Técnica estadística: Series Temporales - ARIMA\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta de Población Activa (EPA)\n            \n            \n          \n          \n          Número de observaciones: 46\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - ARIMA: ipc_series\n          Datos relativos a la serie del IPC mensual en españa desde 2002 hasta 2022.\n          Técnica estadística: Series Temporales - ARIMA\n          Temas: Precios\n          Operaciones:\n          \n            \n              Índice de Precios de Consumo (IPC)\n            \n            \n          \n          \n          Número de observaciones: 252\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - ARIMA: madrid_series\n          Datos relativos a la población de la CCAA de Madrid desde 1971 hasta 2022 desagregados por sexo y con cáracter bianual.\n          Técnica estadística: Series Temporales - ARIMA\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Estadística Continua de Población (ECP)\n            \n            \n          \n          \n          Número de observaciones: 104\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - ARIMA: navarra\n          Datos relativos a la población de la CCAA Navarra desde 1971 hasta 2021 desagregados por sexo y con cáracter bianual.\n          Técnica estadística: Series Temporales - ARIMA\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Estadística Continua de Población (ECP)\n            \n            \n          \n          \n          Número de observaciones: 101\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - Holt Winters: Paro\n          Datos relativos al número de parados de España desde 2013 hasta 2024 con carácter trimestral.\n          Técnica estadística: Series Temporales - Holt Winters\n          Temas: Mercado laboral y salarios\n          Operaciones:\n          \n            \n              Encuesta de Población Activa (EPA)\n            \n            \n          \n          \n          Número de observaciones: 46\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - Holt Winters: ipc_series\n          Datos relativos a la serie del IPC mensual en españa desde 2002 hasta 2022.\n          Técnica estadística: Series Temporales - Holt Winters\n          Temas: Precios\n          Operaciones:\n          \n            \n              Índice de Precios de Consumo (IPC)\n            \n            \n          \n          \n          Número de observaciones: 252\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n      \n        \n          Series Temporales - Holt Winters: madrid_series\n          Datos relativos a la población de la CCAA de Madrid desde 1971 hasta 2022 desagregados por sexo y con cáracter bianual.\n          Técnica estadística: Series Temporales - Holt Winters\n          Temas: Demografía y población\n          Operaciones:\n          \n            \n              Estadística Continua de Población (ECP)\n            \n            \n          \n          \n          Número de observaciones: 104\n          \n            \n              Fichero\n            \n            \n              Notebook\n            \n            \n          \n        \n      \n    \n  \n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html",
    "title": "Regresión Lineal: salud_reg",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo una Regresión Lineal a partir de un conjunto de datos, explicando las hipótesis que se deben satisfacer y un posterior análisis de bondad del modelo. Por último se explicará como utilizar el modelo creado para predecir nuevas observaciones.\n\n\nEn este cuaderno vamos a analizar el dataset llamado salud_reg.xlsx. Este contiene microdatos relativos a la Encuesta Nacional de Salud a personas en 2017. Altura, Peso, Edad, Sexo e IMC del encuestado con el fin de hacer una regresión del Peso en función del resto de variables y explicar por qué es interesante este estudio.\n\nConcretamente tenemos las siguientes variables:\nEdad: Edad en años cumplidos del encuestado.\nSexo : Sexo del encuestado (1=Hombre, 0=Mujer).\nAltura : Edad en cm del encuestado.\nPeso : Peso en kg del encuestado.\nIMC : “Clasificación del IMC de la persona.\n\n1: Peso insuficiente\n2: Normopeso\n3: Sobrepeso\n4: Obesidad\n\n\nNota: Notar que el Índice de Masa Corporal (IMC) es un indicador que pone en relación el peso (en kg) de la persona, con su altura en metros (al cuadrado). Informalmente, esta medida se toma como un indicador básico del estado de salud de la persona.\nSi conociéramos el IMC exacto, no tendría sentido efectuar el Análisis de Regresión puesto que el Peso se podría calcular directamente despejando IMC y Altura. Sin embargo, tenemos categorías para el IMC, que son: Peso Insuficiente, Normal, Sobrepeso y Obesidad. Bajo este paradigma tiene sentido plantear la regresión para ver si somos capaces de recuperar el Peso de una persona, sin conocer directamente su IMC, sino la clase en la que cae, junto con su Altura. Además se verá si el resto de variables ayudan a estimar el Peso.\n\n\n\nElaborar una Regresión Lineal que explique el Peso en función del resto de las variables.\n\nHacer un análisis exploratorio.\nPlantear las hipótesis de una regresión (incluyendo todas variables).\nAnalizar el modelo planteado y su ajuste de bondad.\nEvaluar distintos modelos y seleccionar el más adecuado.\nConclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#dataset",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#dataset",
    "title": "Regresión Lineal: salud_reg",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado salud_reg.xlsx. Este contiene microdatos relativos a la Encuesta Nacional de Salud a personas en 2017. Altura, Peso, Edad, Sexo e IMC del encuestado con el fin de hacer una regresión del Peso en función del resto de variables y explicar por qué es interesante este estudio.\n\nConcretamente tenemos las siguientes variables:\nEdad: Edad en años cumplidos del encuestado.\nSexo : Sexo del encuestado (1=Hombre, 0=Mujer).\nAltura : Edad en cm del encuestado.\nPeso : Peso en kg del encuestado.\nIMC : “Clasificación del IMC de la persona.\n\n1: Peso insuficiente\n2: Normopeso\n3: Sobrepeso\n4: Obesidad\n\n\nNota: Notar que el Índice de Masa Corporal (IMC) es un indicador que pone en relación el peso (en kg) de la persona, con su altura en metros (al cuadrado). Informalmente, esta medida se toma como un indicador básico del estado de salud de la persona.\nSi conociéramos el IMC exacto, no tendría sentido efectuar el Análisis de Regresión puesto que el Peso se podría calcular directamente despejando IMC y Altura. Sin embargo, tenemos categorías para el IMC, que son: Peso Insuficiente, Normal, Sobrepeso y Obesidad. Bajo este paradigma tiene sentido plantear la regresión para ver si somos capaces de recuperar el Peso de una persona, sin conocer directamente su IMC, sino la clase en la que cae, junto con su Altura. Además se verá si el resto de variables ayudan a estimar el Peso.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Lineal: salud_reg",
    "section": "",
    "text": "Elaborar una Regresión Lineal que explique el Peso en función del resto de las variables.\n\nHacer un análisis exploratorio.\nPlantear las hipótesis de una regresión (incluyendo todas variables).\nAnalizar el modelo planteado y su ajuste de bondad.\nEvaluar distintos modelos y seleccionar el más adecuado.\nConclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#cargar-librerías",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#cargar-librerías",
    "title": "Regresión Lineal: salud_reg",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(readxl) # Para leer los excels\n# library(kableExtra) # Para dar formato a las tablas html\nlibrary(knitr)\nlibrary(gridExtra) # Para cargar bien las tab\nlibrary(car) # for bonfferroni test\nlibrary(ggplot2)",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#lectura-datos",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#lectura-datos",
    "title": "Regresión Lineal: salud_reg",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\nsalud &lt;- read_excel(\"../../../files/salud_reg.xlsx\", sheet = \"Datos\")\n\n\nanyNA(salud) # Any missing data\n\n[1] FALSE",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#hipótesis-y-indicadores-de-bondad",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#hipótesis-y-indicadores-de-bondad",
    "title": "Regresión Lineal: salud_reg",
    "section": "Hipótesis y indicadores de bondad",
    "text": "Hipótesis y indicadores de bondad\nPara que una regresión lineal proporcione un buen ajuste a los datos debe cumplir una serie de requisitos que por tanto deben ser verificados al llevar a cabo el estudio. Recordar que la regresión lineal se expresa como:\n\\[\n\\mathbf{Y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\n\\] donde \\(\\mathbf{Y}\\) es la variable respuesta, \\(\\mathbf{X}\\) los predictores (hay \\(k\\) variables predictoras), \\(\\boldsymbol{\\beta}\\) los coeficientes de la regresión y \\(\\boldsymbol{\\varepsilon}\\) el error. \\[\n\\mathbf{Y}=\\left[\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array}\\right] \\quad \\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1 k} \\\\\n1 & x_{21} & \\ldots & x_{2 k} \\\\\n\\vdots & \\ddots & \\vdots & \\\\\n1 & x_{n 1} & \\ldots & x_{n k}\n\\end{array}\\right] \\quad \\boldsymbol{\\beta}=\\left[\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{array}\\right] \\quad \\boldsymbol{\\varepsilon}=\\left[\\begin{array}{c}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{array}\\right]\n\\] Las hipótesis que se deben cumplir son:\n\nLinealidad: La media de la respuesta es función lineal de los predictores. En términos matemáticos: \\[E\\left[\\mathbf{Y} \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k \\]\nIndependencia de errores: Los errores \\(\\varepsilon_i\\) deben ser independientes, es decir, \\(Cov[\\varepsilon_i,\\varepsilon_j] =0, \\; \\forall i\\neq j\\).\nHomocedasticidad: La varianza del error debe ser constante.\n\n\\[Var\\left[\\varepsilon_i \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\sigma^2 \\quad \\forall \\;i \\]\n\nNormalidad : Los errores deben estar distribuidos normalmente, es decir, \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\; \\forall i\\).\n\nPara analizar la bondad, hay algunos indicadores como el Coeficiente de Determinación o \\(R^2\\) que representa el porcentaje de variabilidad de la variable respuesta que es capaz de explicar el modelo. Es decir, si toma valor 1 hay una dependencia lineal exacta entre los predictores y la variable respuesta y por tanto las predicciones serán perfectas. Por el contrario, si toma valor 0 habrá que desechar el modelo puesto que no es capaz de predecir con nada de exactitud.\nEn caso de que haya más de un predictor (\\(k &gt;1\\), Regresión Lineal Múltiple), es más recomendable usar el Coeficiente de Determinación Ajustado \\(R^2\\_adj\\) como indicador de bondad, pues el \\(R^2\\) puede inflarse artificialmente debido a la presencia de varios predictores. Su interpretación es similar.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/salud_reg.html#modelo",
    "href": "notebooks/Linear Regression/salud_reg/salud_reg.html#modelo",
    "title": "Regresión Lineal: salud_reg",
    "section": "Modelo",
    "text": "Modelo\nEn este caso nos encontramos ante una Regresión Lineal Múltiple puesto que tenemos más de una variable predictora. Inicialmente vamos a considerar un modelo con todas variables predictoras para intentar predecir el \\(peso\\) y veremos si este modelo cumple las hipótesis necesarias y cuan bueno es.\nNOTA IMPORTANTE: Destacar que hay predictores que son variables discretas (no continuas), luego se deberán tratar como variables factor en R. En este notebook se verá como interpretar los resultados cuando están presentes este tipo de variables. La principal diferencia es que los modelos de regresión usan internamente variables dummy para este tipo de variables, luego por cada variable factor habrá tantos coeficientes de regresión como categorías menos uno, es decir, como número de variables dummy. Este tipo de codificación se denomina One Hot Encoding, para más información leer el link.\n\n# Convertir a factor variables\nsalud$SEXO &lt;- as.factor(salud$SEXO)\nsalud$IMC &lt;- as.factor(salud$IMC)\n\n\n\n# Dividimos los datos en Train/Test\n\nsalud_test &lt;- salud[22000:22019, ]\nsalud &lt;- salud[1:21999, ]\n\n\n\n# Modelo inicial\nlm1 &lt;- lm(Peso ~ EDAD + Altura + IMC + SEXO, salud)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = Peso ~ EDAD + Altura + IMC + SEXO, data = salud)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.946  -3.845  -0.137   3.510  79.105 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -94.986808   1.027145 -92.477   &lt;2e-16 ***\nEDAD          0.027838   0.002288  12.169   &lt;2e-16 ***\nAltura        0.854409   0.005848 146.115   &lt;2e-16 ***\nIMC2         13.430315   0.270893  49.578   &lt;2e-16 ***\nIMC3         26.165755   0.274741  95.238   &lt;2e-16 ***\nIMC4         43.391437   0.282888 153.387   &lt;2e-16 ***\nSEXO1         1.000631   0.106538   9.392   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.735 on 21992 degrees of freedom\nMultiple R-squared:  0.852, Adjusted R-squared:  0.852 \nF-statistic: 2.11e+04 on 6 and 21992 DF,  p-value: &lt; 2.2e-16\n\n\nA primera vista vemos un valor de Múltiple R-squared: 0.8521,, lo cual es bastante alto y por tanto nuestro modelo parece capturar bien la variabilidad de la variable respuesta, concretamente un \\(85\\%\\). Sin embargo, en los sucesivos modelos que plantemos no podemos usar como criterio de comparación el \\(R-squared\\) pues aumenta a la vez que lo hace el número de variables, y por tanto para comparar modelos entre si se debe usar el Adjusted R-squared (que tiene en cuenta el número de variables).\nEn la línea de los residuos no parece haber contraindicaciones a que estos sigan una distribución normal centrada en cero puesto que tenemos unas medidas de dispersión bastante simétricas. No obstante, más adelante se procederán a hacer los test pertinentes.\nAhora vamos a reducir el número de predictores que incluimos en el modelo y analizaremos que impacto tiene esto en la bondad.\n\nEn el modelo lm2, quitamos en primer lugar la variable Sexo, dejando EDAD, Altura, IMC.\nEn el modelo lm3, quitamos en primer lugar la variable IMC, dejando EDAD, Altura.\n\n\n# Sin Sexo\nlm2 &lt;- lm(Peso ~ EDAD + Altura + IMC, salud)\nsummary(lm2)\n\n\nCall:\nlm(formula = Peso ~ EDAD + Altura + IMC, data = salud)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.897  -3.891  -0.126   3.543  79.268 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.010e+02  8.017e-01 -126.03   &lt;2e-16 ***\nEDAD         3.109e-02  2.266e-03   13.72   &lt;2e-16 ***\nAltura       8.912e-01  4.352e-03  204.79   &lt;2e-16 ***\nIMC2         1.358e+01  2.710e-01   50.11   &lt;2e-16 ***\nIMC3         2.644e+01  2.737e-01   96.59   &lt;2e-16 ***\nIMC4         4.368e+01  2.818e-01  154.99   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.747 on 21993 degrees of freedom\nMultiple R-squared:  0.8514,    Adjusted R-squared:  0.8514 \nF-statistic: 2.521e+04 on 5 and 21993 DF,  p-value: &lt; 2.2e-16\n\n# Sin IMC\nlm3 &lt;- lm(Peso ~ EDAD + Altura, salud)\nsummary(lm3)\n\n\nCall:\nlm(formula = Peso ~ EDAD + Altura, data = salud)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.184  -8.402  -1.695   6.465 101.080 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -87.869119   1.660849  -52.91   &lt;2e-16 ***\nEDAD          0.171006   0.004753   35.98   &lt;2e-16 ***\nAltura        0.908802   0.009384   96.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.47 on 21996 degrees of freedom\nMultiple R-squared:  0.2999,    Adjusted R-squared:  0.2998 \nF-statistic:  4711 on 2 and 21996 DF,  p-value: &lt; 2.2e-16\n\n\nVemos que el Sexo no parece tener mucha importancia, no así el IMC que cuando lo sustraemos del modelo experimenta una bajada grande de bondad. Sin embargo al sustraer la variable IMC, vemos que el Adjusted R-squared baja bastante, por lo que se puede decir que dicha variable es de gran relevancia. Conceptualmente el IMC=peso/altura, y aunque nosotros sólo tengamos 4 categorías de IMC (en vez de la variable continua), es directo ver que dicha variable nos va a servir como gran fuente de información para predecir el peso. De manera contraria, el Sexo de una persona no parece estar muy correlacionado con el peso, hay mujeres y hombres con sobrepeso y mujeres y hombres con pesos bajos. Con la altura cabría esperar una relación más directa puesto que conforme crecemos (nos hacemos más altos), vamos aumentando de peso.\nEs por ello que nos vamos a quedar con el modelo lm1 o lm2. Vamos a interpretar ahora los parámetros del modelo 1:\n\nsummary(lm1)\n\n\nCall:\nlm(formula = Peso ~ EDAD + Altura + IMC + SEXO, data = salud)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.946  -3.845  -0.137   3.510  79.105 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -94.986808   1.027145 -92.477   &lt;2e-16 ***\nEDAD          0.027838   0.002288  12.169   &lt;2e-16 ***\nAltura        0.854409   0.005848 146.115   &lt;2e-16 ***\nIMC2         13.430315   0.270893  49.578   &lt;2e-16 ***\nIMC3         26.165755   0.274741  95.238   &lt;2e-16 ***\nIMC4         43.391437   0.282888 153.387   &lt;2e-16 ***\nSEXO1         1.000631   0.106538   9.392   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.735 on 21992 degrees of freedom\nMultiple R-squared:  0.852, Adjusted R-squared:  0.852 \nF-statistic: 2.11e+04 on 6 and 21992 DF,  p-value: &lt; 2.2e-16\n\n\nVemos los siguientes coeficientes:\n\n\\(\\beta (EDAD)\\): tiene un valor muy bajo (0.02), siendo positivo. Esto indica que no parece tener mucho peso en el modelo y que conforme aumente la edad, aumenta el Peso de la persona muy ligeramente, lo cual podría parecer razonable.\n\\(\\beta (Altura)\\): tiene un valor de (0.85), siendo positivo. Parece razonable que sea mayor que el del beta relativo a la edad puesto que a más altura, más posibilidad de tener mas masa y por tanto más peso. De ahí que además sea positivo. Por cada unidad más de altura (cada cm), aumenta el peso en 0.85kg en media.\n\nDestacar que ahora los siguientes betas se refieren a la variable IMC que es categórica. Es por ello que se toma como referencia la primera categoría (IMC=1) y los coeficientes representan el cambio de pasar de una categoría a otra.\n\n\\(\\beta (IMC2)\\): tiene un valor bastante alto (13.42), siendo positivo. Esto indica que al pasar de la clase IMC=1 (nivel base) A IMC=2, el peso aumenta en 13.42 puntos. Esto parece muy razonable puesto que en IMC=1 se representaba la gente con peso insuficiente y en IMC=2 la gente con peso normal. Es decir, el modelo nos está indicando que la diferencia entre ambas clases es de 13.42 kg.\n\\(\\beta (IMC3)\\): tiene un valor bastante alto (26), siendo positivo. Esto indica que al pasar de la clase IMC=1 A IMC=3, el peso aumenta en 13.42 puntos. Esto parece muy razonable puesto que en IMC=1 se representaba la gente con peso insuficiente y en IMC=3 la gente con sobrepeso. Además también es más grande que el beta anterior, indicando que es mas grande el cambio de peso entre gente con peso insuficiente y sobrepeso que entre las personas con peso insuficiente y peso normal.\n\\(\\beta (IMC4)\\): tiene un valor bastante alto (43), siendo positivo. Esto indica que al pasar de la clase IMC=1 A IMC=4, el peso aumenta en 13.42 puntos. Esto parece muy razonable puesto que en IMC=1 se representaba la gente con peso insuficiente y en IMC=4 la gente con obesidad. Además también es más grande que los betas anteriores, i indicando que es mas grande el cambio de peso entre gente con peso insuficiente y obesidad que entre el resto de clases.\n\\(\\beta (SEXO1)\\): tiene un valor igual a 1 (+1), siendo positivo. Puesto que es una variable factor, esto indica que pasar de la categoría base, mujer, a hombre, aumenta el peso. Esto también podría ser razonable puesto que manteniendo el resto de variables constantes, los hombres suelen tender ser más altos que las mujeres y por tanto tienden a pesar más. Es verdad que como hemos comentado antes, no es una correlación a prior tan evidente.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Regresión Lineal: salud_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo una Regresión Lineal a partir de un conjunto de datos, explicando las hipótesis que se deben satisfacer y un posterior análisis de bondad del modelo. Por último se explicará como utilizar el modelo creado para predecir nuevas observaciones.\nNota: Se trata de un ejemplo meramente didáctico ya que como se verá no tiene utilidad práctica puesto que se ha suprimido una dimensión.\n\n\nEl dataset imcv_reg.xlsx dispone de datos por Comunidades Autónomas de las nueve dimensiones relativas a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV) ¡Pincha aquí!, una estadística con carácter experimental. Datos correspondientes al año 2020.\nConcretamente tenemos las siguientes variables:\n\níndice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\ndim1 : Indicador sobre las condiciones materiales de vida.\ndim2 : Indicador sobre el trabajo.\ndim3 : Indicador sobre la salud.\ndim4 : Indicador sobre la educación.\ndim5 : Indicador sobre el ocio y relaciones sociales.\n\ndim6 : Indicador sobre la seguridad física y personal.\n\ndim7 : Indicador sobre la gobernanza y los derechos básicos.\n\ndim8 : Indicador sobre el entorno y el medioambiente.\nCCAA: Comunidades Autónomas.\n\nLos datos relativos a este estudio corresponden, como ya se ha comentado, a la estadística experimental sobre el ÍndiceMultidimensional de Calidad de Vida (IMCV). Se construye a partir de los indicadores de calidad del INE, que ofrecen una visión panorámica (multidimensional) de la calidad de vida en España, mediante la elección de un conjunto amplio pero limitado de indicadores (actualmente 60) que cubren nueve dimensiones usadas para describir la calidad de vida.\nNota: Notar que en este dataset no se tiene la variable dim9 ya que en ese caso el ajuste de la regresión sería prefecto puesto que la variable índice_total es la media aritmética de las 9 dimensiones.\n\n\n\nElaborar una Regresión Lineal que explique el índice total en función de las dimensiones (sin tener en cuenta la variable CCAA que tiene mero sentido identificador de las observaciones). Pasos recomendados:\n\nHacer un análisis exploratorio.\nPlantear las hipótesis de una regresión (incluyendo todas variables).\nAnalizar el modelo planteado y su ajuste de bondad.\nEvaluar distintos modelos y seleccionar el más adecuado.\nConclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#dataset",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#dataset",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "",
    "text": "El dataset imcv_reg.xlsx dispone de datos por Comunidades Autónomas de las nueve dimensiones relativas a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV) ¡Pincha aquí!, una estadística con carácter experimental. Datos correspondientes al año 2020.\nConcretamente tenemos las siguientes variables:\n\níndice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\ndim1 : Indicador sobre las condiciones materiales de vida.\ndim2 : Indicador sobre el trabajo.\ndim3 : Indicador sobre la salud.\ndim4 : Indicador sobre la educación.\ndim5 : Indicador sobre el ocio y relaciones sociales.\n\ndim6 : Indicador sobre la seguridad física y personal.\n\ndim7 : Indicador sobre la gobernanza y los derechos básicos.\n\ndim8 : Indicador sobre el entorno y el medioambiente.\nCCAA: Comunidades Autónomas.\n\nLos datos relativos a este estudio corresponden, como ya se ha comentado, a la estadística experimental sobre el ÍndiceMultidimensional de Calidad de Vida (IMCV). Se construye a partir de los indicadores de calidad del INE, que ofrecen una visión panorámica (multidimensional) de la calidad de vida en España, mediante la elección de un conjunto amplio pero limitado de indicadores (actualmente 60) que cubren nueve dimensiones usadas para describir la calidad de vida.\nNota: Notar que en este dataset no se tiene la variable dim9 ya que en ese caso el ajuste de la regresión sería prefecto puesto que la variable índice_total es la media aritmética de las 9 dimensiones.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "",
    "text": "Elaborar una Regresión Lineal que explique el índice total en función de las dimensiones (sin tener en cuenta la variable CCAA que tiene mero sentido identificador de las observaciones). Pasos recomendados:\n\nHacer un análisis exploratorio.\nPlantear las hipótesis de una regresión (incluyendo todas variables).\nAnalizar el modelo planteado y su ajuste de bondad.\nEvaluar distintos modelos y seleccionar el más adecuado.\nConclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#cargar-librerías",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#cargar-librerías",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(readxl) # Para leer los excels\nlibrary(kableExtra) # Para dar formato a las tablas html\nlibrary(knitr) # Formato tablas en html\nlibrary(gridExtra) # Para el layout de los gráficos\nlibrary(car) # Para el bonfferroni test\nlibrary(corrplot) # Para el gráfico de correlaciones\nlibrary(ggplot2)\nlibrary(lmtest) # Test Homocedasticidad",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#lectura-datos",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#lectura-datos",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\nIMCV &lt;- read_excel(\"../../../files/IMCV_reg.xlsx\", sheet = \"Datos\")\n\n\nc(\n  anyNA(IMCV), # Any missing data\n  any(IMCV == 0)\n) # Any value equal to 0\n\n[1] FALSE FALSE",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#análisis",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#análisis",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Análisis",
    "text": "Análisis\nRealizando un resumen numérico vemos que todas las dimensiones toman valores en torno a \\([90,110]\\). Recordar que estas representan índices. Para ninguna de ellas parece haber valores atípicos en relación a los demás luego no parece necesario hacer ningún tipo de ajuste a los datos.\nAdemás las medias y medianas son muy parecidas. Junto con los histogramas y los boxplots se podría concluir que no parece haber ningún dato atípico/outlier y tampoco parece haber mucha asimetría, hecho que se puede ver en los histogramas y en los boxplots (mirando las distancias entre máximo/tercer cuartil y mínimo/1er cuartil son parecidas).\n\n\n\nTable showing the measures of interest.\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nMin\nQ1\nMean\nMedian\nQ3\nMax\n\n\n\n\naño\naño\n2020.00\n2020.00\n2020.50\n2020.50\n2021.00\n2021.00\n\n\ndim1\ndim1\n90.99\n97.41\n100.04\n100.64\n103.46\n105.35\n\n\ndim2\ndim2\n94.47\n97.59\n99.69\n99.92\n102.57\n103.42\n\n\ndim3\ndim3\n98.20\n99.69\n101.14\n100.80\n102.32\n105.49\n\n\ndim4\ndim4\n97.35\n103.97\n107.33\n106.94\n109.58\n118.46\n\n\ndim5\ndim5\n94.32\n96.69\n99.75\n99.48\n102.05\n107.28\n\n\ndim6\ndim6\n92.89\n99.81\n102.83\n102.76\n106.39\n109.48\n\n\ndim7\ndim7\n91.22\n96.36\n99.97\n100.23\n102.73\n109.35\n\n\ndim8\ndim8\n93.17\n98.93\n102.51\n103.99\n105.88\n109.42\n\n\nindice_total\nindice_total\n96.05\n100.33\n101.80\n102.07\n103.61\n105.87\n\n\n\n\n\n\n\n\n\n\nPlots de todas las dimensiones.\n\n\n\n\n\n\n\nPlots de todas las dimensiones.\n\n\n\n\n\n\n\n\n\nPlots para la dimensión total.\n\n\n\n\nEn general parecen unos datos bien balanceados y con buenas propiedades.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#hipótesis-y-indicadores-de-bondad",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#hipótesis-y-indicadores-de-bondad",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Hipótesis y indicadores de bondad",
    "text": "Hipótesis y indicadores de bondad\nPara que una regresión lineal proporcione un buen ajuste a los datos debe cumplir una serie de requisitos que por tanto deben ser verificados al llevar a cabo el estudio. Recordar que la regresión lineal se expresa como: \\[\n\\mathbf{Y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\n\\] donde \\(\\mathbf{Y}\\) es la variable respuesta, \\(\\mathbf{X}\\) los predictores (hay \\(k\\) variables predictoras), \\(\\boldsymbol{\\beta}\\) los coeficientes de la regresión y \\(\\boldsymbol{\\varepsilon}\\) el error. \\[\n\\mathbf{Y}=\\left[\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array}\\right] \\quad \\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1 k} \\\\\n1 & x_{21} & \\ldots & x_{2 k} \\\\\n\\vdots & \\ddots & \\vdots & \\\\\n1 & x_{n 1} & \\ldots & x_{n k}\n\\end{array}\\right] \\quad \\boldsymbol{\\beta}=\\left[\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{array}\\right] \\quad \\boldsymbol{\\varepsilon}=\\left[\\begin{array}{c}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{array}\\right]\n\\] Las hipótesis que se deben cumplir son:\n\nLinealidad: La media de la respuesta es función lineal de los predictores. En términos matemáticos: \\[E\\left[\\mathbf{Y} \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k \\]\nIndependencia de errores: Los errores \\(\\varepsilon_i\\) deben ser independientes, es decir, \\(Cov[\\varepsilon_i,\\varepsilon_j] =0, \\; \\forall i\\neq j\\).\nHomocedasticidad: La varianza del error debe ser constante.\n\n\\[Var\\left[\\varepsilon_i \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\sigma^2 \\quad \\forall \\;i \\]\n\nNormalidad : Los errores deben estar distribuidos normalmente, es decir, \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\; \\forall i\\).\n\nPara analizar la bondad, hay algunos indicadores como el Coeficiente de Determinación o \\(R^2\\) que representa el porcentaje de variabilidad de la variable respuesta que es capaz de explicar el modelo. Es decir, si toma valor 1 hay una dependencia lineal exacta entre los predictores y la variable respuesta y por tanto las predicciones serán perfectas. Por el contrario, si toma valor 0 habrá que desechar el modelo puesto que no es capaz de predecir con nada de exactitud.\nEn caso de que haya más de un predictor (\\(k &gt;1\\), Regresión Lineal Múltiple), es más recomendable usar el Coeficiente de Determinación Ajustado \\(R^2\\_adj\\) como indicador de bondad, pues el \\(R^2\\) puede inflarse artificialmente debido a la presencia de varios predictores. Su interpretación es similar.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#modelo",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#modelo",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Modelo",
    "text": "Modelo\nEn este caso nos encontramos ante una Regresión Lineal Múltiple puesto que tenemos más de una variable predictora. Inicialmente vamos a considerar un modelo con todas variables predictoras para intentar predecir el \\(índice\\_total\\) y veremos si este modelo cumple las hipótesis necesarias y cuan bueno es.\nEste modelo busca ajustar el índice_total en función de las 8 primeras dimensiones.\n\nIMCV_test &lt;- IMCV[35:38, ] # Guardamos parte de los datos para evaluar después\nIMCV &lt;- IMCV[1:34, ]\n\n\n# Modelo inicial\nlm1 &lt;- lm(indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + dim6 + dim7 + dim8, IMCV)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + \n    dim6 + dim7 + dim8, data = IMCV)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58365 -0.12689 -0.02845  0.16057  0.55151 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.85420    4.38109   1.108  0.27841    \ndim1         0.09629    0.04879   1.973  0.05961 .  \ndim2         0.09238    0.04681   1.973  0.05960 .  \ndim3         0.05916    0.05847   1.012  0.32136    \ndim4         0.15649    0.02109   7.419 9.04e-08 ***\ndim5         0.13516    0.02317   5.833 4.40e-06 ***\ndim6         0.08815    0.02989   2.949  0.00682 ** \ndim7         0.14988    0.01256  11.935 8.07e-12 ***\ndim8         0.17320    0.03310   5.233 2.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2995 on 25 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9834 \nF-statistic: 245.3 on 8 and 25 DF,  p-value: &lt; 2.2e-16\n\n\nA primera vista vemos un valor de Múltiple R-squared: 0.9812,, lo cual es bastante alto y por tanto nuestro modelo parece capturar bien la variabilidad de la variable respuesta, concretamente un \\(98\\%\\). Sin embargo, en los sucesivos modelos que planteemos no podemos usar como criterio de comparación el \\(R-squared\\) pues aumenta a la vez que lo hace el número de variables, y por tanto para comparar modelos entre si se debe usar el Adjusted R-squared (que tiene en cuenta el número de variables).\nEn la línea de los residuos no parece haber contraindicaciones a que estos sigan una distribución normal centrada en cero puesto que tenemos unas medidas de dispersión bastante simétricas. No obstante, más adelante se procederán a hacer los test pertinentes.\nEn la última linea se lleva a cabo un Test de Significación Global \\(F-Test\\) lo que considera es la hipótesis nula de\n\\[H_0: \\beta_i =0\\; \\forall i\n\\\\\nH_1: al\\; menos \\; un \\; \\beta_i \\neq 0\\].\nPara un nivel de confianza de \\(0.95\\) podemos rechazar la hipótesis nula (puesto que p_val&lt; 0.05) y por tanto aceptar la alternativa, lo cual es buena señal.\nNo obstante es necesario analizar que se cumplan las hipótesis iniciales para poder asegurar que estamos ante un buen modelo.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#test-de-bonferroni-datos-atípicos",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#test-de-bonferroni-datos-atípicos",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Test de Bonferroni (datos atípicos)",
    "text": "Test de Bonferroni (datos atípicos)\nLa idea principal es verificar si los residuos de las observaciones son significativamente diferentes de cero. Si un residuo tiene un valor “studentizado” grande en comparación con una distribución t, puede considerarse como un posible valor atípico. Esto se debe a que teóricamente se demuestra que los residuos “studentizados” \\(r^* _i \\sim t_{n-k-1}\\) con k el número de predictores. En este caso parece no haber indicación de valores atípicos.\n\noutlierTest(lm1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n31 -2.511136           0.019173      0.65187",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#test-homocedasticidad",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#test-homocedasticidad",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Test homocedasticidad",
    "text": "Test homocedasticidad\nEn términos sencillos, la Prueba de Breusch-Pagan evalúa si la varianza de los errores en un modelo de regresión es constante o si varía a lo largo de los valores de las variables predictoras. Una violación de la homocedasticidad puede afectar la validez de las inferencias realizadas a partir del modelo.\nEl test funciona de la siguiente manera: se obtienen los residuos al cuadrado y se realiza una regresión auxiliar para determinar si hay una relación significativa entre los residuos al cuadrado y las variables predictoras. Si se encuentra evidencia significativa, puede indicar la presencia de heterocedasticidad.\n\nbptest(indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + dim6 + dim7 + dim8, data = IMCV, varformula = ~ fitted.values(lm1), studentize = FALSE)\n\n\n    Breusch-Pagan test\n\ndata:  indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + dim6 + dim7 +     dim8\nBP = 0.93094, df = 1, p-value = 0.3346\n\n\n\\[H_0: Var[\\varepsilon_i ] =\\sigma^2 \\; \\forall \\;i\n\\\\\nH_1: Var[\\varepsilon_i ] \\neq \\sigma^2 \\; \\forall \\;i\\].\nSi el valor p obtenido de la prueba de Breusch-Pagan es \\(0.33\\), interpretaríamos esto como evidencia insuficiente para rechazar la hipótesis nula de homocedasticidad (a nivel de significancia de \\(0.05\\)). En otras palabras, no tendríamos suficiente evidencia estadística para decir que hay heterocedasticidad en los errores del modelo de regresión.\nEn términos prácticos, esto sugiere que la varianza de los errores parece ser constante a lo largo de los valores de las variables predictoras.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#normalidad-de-residuos",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#normalidad-de-residuos",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Normalidad de residuos",
    "text": "Normalidad de residuos\nEl Test de Shapiro es una prueba de normalidad que se utiliza para evaluar si una muestra proviene de una población con una distribución normal. La hipótesis nula del test es que la muestra sigue una distribución normal. Si el valor p obtenido en la prueba es menor que el nivel de significancia (comúnmente establecido en \\(0.05\\)), se rechaza la hipótesis nula, indicando que la muestra no sigue una distribución normal.\n\\[H_0: \\varepsilon_i  \\sim N(\\;,\\;) \\; \\forall\n\\\\\nH_1: \\varepsilon_i  \\nsim N(\\;,\\;) \\; \\forall \\].\n\nshapiro.test(lm1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm1$residuals\nW = 0.98163, p-value = 0.8226\n\n\nAceptamos la normalidad de los residuos puesto que el \\(p-value&gt;0.05\\).",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#test-linealidad",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#test-linealidad",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Test linealidad",
    "text": "Test linealidad\nLa hipótesis alternativa analiza si la inclusión de términos cuadráticos (potencia 2) de las variables predictoras mejora significativamente el modelo en comparación con un modelo que solo incluye términos lineales.\n\nresettest(indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + dim6 + dim7 + dim8, data = IMCV, power = 2, type = \"regressor\")\n\n\n    RESET test\n\ndata:  indice_total ~ dim1 + dim2 + dim3 + dim4 + dim5 + dim6 + dim7 +     dim8\nRESET = 1.7105, df1 = 8, df2 = 17, p-value = 0.1675\n\n\nAceptamos la linealidad puesto que el \\(p-value&gt;0.05\\), a un nivel de significancia de \\(\\alpha =0.05\\), luego no es necesario incluir términos cuadráticos.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#gráfico-de-influencia-del-modelo-porpuesto",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#gráfico-de-influencia-del-modelo-porpuesto",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Gráfico de influencia del modelo porpuesto",
    "text": "Gráfico de influencia del modelo porpuesto\nEn el siguiente gráfico se muestran los residuos “studentizados”, es decir, los residuos transformados a una \\(N(0,1)\\). Es por ello, que debido a el cuantil \\(z_{\\alpha/2}=-1.96\\; con \\; \\alpha=0.05\\) de una normal, sabemos que el \\(95\\%\\) de elementos deben estar contenidos en \\((-1,96,1.96)\\) que son las rayas horizontales del gráfico.\n\ninfluencePlot(lm1, id = list(method = \"noteworth\", n = 2))\n\n\n\n\n\n\n\n\n\nLos residuos bajo hipótesis de RLM siguen una N(0,sigma) y los “studentizados” un N(0,1), es decir el \\(95\\%\\) de datos están entre \\((-1.96,1.96)\\), las líneas horizontales. Tenemos 20 observaciones y 2 datos fuera de la línea lo que a priori podría ser correcto.\nLas líneas verticales indican los datos con apalancamiento en el modelo. Es decir los datos fuera de la línea vertical derecha No vemos ni siquiera las lineas entonces no parece haber apalancamiento.\nEl área de las burbujas es proporcional a la Distancia de Cook (mide cómo cambian los parámetros del modelo cuando se excluye una observación específica). Vemos que hay uno con una gran distancia de cook (tienen residuo grande), luego esto nos indica que podría considerarse en cierto modo atípico. Como no hemos encontrado más evidencias de que fuera atípico lo vamos a dejar así.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/IMCV/IMCV.html#colinealidad",
    "href": "notebooks/Linear Regression/IMCV/IMCV.html#colinealidad",
    "title": "Regresión Lineal: IMCV_reg",
    "section": "Colinealidad",
    "text": "Colinealidad\nCuando los regresores no tienen una relación lineal, se consideran ortogonales. Sin embargo, en la mayoría de las aplicaciones de regresión, los regresores no cumplen con esta condición. En algunos casos, la falta de ortogonalidad no representa un problema significativo, pero en otros, los regresores pueden estar tan estrechamente relacionados linealmente de manera que las predicciones del modelo de regresión se vuelven poco fiables o incorrectas.\nEste fenómeno, en el que los regresores presentan dependencias lineales casi perfectas, se conoce como el problema de colinealidad. Se realiza la matriz de correlaciones para ver la dependencia lineal entre las distintas variables.\n\nmat_cor &lt;- cor(IMCV[, 2:10]) %&gt;% round(digits = 2)\ncorrplot(mat_cor, type = \"upper\", order = \"hclust\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nCombinando los resultados de la matriz anterior y el conocimiento de las variables es posible identificar variables que podrían generar el problema de multicolinealidad. De hecho si en el conjunto de datos hubiéramos tenido todas las dimensiones (de la 1 a la 9) y índice_total, hubiera habido un problema de colinealidad puesto que el índice total es la media aritmética del resto de dimensiones. Es por ello que se ha optado por suprimir la última dimensión.\nPor otro lado, el factor de inflación de la varianza (FIV) detecta si una variable independiente es colineal con el resto. Es decir, mira cuanto se infla la varianza de los estimadores por culpa de la colinealidad de unas variables respecto a otras. Decisión: Un Valor del FIV mayor de 10 requiere actuación.\n\n# Inflación de la varianza\nvif(lm1)\n\n     dim1      dim2      dim3      dim4      dim5      dim6      dim7      dim8 \n12.814957  7.087505  4.764287  3.043847  2.324441  5.740449  1.424524  7.781270 \n\n\nVemos que la primera dimensión podría presentar problemas de colinealidad.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "IMCV",
      "Regresión Lineal: IMCV_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/Dataset_cleaning.html",
    "href": "notebooks/Linear Regression/matrimionios_reg/Dataset_cleaning.html",
    "title": "Preproceso",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para predecir el número de matrimonios en las ciudades española a partir del número de mujeres que las habitan y el número de nacimientos que ha habido ese año. Concretamente, se han tomado los datos relativos a 2022. Este análisis parece razonable a simple vista ya que el matrimonio la mayor parte de las veces va a acompañado de un nacimiento en los meses anteriores o posteriores.\nLos datos del año 2022 necesitan ser combinados para obtener unos datos consientes para usar. Concretamente tomaremos:\n\nCifras oficiales del padrón por municipios en 2022.\nNacimientos por municipios en el año 2022.\nMatrimonios por municipios en el año 2022.\n\nEn todo caso trataremos las poblaciones con un número de habitantes comprendido entre 50.000 y 300.000, habiendo aproximadamente 150 poblaciones en España de este tipo. Los datos pueden descargarse en .csv (Formato delimitado por comillas) en:\n\nPincha aquí. Población total\nPincha aquí. Población femenina\nPincha aquí. Nacimientos\nPincha aquí. Matrimonios.\n\n\nPreproceso\nProcedemos a leer los cuatro ficheros de datos ya que vienen de orígenes distintos (consultar en los links del apartado anterior). Todos ellos se leerán directamente como un csv desde su dirección web. Concretamente se busca extraer de cada uno:\n\nPoblación total: Se busca extraer los códigos de las ciudades y su población para posteriormente poder filtrar las ciudades por número de habitantes.\nPoblación femenina: Código de ciudades y número de habitantes femeninas.\nNacimientos: Códigos de ciudades y número de nacimientos en dicho año en inscritos en la ciudad.\nMatrimonios: Códigos de ciudades y número de matrimonios en dicho año en inscritos en la ciudad.\n\nLos datos de las diferentes fuentes se van a juntar mediante el uso de la variable código de ciudad (JOIN).\nlibrary(readr)\nlibrary(dplyr)\n# Intuimos el encoding que siguen los ficheros\nguess_encoding(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/29005.csv?nocab=1\")\n\n\nguess_encoding(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/31934.csv?nocab=1\")\n\n# Población mujeres\npoblmuj &lt;- read_delim(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/29005.csv?nocab=1\",\n  delim = \";\", escape_double = FALSE, locale = locale(decimal_mark = \",\", encoding = \"ISO-8859-1\", asciify = TRUE),\n  trim_ws = TRUE\n)\npoblmuj &lt;- filter(poblmuj, Periodo == \"2022\")\n\n# Población total\npobltotal &lt;- read_delim(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/29005.csv?nocab=1\",\n  delim = \";\", escape_double = FALSE, locale = locale(decimal_mark = \",\", encoding = \"ISO-8859-1\", asciify = TRUE),\n  trim_ws = TRUE\n)\n\npobltotal &lt;- filter(pobltotal, Periodo == \"2022\")\n\n\n# Nacimientos\nnacim22 &lt;- read_delim(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/31934.csv?nocab=1\",\n  delim = \";\", escape_double = FALSE, locale = locale(decimal_mark = \",\", encoding = \"ISO-8859-1\", asciify = TRUE),\n  trim_ws = TRUE\n)\n\nnacim22 &lt;- as.data.frame(nacim22)\ncolnames(nacim22)[3] &lt;- \"Edaddelamadre\"\ncolnames(nacim22)[4] &lt;- \"Mesdelnacimiento\"\nnacim22 &lt;- filter(nacim22, Periodo == \"2022\", Sexo == \"Mujeres\", Mesdelnacimiento == \"Total\", Edaddelamadre == \"Todas las edades\")\n\n\n# Matrimonios\nmatrim22 &lt;- read_delim(\"https://www.ine.es/jaxiT3/files/t/es/csv_bdsc/37645.csv?nocab=1\",\n  delim = \";\", escape_double = FALSE, locale = locale(decimal_mark = \",\", encoding = \"ISO-8859-1\", asciify = TRUE),\n  trim_ws = TRUE\n)\n\nmatrim22 &lt;- as.data.frame(matrim22)\ncolnames(matrim22)[3] &lt;- \"Mesdecelebracion\"\ncolnames(matrim22)[4] &lt;- \"Formadecelebracion\"\nmatrim22 &lt;- filter(matrim22, Periodo == \"2022\", Mesdecelebracion == \"Total\", Formadecelebracion == \"Total\")\nEn cada conjunto de datos nos quedamos únicamente con los nombres de los municipios y la variable “Total” que indica la magnitud de cada fenómeno.\npoblmuj &lt;- poblmuj[, c(\"Municipios\", \"Total\")]\nnacim22 &lt;- nacim22[, c(\"Municipios\", \"Total\")]\nmatrim22 &lt;- matrim22[, c(\"Municipios\", \"Total\")]\npobltotal &lt;- pobltotal[, c(\"Municipios\", \"Total\")]\n\n\n# Convertimos a data frame\npoblmuj &lt;- as.data.frame(poblmuj)\nnacim22 &lt;- as.data.frame(nacim22)\nmatrim22 &lt;- as.data.frame(matrim22)\npobltotal &lt;- as.data.frame(pobltotal)\n\n\n\n\n# Extraemos códigos de ciudades de la variable municipio\n# Ej 20003 Albacete\npoblmuj$Ciudad &lt;- sub(\".* \", \"\", poblmuj$Municipios)\npoblmuj$Municipios &lt;- sub(\" .*\", \"\", poblmuj$Municipios)\nnacim22$Municipios &lt;- sub(\" .*\", \"\", nacim22$Municipios)\nmatrim22$Municipios &lt;- sub(\" .*\", \"\", matrim22$Municipios)\npobltotal$Municipios &lt;- sub(\" .*\", \"\", pobltotal$Municipios)\nJuntamos ahora todos datos en una tabla mediante el uso de la variable Municipios, que contiene los códigos de los municipios.\n# Juntamos los matrimonios con la pobalción de mujeres de cada ciudad\ninner1 &lt;- inner_join(matrim22, poblmuj, by = \"Municipios\", suffix = c(\".matr\", \".muj\"))\ninner1 &lt;- as.data.frame(inner1)\n\n# Juntamos el anterior con los nacimientos\ninner2 &lt;- inner_join(inner1, nacim22, by = \"Municipios\")\ninner2 &lt;- as.data.frame(inner2)\nCreamos un dataset con los nombres de las variables correctos y volvemos a hacer un join con la Población Total de cada ciudad.\ndata &lt;- data.frame(Municipios = inner2$Municipios, Ciudad = inner2$Ciudad, Matrimonios = inner2$Total.matr, Mujeres = inner2$Total.muj, Nacimientos = inner2$Total)\ndata &lt;- as.data.frame(data)\npobltotal &lt;- as.data.frame(pobltotal)\n\ndata &lt;- inner_join(data, pobltotal, by = \"Municipios\")\n\n\n# Filtramos por ciudades entre 50.000 y 300.000 habitantes\ndata &lt;- filter(data, Total &lt; 300000)\n\ndata &lt;- filter(data, Total &gt; 50000)\n\n\nVariables finales\nEs decir, finalmente tenemos un dataset conteniendo para las ciudades españolas de entre 50.000 y 300.000 habitantes la siguiente información:\n\nMunicipios: Códigos de los municipios.\nCiudad: Nombres de los municipios.\nMatrimonios: Número de matrimonios en cada municipio.\nMujeres: Número total de habitantes mujeres en cada municipio.\nNacimientos:Número de nacimientos en cada municipio.\nTotal: Número total de habitantes en cada municipio.\n\n\n\nExportamos datos\nTeniendo en cuenta las variables anteriores, se procede a exportar el conjutno de datos.\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(data, \"../../../files/matrimonios_reg.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. Se puede encontrar en matrimonios_reg.xlsx.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Preproceso"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "",
    "text": "En este notebook se va a exponer como llevar a cabo el análisis de una serie temporal mediante un modelo Holt Winters, esto es, un suavizado exponencial triple. Para ello se verá la teoría que sustenta este método y se mostrará un caso práctico con un conjunto de datos real.\n\n\nEn este cuaderno vamos a analizar el dataset llamado Paro.xlsx. Este dataset presenta los datos de paro por cuatrimestre en España a partir del año 2013, posterior a la crisis económica de 2011.\nCorresponde a la operación estadística del INE 30308 Encuesta de Población Activa (EPA). El objetivo de este estudio es intentar modelizar el número de parados a través de un modelo Holt-Winters y ver si realmente el número de parados se podría aproximar por este modelo sin tener en cuenta variables externas.\nParece razonable aclarar a priori el concepto de “parado” con el fin de entender que es lo que queremos modelizar. De acuerdo con el informe metodológico presentado por el INE:\n\nSe considerarán paradas a todas las personas de 16 o más años que reúnan simultáneamente las siguientes condiciones:\n\nestar sin trabajo, es decir, que no hayan tenido un empleo por cuenta ajena ni por cuenta propia durante la semana de referencia.\nestar buscando trabajo, es decir, que hayan buscado activamente un trabajo por cuenta ajena o hayan hecho gestiones para establecerse por su cuenta durante el mes precedente.\nestar disponibles para trabajar, es decir, en condiciones de comenzar a hacerlo en un plazo de dos semanas a partir del domingo de la semana de referencia.\n\nTambién se consideran paradas las personas de 16 o más años que durante la semana de referencia han estado sin trabajo, disponibles para trabajar y que no buscan empleo porque ya han encontrado uno al que se incorporarán dentro de los tres meses posteriores a la dicha semana. Por lo tanto, en este caso no se exige el criterio de búsqueda efectiva de empleo.\n\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha de medición del número de parados.\nTotal: Número de parados en España en la fecha correspondiente.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene el número de parados mediante un modelo Holt Winters.\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#dataset",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#dataset",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado Paro.xlsx. Este dataset presenta los datos de paro por cuatrimestre en España a partir del año 2013, posterior a la crisis económica de 2011.\nCorresponde a la operación estadística del INE 30308 Encuesta de Población Activa (EPA). El objetivo de este estudio es intentar modelizar el número de parados a través de un modelo Holt-Winters y ver si realmente el número de parados se podría aproximar por este modelo sin tener en cuenta variables externas.\nParece razonable aclarar a priori el concepto de “parado” con el fin de entender que es lo que queremos modelizar. De acuerdo con el informe metodológico presentado por el INE:\n\nSe considerarán paradas a todas las personas de 16 o más años que reúnan simultáneamente las siguientes condiciones:\n\nestar sin trabajo, es decir, que no hayan tenido un empleo por cuenta ajena ni por cuenta propia durante la semana de referencia.\nestar buscando trabajo, es decir, que hayan buscado activamente un trabajo por cuenta ajena o hayan hecho gestiones para establecerse por su cuenta durante el mes precedente.\nestar disponibles para trabajar, es decir, en condiciones de comenzar a hacerlo en un plazo de dos semanas a partir del domingo de la semana de referencia.\n\nTambién se consideran paradas las personas de 16 o más años que durante la semana de referencia han estado sin trabajo, disponibles para trabajar y que no buscan empleo porque ya han encontrado uno al que se incorporarán dentro de los tres meses posteriores a la dicha semana. Por lo tanto, en este caso no se exige el criterio de búsqueda efectiva de empleo.\n\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha de medición del número de parados.\nTotal: Número de parados en España en la fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene el número de parados mediante un modelo Holt Winters.\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#librerías",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#librerías",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "Librerías",
    "text": "Librerías\nEn este apartado se van a cargar todas las librerías necesarias para ejecutar el resto del código. Se recomienda instalarlas en caso de no disponer de ellas.\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts\n\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/paro.xlsx\",\n  sheet = \"Datos\",\n  col_types = c(\"date\", \"numeric\")\n)\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 46  2\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [46 × 2] (S3: tbl_df/tbl/data.frame)\n $ Fecha: POSIXct[1:46], format: \"2024-04-01\" \"2024-01-01\" ...\n $ Total: num [1:46] 2755 2978 2861 2894 2808 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#comparación-con-medias-móviles",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#comparación-con-medias-móviles",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "Comparación con Medias Móviles",
    "text": "Comparación con Medias Móviles\n\nAmbos tienen aproximadamente la misma distribución de error de pronóstico cuando \\(\\alpha = 2/(k + 1)\\).\nDifieren en que el suavizado exponencial tiene en cuenta todos las observaciones pasados, mientras que las ** medias móviles** solo tiene en cuenta k observaciones pasadas.\nComputacionalmente hablando, también difieren en que el método de medias móviles requiere que se conserven las últimas \\(k\\) observaciones de datos, mientras que el suavizado exponencial solo necesita conservar el valor suavizado para el \\(t\\) más reciente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#hyperparameter-tunning",
    "href": "notebooks/Series Temporales/Holt Winters/Paro/Paro.html#hyperparameter-tunning",
    "title": "Series Temporales - Holt Winters: Paro",
    "section": "Hyperparameter tunning",
    "text": "Hyperparameter tunning\nAunque en este notebook en la función Holt.Wintersse han dejado los parámetros alpha, betay gamma autoajustarse, un enfoque alternativo podría ser establecer una rejilla de valores creando diferentes modelos con diferentes valores y comparando el MPE y el MAPE para ver que modelo tiene mejores métricas. Se tomarían como adecuados los parámetros del modelo con mejores métricas.\nVeamos un ejemplo, vamos a buscar el gamma óptimo.\n\n# Rejilla de gammas\ngamma &lt;- seq(0.1, 0.9, 0.05)\n\n# Inicializamos\nRMSE &lt;- NA\n\n\n# Bucle que analiza los modelos con todos los gammas propuestos (autoajustando el resto de hiperparámetros)\nfor (i in seq_along(gamma)) {\n  mod1.bucle &lt;- HoltWinters(data.train, gamma = gamma[i], seasonal = \"additive\")\n\n  future &lt;- forecast(mod1.bucle, h = 5)\n  RMSE[i] &lt;- accuracy(future, data.test)[2, 4]\n}\n\n# Copiamos el RMSE de todos\nerror &lt;- data.frame(gamma, RMSE)\n\n# Buscamos el mínimo\nminimum &lt;- error[which(error$RMSE == min(error$RMSE)), ]\n\nggplot(error, aes(gamma, RMSE)) +\n  geom_line() +\n  geom_point(data = minimum, color = \"blue\", size = 2) +\n  ggtitle(\"gamma's impact on forecast errors\",\n    subtitle = \"gamma = 0.65 minimizes RMSE\"\n  )\n\n\n\n\n\n\n\n\nSi actualizamos nuestro modelo con este parámetro “óptimo” de gamma, observamos que el MAPE se queda parecido. Esto representa una mejora pequeña, pero a menudo las mejoras pequeñas pueden tener grandes implicaciones comerciales.\nVeamos concretamente las métricas:\n\nmod1.bucle &lt;- HoltWinters(data.train, gamma = 0.65, seasonal = \"additive\")\nfuture &lt;- forecast(mod1.bucle, h = 6)\naccuracy(future, data.test)\n\n                    ME      RMSE      MAE        MPE     MAPE       MASE\nTraining set  12.68438 134.47703 91.80854  0.4192598 2.463449 0.21388543\nTest set     -17.58313  54.74325 39.10813 -0.6471846 1.357927 0.09110981\n                   ACF1 Theil's U\nTraining set  0.2115799        NA\nTest set     -0.7018512 0.2788822\n\ncheckresiduals(mod1.bucle)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from HoltWinters\nQ* = 8.2535, df = 7, p-value = 0.3108\n\nModel df: 0.   Total lags used: 7\n\n\nVamos a graficarlo a ver que las predicciones futuras:\n\nautoplot(future)",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Paro",
      "Series Temporales - Holt Winters: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "",
    "text": "A continuación se va a explicar como modelizar una serie temporal con un ARIMA y todas las consideraciones que se deben tener en cuenta. Se llevará a cabo un ejemplo práctico a partir de un conjunto de datos, mostrando como interpretar los resultados que se van obteniendo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado Madrid.xlsx. Este dataset presenta los datos de población de la Comunidad Autónoma de Madrid a 1 de Enero y 1 de Julio de cada año desde 1971. La serie temporal se encuentra desagregada por Sexos, mostrando para Hombres, Mujeres y el total de la población. El objetivo de este estudio es intentar modelar dichas series mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nccaa: Madrid siempre.\nsexo: Hombres, Mujeres, Total.\nfecha: Fecha correspondiente.\npoblación: Cifras de población.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene la población de Madrid mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#dataset",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#dataset",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado Madrid.xlsx. Este dataset presenta los datos de población de la Comunidad Autónoma de Madrid a 1 de Enero y 1 de Julio de cada año desde 1971. La serie temporal se encuentra desagregada por Sexos, mostrando para Hombres, Mujeres y el total de la población. El objetivo de este estudio es intentar modelar dichas series mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nccaa: Madrid siempre.\nsexo: Hombres, Mujeres, Total.\nfecha: Fecha correspondiente.\npoblación: Cifras de población.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene la población de Madrid mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#librerías",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#librerías",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "Librerías",
    "text": "Librerías\nEn este apartado se van a cargar todas las librerías necesarias para ejecutar el resto del código. Se recomienda instalarlas en caso de no disponer de ellas.\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras. acf() y pacf()\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts()\nlibrary(tseries) # Para verificar estacionaridad de una serie | función adf.test()\nlibrary(purrr) # para map\n\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/madrid.xlsx\",\n  sheet = \"Datos\", col_types = c(\n    \"date\",\n    \"numeric\", \"numeric\", \"numeric\"\n  )\n)\n\n# Les damos nombre nuevo\ndata$Total &lt;- data$`Pobl. Total`\ndata$H &lt;- data$`Pobl. Hombres`\ndata$M &lt;- data$`Pobl. Mujeres`\n\n# Quitamos columnas que no queremos ya\ndata$`Pobl. Total` &lt;- NULL\ndata$`Pobl. Mujeres` &lt;- NULL\ndata$`Pobl. Hombres` &lt;- NULL\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 104   4\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [104 × 4] (S3: tbl_df/tbl/data.frame)\n $ Fecha: POSIXct[1:104], format: \"2022-07-01\" \"2022-01-01\" ...\n $ Total: num [1:104] 6825005 6769373 6738361 6755828 6757042 ...\n $ H    : num [1:104] 3268934 3243712 3228906 3236830 3238421 ...\n $ M    : num [1:104] 3556072 3525661 3509455 3518998 3518621 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#introducción-1",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#introducción-1",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "Introducción",
    "text": "Introducción\nEl modelo ARIMA es uno de los modelos más comunes y poderosos para el análisis de series temporales. Se utiliza para modelar y predecir datos que exhiben comportamientos de tendencia y estacionalidad.\n\nComponentes del modelo ARIMA:\n\nAR (Auto-regresivo): Representa la relación entre una observación actual y un número determinado de observaciones anteriores (retardos). p es el componente autoregresivo, que indica cuántas observaciones pasadas influyen en la observación actual. Para determinar el valor de p, se puede utilizar el gráfico de la función de autocorrelación parcial (PACF) de la serie diferenciada. Las barras que se salen significativamente del intervalo de confianza pueden indicar el orden de p que debería considerarse. Se recomienda ser conservador y elegir un número reducido de los valores más prominentes.\nI (Integrated):. Representa el número de diferencias necesarias para hacer estacionaria la serie temporal. d es el número de diferenciaciones necesarias para hacer que la serie sea estacionaria. Esto se determina mediante pruebas estadísticas como el test de Dickey-Fuller aumentado (ADF test). Es importante tener cuidado de no sobrediferenciar la serie, lo que se puede observar en el gráfico de la función de autocorrelación (ACF) si los valores comienzan a ser negativos rápidamente.\nMA (Media Móvil): Representa la relación entre una observación actual y un error de predicción residual de observaciones anteriores. q es el componente de media móvil, que indica cuántos términos de los residuos anteriores influyen en la observación actual. Para determinar el valor de q, se utiliza el gráfico de la función de autocorrelación (ACF) de la serie diferenciada. Los términos MA son esencialmente errores de pronóstico retrasados, y el ACF muestra cuántos términos MA se necesitan para eliminar la autocorrelación en la serie estacionaria. Se sugiere seleccionar tantos términos MA como los lags que estén significativamente por encima del intervalo de confianza.\n\n\nSi la serie está ligeramente por debajo del nivel de diferenciación adecuado (subdiferenciada), se pueden agregar uno o más términos de AR adicionales. Por otro lado, si la serie está sobrediferenciada, se puede considerar agregar términos MA adicionales para mejorar el modelo.\nHasta ahora, hemos restringido nuestra atención a datos no estacionales y modelos ARIMA no estacionales. Sin embargo, los modelos ARIMA también son capaces de modelar una amplia gama de datos estacionales.\nUn modelo SARIMA estacional se forma incluyendo términos estacionales adicionales en los modelos ARIMA que hemos visto hasta ahora. Está escrito de la siguiente manera:",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#modelo",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#modelo",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "Modelo",
    "text": "Modelo\nSabiendo que los datos tienen frecuencia bianual y parece razonable pensar que la población puede tener la misma tendencia cada año en los mismos meses (por ej en verano crecer y en invierno decrecer), vamos a considerar diferenciar la serie 2 veces. Es decir, plantearemos un modelo Seasonal ARIMA. La dibujamos de nuevo a ver si es necesario diferenciar una vez más para eliminar la tendencia.\n\n# Series diferenciadas\nt1 &lt;- diff(tss, 2)\nplot(t1)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t1, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t1\nDickey-Fuller = -1.8885, Lag order = 4, p-value = 0.6229\nalternative hypothesis: stationary\n\n\nNo es estacionaria luego valoramos el volver a diferenciar para ver si conseguimos la estacionaridad.\n\n# Diferenciar\nt2 &lt;- diff(t1, 1)\n\n# La dibujamos\nplot(t2)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t2, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t2\nDickey-Fuller = -4.0173, Lag order = 4, p-value = 0.0111\nalternative hypothesis: stationary\n\n\nAl trazar la serie diferenciada, ya vemos un patrón oscilante alrededor de 0, sin una tendencia fuerte visible (aunque vemos que las últimas observaciones aumenta la variabilidad).. Esto sugiere que la diferenciación es suficiente y debe incluirse en el modelo. Además el test de estacionariedad ya lo pasa.\nYa se ha ido la tendencia luego parece razonable pensar que la serie ya es estacionaria aunque la varianza aumente un poco al final, es decir, tomamos D=1, d=1. Examinemos ahora ACF/PACF para determinar los p,q y P,Q.\n\n# ACF plot\nAcf(t2, main = \"ACF para la serie diferenciada 2 vez\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de ACF vemos que hay 1 barra que se salen notablemente del límite deseado (dentro del periodo m=6 meses), luego tomaríamos como q= 1.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de ACF vemos que hay 2 barras que se sale notablemente del límite deseado (dentro de lags múltiplos m=2 ), luego tomaríamos como Q= 1.\n\n\n# PACF plot\n\nPacf(t2, main = \"PACF para la serie diferenciada 2 vez\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de PACF vemos que hay 1 barra que se salen notablemente del límite deseado (dentro del periodo m= 2), luego tomaríamos como p=1.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de PACF vemos que hay 1 barra que se sale notablemente del límite deseado (dentro de lags múltiplos m=2 ), luego tomaríamos como P= 1.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#arima-automático",
    "href": "notebooks/Series Temporales/ARIMA/Madrid/madrid_series.html#arima-automático",
    "title": "Series Temporales - ARIMA: madrid_series",
    "section": "ARIMA automático",
    "text": "ARIMA automático\nExiste una función que permite identificar los parámetros de manera automática. Generalmente se suele usar como primer approach está función para después modificar según evidencia los parámetros.\n\n# Identificar los parámetros del modelo ARIMA de manera automática\nauto_arima &lt;- auto.arima(tss)\nauto_arima\n\nSeries: tss \nARIMA(1,1,1)(1,0,0)[2] with drift \n\nCoefficients:\n         ar1     ma1    sar1     drift\n      0.4767  0.6320  0.7089  33570.24\ns.e.  0.1139  0.1199  0.1354  10083.59\n\nsigma^2 = 1.03e+08:  log likelihood = -1095.41\nAIC=2200.82   AICc=2201.44   BIC=2214\n\n\nNotar que nos propone ARIMA(1,1,1)(1,0,0)[2]. Veamos si esto mejora el modelo (teniendo en cuenta la estacionalidad de los datos).\n\ncheckresiduals(auto_arima)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(1,0,0)[2] with drift\nQ* = 14.334, df = 3, p-value = 0.002484\n\nModel df: 3.   Total lags used: 6\n\n\n\n# Modelo\nauto_arima_train &lt;- arima(data.train, order = c(1, 1, 1), seasonal = c(1, 0, 0))\npred2 &lt;- forecast(auto_arima_train, h = 4)\naccuracy(pred2, data.test)\n\n                    ME     RMSE       MAE       MPE       MAPE       MASE\nTraining set  773.4741  8960.66  5124.491 0.0171775 0.08966484 0.08194831\nTest set     7653.2643 36110.00 26608.662 0.1105397 0.39157880 0.42551245\n                   ACF1 Theil's U\nTraining set 0.03219325        NA\nTest set     0.10527626  1.072081\n\n\n\n# Gráfico con los datos originales y las predicciones de los modelos\nplot(tss,\n  xlim = c(1971, 2021 + 5), ylim = c(min(tss, pred2$pred), max(tss, pred2$pred)),\n  xlab = \"Fecha\", ylab = \"Valor\", main = \"Comparación de Predicciones ARIMA\"\n)\npoints(pred2$mean, col = \"red\", pch = 16, cex = 0.5) # Predicciones con arima_model1 en rojo\nlines(tss, col = \"blue\", lwd = 2) # Serie original en azul\nlines(fitted(auto_arima_train), col = \"red\")\n\nlegend(\"bottomright\", legend = c(\"Original\", \"ARIMA automático \", \"ARIMA manual\"), col = c(\"blue\", \"red\", \"green\"), lty = 1)\n\nlines(pred2$lower[, 2], col = \"red\", lty = \"solid\")\nlines(pred2$upper[, 2], col = \"red\", lty = \"solid\")\n\n\n# ARIMA manual previo\nlines(fitted(arima_model_4), col = \"green\", lwd = 0.5)\npoints(pred$mean, col = \"green\", pch = 1, cex = 0.5) # Predicciones con arima_model1 en rojo\n\nlines(pred$lower[, 2], col = \"green\", lty = \"solid\")\nlines(pred$upper[, 2], col = \"green\", lty = \"solid\")\n\n\n\n\n\n\n\n\nVemos que prácticamente ambos modelos predicen de la misma manera, aunque cambien ligeramente los parámetros de uno y otro. El seleccionado por nosotros parece mejor sin embargo es más complicado. Esto evidencia que como primer approach se puede usar la función auto.arima() obteniendo buenos resultados. No obstante es aconsejable estudiar el problema más a fondo para estudiar todas las posibles modelizaciones y tomar la que mejor se ajuste a nuestras expectativas.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Madrid",
      "Series Temporales - ARIMA: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "",
    "text": "A continuación se va a explicar como modelizar una serie temporal con un ARIMA y todas las consideraciones que se deben tener en cuenta. Se llevará a cabo un ejemplo práctico a partir de un conjunto de datos, mostrando como interpretar los resultados que se van obteniendo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado Paro.xlsx. Este dataset presenta los datos de paro por trimestre en España a partir del año 2013, posterior a la crisis económica de 2011.\nCorresponde a la operación estadística del INE 30308 Encuesta de Población Activa (EPA). El objetivo de este estudio es intentar modelizar el número de parados a través de un modelo ARIMA y ver si realmente el número de parados se podría aproximar por este modelo sin tener en cuenta variables externas.\nParece razonable aclarar a priori el concepto de “parado” con el fin de entender que es lo que queremos modelizar. De acuerdo con el informe metodológico presentado por el INE:\n\nSe considerarán paradas a todas las personas de 16 o más años que reúnan simultáneamente las siguientes condiciones:\n\nestar sin trabajo, es decir, que no hayan tenido un empleo por cuenta ajena ni por cuenta propia durante la semana de referencia.\nestar buscando trabajo, es decir, que hayan buscado activamente un trabajo por cuenta ajena o hayan hecho gestiones para establecerse por su cuenta durante el mes precedente.\nestar disponibles para trabajar, es decir, en condiciones de comenzar a hacerlo en un plazo de dos semanas a partir del domingo de la semana de referencia.\n\nTambién se consideran paradas las personas de 16 o más años que durante la semana de referencia han estado sin trabajo, disponibles para trabajar y que no buscan empleo porque ya han encontrado uno al que se incorporarán dentro de los tres meses posteriores a la dicha semana. Por lo tanto, en este caso no se exige el criterio de búsqueda efectiva de empleo.\n\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha de medición del número de parados.\nTotal: Número de parados en España en la fecha correspondiente.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene el número de parados mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#dataset",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#dataset",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado Paro.xlsx. Este dataset presenta los datos de paro por trimestre en España a partir del año 2013, posterior a la crisis económica de 2011.\nCorresponde a la operación estadística del INE 30308 Encuesta de Población Activa (EPA). El objetivo de este estudio es intentar modelizar el número de parados a través de un modelo ARIMA y ver si realmente el número de parados se podría aproximar por este modelo sin tener en cuenta variables externas.\nParece razonable aclarar a priori el concepto de “parado” con el fin de entender que es lo que queremos modelizar. De acuerdo con el informe metodológico presentado por el INE:\n\nSe considerarán paradas a todas las personas de 16 o más años que reúnan simultáneamente las siguientes condiciones:\n\nestar sin trabajo, es decir, que no hayan tenido un empleo por cuenta ajena ni por cuenta propia durante la semana de referencia.\nestar buscando trabajo, es decir, que hayan buscado activamente un trabajo por cuenta ajena o hayan hecho gestiones para establecerse por su cuenta durante el mes precedente.\nestar disponibles para trabajar, es decir, en condiciones de comenzar a hacerlo en un plazo de dos semanas a partir del domingo de la semana de referencia.\n\nTambién se consideran paradas las personas de 16 o más años que durante la semana de referencia han estado sin trabajo, disponibles para trabajar y que no buscan empleo porque ya han encontrado uno al que se incorporarán dentro de los tres meses posteriores a la dicha semana. Por lo tanto, en este caso no se exige el criterio de búsqueda efectiva de empleo.\n\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha de medición del número de parados.\nTotal: Número de parados en España en la fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene el número de parados mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#librerías",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#librerías",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "Librerías",
    "text": "Librerías\nEn este apartado se van a cargar todas las librerías necesarias para ejecutar el resto del código. Se recomienda instalarlas en caso de no disponer de ellas.\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras. acf() y pacf()\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts()\nlibrary(tseries) # Para verificar estacionaridad de una serie | función adf.test()\nlibrary(purrr) # para map\n\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/paro.xlsx\",\n  sheet = \"Datos\",\n  col_types = c(\"date\", \"numeric\")\n)\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 46  2\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [46 × 2] (S3: tbl_df/tbl/data.frame)\n $ Fecha: POSIXct[1:46], format: \"2024-04-01\" \"2024-01-01\" ...\n $ Total: num [1:46] 2755 2978 2861 2894 2808 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#introducción-1",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#introducción-1",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "Introducción",
    "text": "Introducción\nEl modelo ARIMA es uno de los modelos más comunes y poderosos para el análisis de series temporales. Se utiliza para modelar y predecir datos que exhiben comportamientos de tendencia y estacionalidad.\n\nComponentes del modelo ARIMA:\n\nAR (Auto-regresivo): Representa la relación entre una observación actual y un número determinado de observaciones anteriores (retardos). p es el componente autoregresivo, que indica cuántas observaciones pasadas influyen en la observación actual.\nI (Integrated):. Representa el número de diferencias necesarias para hacer estacionaria la serie temporal. d es el número de diferenciaciones necesarias para hacer que la serie sea estacionaria. Esto se determina mediante pruebas estadísticas como el test de Dickey-Fuller aumentado (ADF test). Es importante tener cuidado de no sobrediferenciar la serie, lo que se puede observar en el gráfico de la función de autocorrelación (ACF) si los valores comienzan a ser negativos rápidamente.\nMA (Media Móvil): Representa la relación entre una observación actual y un error de predicción residual de observaciones anteriores. q es el componente de media móvil, que indica cuántos términos de los residuos anteriores influyen en la observación actual.\n\n\nSi la serie está ligeramente por debajo del nivel de diferenciación adecuado (subdiferenciada), se pueden agregar uno o más términos de AR adicionales. Por otro lado, si la serie está sobrediferenciada, se puede considerar agregar términos MA adicionales para mejorar el modelo.\nHasta ahora, hemos restringido nuestra atención a datos no estacionales y modelos ARIMA no estacionales. Sin embargo, los modelos ARIMA también son capaces de modelar una amplia gama de datos estacionales.\nUn modelo SARIMA estacional se forma incluyendo términos estacionales adicionales en los modelos ARIMA que hemos visto hasta ahora. Está escrito de la siguiente manera:",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#modelo",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#modelo",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "Modelo",
    "text": "Modelo\nSabiendo que los datos tienen frecuencia trimestral y siendo razonable pensar que puede haber una componente estacional ya que de cara a verano se crean más puestos de trabajo (temporales), que luego se destruyen, vamos a considerar un Seasonal ARIMA. Dibujamos la serie de nuevo a ver si es necesario diferenciar una vez más para eliminar la tendencia.\nEn primer lugar, como hemos comentado, parece razonable tener en cuenta la componente estacional luego diferenciamos un número de veces igual al periodo de los datos\n\n# Series diferenciadas\nt1 &lt;- diff(tss, 4)\nplot(t1)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t1, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t1\nDickey-Fuller = -3.275, Lag order = 3, p-value = 0.08903\nalternative hypothesis: stationary\n\n\nNo es estacionaria luego valoramos el diferenciar una vez para ver si conseguimos la estacionaridad.\n\n# Diferenciar\nt2 &lt;- diff(t1, 1)\n\n# La dibujamos\nplot(t2)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t2, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t2\nDickey-Fuller = -4.5648, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nAl trazar la serie diferenciada, ya vemos un patrón oscilante alrededor de 0, sin una tendencia fuerte visible. Esto sugiere que la diferenciación es suficiente y debe incluirse en el modelo. Además el test de estacionariedad ya lo pasa.\nEs decir, tomamos D=1, d=1. Examinemos ahora ACF/PACF para determinar los p,q y P,Q.\n\n# ACF plot\nAcf(t2, main = \"ACF para la serie estacionaria\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de ACF vemos que para lag=1 la barra se sale notablemente del límite deseado, reduciéndose en las sucesivas, luego podríamos tomaríamos como q= 1.\nLags múltiplos de 4 (parámetros estacionales) En el gráfico de ACF vemos que para el lag=4 la barra que se sale del límite deseado, no así para lag=8,16,.. (múltiplos m=4 ), luego tomaríamos como Q= 1.\n\n\n# PACF plot\n\nPacf(t2, main = \"PACF para la serie estacionaria\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de PACF vemos que para lag=1 la barra se sale notablemente del límite deseado, reduciéndose en las sucesivas, luego podríamos tomaríamos como q= 1.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de PACF vemos que para el lag=4 la barra que se sale del límite deseado, no así para lag=8,16,.. (múltiplos m=4 ), luego tomaríamos como P= 1.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#arima-automático",
    "href": "notebooks/Series Temporales/ARIMA/Paro/Paro.html#arima-automático",
    "title": "Series Temporales - ARIMA: Paro",
    "section": "ARIMA automático",
    "text": "ARIMA automático\nExiste una función, ya comentada, que permite identificar los parámetros de manera automática. Generalmente se suele usar como primer approach está función para después modificar según evidencia los parámetros.\n\n# Identificar los parámetros del modelo ARIMA de manera automática\nauto_arima &lt;- auto.arima(tss)\nauto_arima\n\nSeries: tss \nARIMA(0,1,1)(0,1,1)[4] \n\nCoefficients:\n         ma1     sma1\n      0.4958  -0.4556\ns.e.  0.1510   0.1833\n\nsigma^2 = 14920:  log likelihood = -254.79\nAIC=515.59   AICc=516.23   BIC=520.73\n\n\nNotar que nos propone ARIMA(1,1,1)(1,0,0)[4], que es el modelo que hemos usado previamente.\nEl modelo seleccionado por nosotros parece un buen modelo y que predice con poco error. Este era muy cercano all detectado automáticamente por R, luego esto evidencia que como primer approach se puede usar la función auto.arima() obteniendo buenos resultados. No obstante es aconsejable estudiar el problema más a fondo para estudiar todas las posibles modelizaciones y tomar la que mejor se ajuste a nuestras expectativas.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Paro",
      "Series Temporales - ARIMA: Paro"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando un Análisis Factorial. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado ECV_factorial.xlsx. Este dataset presenta un conjunto de microdatos relativos a la Encuesta de Condiciones de Vida (ECV) donde se puntúa el nivel de satisfacción en determinados ámbitos de la vida (laboral, económico, relaciones personales..). El objetivo es llevar a cabo un análisis factorial.\nConcretamente en este dataset tenemos las siguientes variables:\n\nPB030: Identificación transversal de la persona.\nPW010: Grado de satisfacción con su vida en la actualidad.\nPW030: Grado de satisfacción con la situación económica en su hogar.\nPW040: Grado de satisfacción con su vivienda.\nPW100: Grado de satisfacción con su trabajo actual.\nPW120: Grado de satisfacción con el tiempo que dispone para hacer lo que le gusta.\nPW160: Grado de satisfacción con sus relaciones personales.\nPW200: Grado de satisfacción con las áreas recreativas o verdes de la zona en la que vive.\nPW210: Grado de satisfacción con la calidad de la zona en la que vive.\n\nExcepto la primera variable, todas ellas están codificadas desde 0 (Nada satisfecho) hasta 10 (Plenamente satisfecho) y 99 NO sabe.\n\n\n\nSe pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Análisis Factorial.\n\nHacer un análisis exploratorio.\nDividir datos en dos partes\nAnálisis Factorial Exploratorio (EFA)\n\nEstimación matriz correlaciones.\nVerificar que la matriz sea factorizable.\nMétodo de extracción de factores.\nNúmero de factores a extraer.\nExtracción factores + rotación.\n\nAnálisis Factorial Confirmatorio (CFA)\n\nMétodo de extracción de factores.\nProbar una o varias estructuras factoriales.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#dataset",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#dataset",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado ECV_factorial.xlsx. Este dataset presenta un conjunto de microdatos relativos a la Encuesta de Condiciones de Vida (ECV) donde se puntúa el nivel de satisfacción en determinados ámbitos de la vida (laboral, económico, relaciones personales..). El objetivo es llevar a cabo un análisis factorial.\nConcretamente en este dataset tenemos las siguientes variables:\n\nPB030: Identificación transversal de la persona.\nPW010: Grado de satisfacción con su vida en la actualidad.\nPW030: Grado de satisfacción con la situación económica en su hogar.\nPW040: Grado de satisfacción con su vivienda.\nPW100: Grado de satisfacción con su trabajo actual.\nPW120: Grado de satisfacción con el tiempo que dispone para hacer lo que le gusta.\nPW160: Grado de satisfacción con sus relaciones personales.\nPW200: Grado de satisfacción con las áreas recreativas o verdes de la zona en la que vive.\nPW210: Grado de satisfacción con la calidad de la zona en la que vive.\n\nExcepto la primera variable, todas ellas están codificadas desde 0 (Nada satisfecho) hasta 10 (Plenamente satisfecho) y 99 NO sabe.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "",
    "text": "Se pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Análisis Factorial.\n\nHacer un análisis exploratorio.\nDividir datos en dos partes\nAnálisis Factorial Exploratorio (EFA)\n\nEstimación matriz correlaciones.\nVerificar que la matriz sea factorizable.\nMétodo de extracción de factores.\nNúmero de factores a extraer.\nExtracción factores + rotación.\n\nAnálisis Factorial Confirmatorio (CFA)\n\nMétodo de extracción de factores.\nProbar una o varias estructuras factoriales.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerias\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(caret) # train/test data\nlibrary(psych) # test mardia /kmo\nlibrary(ggcorrplot)\nlibrary(GPArotation) # Para poder rotar los factores\nlibrary(lavaan) # cfa\nlibrary(semPlot) # diagrama cfa",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#lectura-datos",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../../files/ECV_factorial.xlsx\", sheet = \"Datos\")\n\nEn primer lugar, cargamos los datos que vamos a utilizar. En este caso, se trata de un conjunto de datos compuesto por 26883 filas y 9 columnas. Las filas corresponden a individuos concretos y las 14 columnas a variables mencionadas previamente.\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores perdidos, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación vemos que si hay valores NA, y por ello los imputaremos con el algoritmo que usemos después.\n\nsum(is.na(datos))\n\n[1] 26341\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(datos)\n\n[1] 26883     9\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [26,883 × 9] (S3: tbl_df/tbl/data.frame)\n $ PB030: num [1:26883] 101 102 103 201 202 301 302 401 402 501 ...\n $ PW010: num [1:26883] 8 8 7 10 10 5 5 8 8 6 ...\n $ PW030: num [1:26883] 8 8 7 4 4 5 8 0 0 6 ...\n $ PW040: num [1:26883] 8 8 7 8 7 6 8 8 8 8 ...\n $ PW100: num [1:26883] 8 NA 7 10 9 NA NA NA NA NA ...\n $ PW120: num [1:26883] 8 8 5 10 6 8 NA 8 5 10 ...\n $ PW160: num [1:26883] 8 8 7 10 10 8 8 8 8 8 ...\n $ PW200: num [1:26883] 8 8 7 5 8 8 8 8 10 8 ...\n $ PW210: num [1:26883] 8 8 7 5 8 7 7 8 8 8 ...\n\n# Descripción de las variables\nsummary(datos)\n\n     PB030             PW010            PW030            PW040       \n Min.   :    101   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 369651   1st Qu.: 6.000   1st Qu.: 5.000   1st Qu.: 6.000  \n Median : 748903   Median : 7.000   Median : 6.000   Median : 8.000  \n Mean   : 783591   Mean   : 6.898   Mean   : 5.801   Mean   : 7.349  \n 3rd Qu.:1180002   3rd Qu.: 8.000   3rd Qu.: 7.000   3rd Qu.: 8.000  \n Max.   :1711204   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n                   NA's   :1377     NA's   :1386     NA's   :1396    \n     PW100            PW120            PW160            PW200      \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 6.000   1st Qu.: 5.000   1st Qu.: 7.000   1st Qu.: 5.00  \n Median : 7.000   Median : 7.000   Median : 8.000   Median : 7.00  \n Mean   : 6.962   Mean   : 6.653   Mean   : 7.821   Mean   : 6.65  \n 3rd Qu.: 8.000   3rd Qu.: 8.000   3rd Qu.: 9.000   3rd Qu.: 8.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n NA's   :16589    NA's   :1408     NA's   :1394     NA's   :1402   \n     PW210       \n Min.   : 0.000  \n 1st Qu.: 6.000  \n Median : 8.000  \n Mean   : 7.251  \n 3rd Qu.: 8.000  \n Max.   :10.000  \n NA's   :1389    \n\n\nComo vamos a hacer un doble Análisis Factorial - Exploratorio y Confirmatorio - vamos a dividir la muestra en aproximadamente el 50%, realizando el Exploratorio sobre una mitad, y el Confirmatorio sobre la otra, para ver si los resultados concuerdan.\n\n# Seed para garantizar la reproducibilidad\nset.seed(2021)\n\n# Partición de datos aleatoria (identificadas mediante la columna de ID del alumno)\nsubconjunto.ids &lt;- createDataPartition(datos$PB030, p = 0.5, list = F)\n\n# Para el Exploratorio solo conservamos las observaciones cuyos IDs han sido muestreados\ndatos_AFE &lt;- datos[subconjunto.ids, ]\ndim(datos_AFE) # 13443 observaciones\n\n[1] 13443     9\n\n# Para el Confirmatorio solo conservamos las observaciones cuyos IDs no han sido muestreados\ndatos_AFC &lt;- datos[-subconjunto.ids, ]\ndim(datos_AFC) # 13440 observaciones\n\n[1] 13440     9\n\n# Eliminamos la columna de IDs, ya que no nos hace falta:\ndatos_AFE$PB030 &lt;- NULL\ndatos_AFC$PB030 &lt;- NULL",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#introducción-1",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Introducción",
    "text": "Introducción\nEl objetivo de la Reducción de la dimensionalidad es reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nEl Análisis Factorial Exploratorio (Exploratory Factorial Analysis) es un método para reducir la dimensionalidad de un conjunto de variables/indicadores, es decir, es un método para resumir la información. En este sentido, puede confundirse con el Análisis de Componentes Principales (PCA).\nEl PCA parte de la varianza total para encontrar combinaciones lineales entre las variables originales que expliquen la mayor parte de dicha varianza total. Así, el primer componente principal sería aquella combinación lineal de variables que explica un mayor porcentaje de varianza total; el segundo componente principal sería aquel que explica la mayor proporción de varianza no explicada por el primero, y así sucesivamente. Al aplicar PCA a un conjunto de datos conseguimos pues resumir la información en unas pocas componentes principales. Dichas componentes principales, al ser combinaciones lineales de variables, resultan, muchas veces, difíciles de interpretar.\nPor su parte, el EFA distingue entre varianza común y varianza específica. La varianza común o compartida es aquella parte de la variabilidad total de los datos que está compartida entre las variables; mientras, la varianza no compartida es la específica de cada variable. Los factores comunes son, por tanto, variables subyacentes - no observadas - que explican la asociación entre las variables.\nAsimismo, desde el punto de vista de las variables, decimos que la comunalidad de una variable es la parte de su varianza explicada por los factores comunes. Por ello, interesa que este dato sea alto. Por el contrario, la especificidad es la parte de variabilidad de una variable específica de dicha variable. Esta última conviene pues que sea baja.\nLos factores sí suelen tener una interpretación más sencilla que los componentes principales. Asimismo, mientras los componentes principales, por definición, no están correlacionados, los factores pueden estarlo.\nEl EFA suele utilizarse con datos procendentes de encuestas sociales, psicológicas o sanitarias, sin embargo, también puede aplicarse a otros tipos de datos. En nuestro caso tenemos datos procedentes de una encuesta de carácter sociodemográfico.\nLos pasos generales son:",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-1-estimación-de-la-matriz-de-correlaciones.",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-1-estimación-de-la-matriz-de-correlaciones.",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 1: Estimación de la matriz de correlaciones.",
    "text": "Paso 1: Estimación de la matriz de correlaciones.\nDependiendo del tipo de datos que tengamos, debemos utilizar un tipo de correlación u otro:\n\nDatos continuos: La matriz de correlaciones de Pearson y la de Spearman son las más apropiadas ante continuos o ante datos ordinales (categóricos) con más de 7 categorías de respuesta (tienden a la continuidad).\nDatos categóricos/ordinales: la matriz de correlaciones policórica es la más recomendada ante datos ordinales de 7 o menos categorías de respuesta. Si tuviéramos datos dicotómicos, la matriz a emplear sería la matriz tetracórica (similar a la policórica pero con datos dicotómicos).\n\nEn nuestro caso, como tenemos datos ordinales con 11 opciones de respuesta (del 0 al 10), usaremos la matriz de correlaciones de Pearson.\n\nmatriz_correlaciones &lt;- cor(datos_AFE, method = \"pearson\", use = \"complete.obs\")\nggcorrplot(matriz_correlaciones, lab = T)\n\n\n\n\n\n\n\n\nTodas las correlaciones son positivas y, en general, no son muy altas.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-2-verificar-que-la-matriz-sea-factorizable-es-decir-que-tiene-sentido-hacer-un-afe-con-nuestros-datos",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-2-verificar-que-la-matriz-sea-factorizable-es-decir-que-tiene-sentido-hacer-un-afe-con-nuestros-datos",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 2: Verificar que la matriz sea factorizable (es decir, que tiene sentido hacer un AFE con nuestros datos):",
    "text": "Paso 2: Verificar que la matriz sea factorizable (es decir, que tiene sentido hacer un AFE con nuestros datos):\nSi la matriz no fuese factorizable, la relación entre las variables sería tan baja que no se formarían factores.\nEntre los métodos para comprobar si la matriz es factorizable destacan:\n\nTest de esfericidad de Bartlett: se pregunta si la matriz de correlaciones es o no la matriz identidad. Si la matriz de correlaciones es la matriz identidad, las variables no estarían correlacionadas y no habría varianza compartida de la que extraer los factores (no habría factores comunes). Si rechazamos la hipótesis nula (la matriz de correlaciones es la matriz identidad), la matriz será factorizable.\nMedida KMO (Kaiser-Meyer-Olkin): expresa el grado de predicción de las variables a partir de las demás. Su valor oscila entre 0 y 1. Cuanto mayor sea, mejor. Valores a partir de 0,5 se consideran aceptables.\nCalcular el determinante de la matriz de correlaciones: valores cercanos a cero indican que la matriz es factorizable.\n\n\n# Test de esfericidad de Bartlett:\np_esf &lt;- cortest.bartlett(matriz_correlaciones, n = nrow(datos_AFE))\ncat(\"El p-valor del test de esfiricidad de Bartlett es: \", p_esf$p.value, \"\\n\")\n\nEl p-valor del test de esfiricidad de Bartlett es:  0 \n\n# KMO:\nKMO(matriz_correlaciones)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = matriz_correlaciones)\nOverall MSA =  0.8\nMSA for each item = \nPW010 PW030 PW040 PW100 PW120 PW160 PW200 PW210 \n 0.81  0.83  0.88  0.89  0.88  0.86  0.67  0.70 \n\n# Determinante de la matriz:\ncat(\"El valor del determinante de la matriz es: \", det(matriz_correlaciones))\n\nEl valor del determinante de la matriz es:  0.08410902\n\n\nEl p-valor es menor a los niveles de significación habituales(10%, 5% y 1%) por lo que rechazamos la hipótesis nula: la matriz de correlaciones no es la matriz identidad.\n\nEl resultado del KMO es bueno: 0,80.\nEl determinante tiene un valor bastante bajo.\nLos test muestran que la matriz es factorizable.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-3-determinar-un-método-de-extracción-de-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-3-determinar-un-método-de-extracción-de-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 3: Determinar un método de extracción de factores:",
    "text": "Paso 3: Determinar un método de extracción de factores:\nLa ecuación del modelo factorial es la siguiente:\n\\[X_j = a_{j1}F_1 + a_{j2}F_2+ ... +a_{j n}F_n + u_j\\]\nDonde \\(X_j\\) (variable j) es una combinación lineal de factores comunes y del factor único (especificidad). Los \\(a_{jh}\\) son los pesos factoriales, que representan la importancia que cada factor común tiene en explicar la variable \\(X_j\\).\nEl objetivo de los métodos de extracción de factores es reproducir, con el mínimo error, la matriz de correlaciones a partir de la matriz de pesos factoriales.\nExisten varios métodos de extracción de factores. Los más comunes son:\n\nMáxima Verosimilitud (Maximum Likelihood, ML). Para utilizarlo, nuestros datos deben seguir una normal multivariante (lo que podemos comprobar realizando el test de Mardia, por ejemplo). Estadísticamente es el más adecuado, pues es asintóticamente insesgado, eficiente y consistente.\nMínimos Cuadrados No Ponderados (Unweighted Least Squares, ULS) (También llamado Residuos Mínimos, MinRes). No requiere de la existencia de normalidad multivariante.\nEjes principales. Tampoco requiere de datos que sigan una normal multivariante.\nComponentes principales. Como hemos dicho en la introducción, no es método de ánalisis factorial propiamente dicho, ya que su objetivo es intentar explicar toda la varianza, y no solo la varianza común o compartida. Sin embargo, su uso, aunque no se recomiende, está bastante extendido, pues es la opción por defecto de muchos programas estadísticos.\n\n\n# Test de Mardia:\nmardia(datos_AFE, na.rm = TRUE)\n\n\n\n\n\n\n\n\nCall: mardia(x = datos_AFE, na.rm = TRUE)\n\nMardia tests of multivariate skew and kurtosis\nUse describe(x) the to get univariate tests\nn.obs = 5157   num.vars =  8 \nb1p =  11.34   skew =  9750.3  with probability  &lt;=  0\n small sample skew =  9757.24  with probability &lt;=  0\nb2p =  130.3   kurtosis =  142.78  with probability &lt;=  0\n\nks.test(datos_AFE, pnorm)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  datos_AFE\nD = 0.95804, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nComo los p-valores son inferiores a los niveles de significación habituales (10%, 5% y 1%) se rechaza la hipótesis nula, es decir, se rechaza la normalidad de los datos. También podemos comprobar este hecho haciendo uso del Q-Q plot, ya que la línea de puntos está bastante alejada de la recta que refleja la normal. O lo que e lo mismo, hay mas de un 5% de datos fuera de la diagonal, leugo con un 95% de confianza podremos decir que no se asemeja con una normal.\nComo no existe normalidad, debemos emplear un método de extracción de factores robusto la inexistencia de la misma, como es ULS (Mínimos Cuadrados No Ponderados).",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-4-determinamos-el-número-de-factores-a-extraer",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-4-determinamos-el-número-de-factores-a-extraer",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 4: Determinamos el número de factores a extraer:",
    "text": "Paso 4: Determinamos el número de factores a extraer:\nExisten cuatro métodos principales:\n\nRegla de Kaiser: se deben aceptar todos aquellos factores con un autovalor mayor a 1.\nGráfico de sedimentación (scree plot): gráfico donde podemos ver cómo disminuye la varianza explicada o el autovalor a medida que vamos aumentando el número de factores a extraer. Se complementa muy bien con la regla de Kaiser.\nEstablecimiento de un porcentaje de varianza explicada mínimo (por ejemplo, el 75%): cogeríamos todos los factores necesarios para explicar ese porcentaje mínimo de varianza.\nAnálisis paralelo (AP, método recomendado): el AP parte de generar nuevas muestras aleatorias (mínimo 100) con el mismo número de variables y observaciones que la muestra original y, a partir de ello, se queda con aquellos factores cuyo autovalor es superior a los generados por azar.\n\nEmplearemos el Análisis Paralelo al ser el método más recomendado.\n\nfa.parallel(matriz_correlaciones, n.obs = nrow(datos_AFE), n.iter = 500, fa = \"fa\", fm = \"uls\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n# uls lleva a cabo un método de minimal residuals\n# fa = \"fa\" show the eigen values for  a principal axis factor analysis\n\nEl Análisis Paralelo sugiere la extracción de 4 factores.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-5-extracción-de-factores-rotación",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-5-extracción-de-factores-rotación",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 5: Extracción de factores + rotación:",
    "text": "Paso 5: Extracción de factores + rotación:\nVamos a extraer tres factores, siguiendo lo recomendado por el Análisis Paralelo.\nAsimismo, debemos decidir si vamos a rotar los factores y qué tipo de rotación vamos a emplear. Los métodos de rotación facilitan la interpretación de los factores, ya que sin rotarlos, suele ser muy difíciles de interpretar, por lo que se recomienda el uso de dichas técnicas de rotación.\nExisten dos tipos de rotación:\n\nRotación oblicua: permite que los factores puedan tener correlación entre ellos. Tipo recomendado, ya que es el más cercano a la estructura real que deseamos explorar. Un ejemplo es el método oblimin.\nRotación ortogonal: fuerza a los factores a que no puedan tener correlación entre ellos. Un ejemplo es el método varimax.\n\nProbaremos los dos y nos quedaremos con la solución que mejor resultado arroje. Pero, ¿cómo determinamos qué solución factorial ajusta mejor?\n\nFijándonos en los pesos factoriales: Cuanto mayor sea el peso factorial de un ítem en un factor mejor. Se aceptan pesos factoriales por encima de 0,4. En general, una buena solución presentará variables que pesan mucho en un factor y poco en el resto.\nÍndices de bondad del ajuste. Existen unos cuantos, pero nos podemos fijar en RMSEA, SRMR, TLI y CFI. RMSEA y SRMR cuantos más bajos mejor (se aceptan valores por debajo de 0,08), y TLI y CFI cuantos más altos mejor (valores por encima de 0,95 se consideran muy buenos).\n\n\n# Rotación oblicua oblimin:\nAFE_oblimin &lt;- fa(matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), rotate = \"oblimin\", fm = \"uls\", alpha = 0.05)\nAFE_oblimin\n\nFactor Analysis using method =  uls\nCall: fa(r = matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), \n    rotate = \"oblimin\", fm = \"uls\", alpha = 0.05)\nStandardized loadings (pattern matrix) based upon correlation matrix\n       ULS1  ULS2  ULS4  ULS3   h2     u2 com\nPW010  0.77 -0.06  0.09 -0.02 0.65 0.3543 1.0\nPW030  0.01  0.01  0.98  0.01 1.00 0.0045 1.0\nPW040  0.49  0.22  0.09 -0.03 0.44 0.5587 1.5\nPW100  0.52  0.01  0.06  0.10 0.38 0.6242 1.1\nPW120  0.01  0.00  0.01  0.99 1.00 0.0045 1.0\nPW160  0.64  0.07 -0.14  0.07 0.39 0.6099 1.2\nPW200 -0.07  0.74  0.05  0.06 0.55 0.4544 1.0\nPW210  0.04  0.92 -0.02 -0.03 0.87 0.1317 1.0\n\n                      ULS1 ULS2 ULS4 ULS3\nSS loadings           1.63 1.51 1.07 1.05\nProportion Var        0.20 0.19 0.13 0.13\nCumulative Var        0.20 0.39 0.53 0.66\nProportion Explained  0.31 0.29 0.20 0.20\nCumulative Proportion 0.31 0.60 0.80 1.00\n\n With factor correlations of \n     ULS1 ULS2 ULS4 ULS3\nULS1 1.00 0.44 0.62 0.40\nULS2 0.44 1.00 0.28 0.26\nULS4 0.62 0.28 1.00 0.29\nULS3 0.40 0.26 0.29 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  28  with the objective function =  2.48 with Chi Square =  33268.91\ndf of  the model are 2  and the objective function was  0.01 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  13443 with the empirical chi square  53.96  with prob &lt;  1.9e-12 \nThe total n.obs was  13443  with Likelihood Chi Square =  111.81  with prob &lt;  5.3e-25 \n\nTucker Lewis Index of factoring reliability =  0.954\nRMSEA index =  0.064  and the 95 % confidence intervals are  0.052 0.076\nBIC =  92.8\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                  ULS1 ULS2 ULS4 ULS3\nCorrelation of (regression) scores with factors   0.90 0.94 1.00 1.00\nMultiple R square of scores with factors          0.81 0.89 1.00 1.00\nMinimum correlation of possible factor scores     0.62 0.78 0.99 0.99\n\nfa.diagram(AFE_oblimin)\n\n\n\n\n\n\n\n\n\n# Rotación ortogonal varimax:\nAFE_varimax &lt;- fa(matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), rotate = \"varimax\", fm = \"uls\", alpha = 0.05)\nAFE_varimax\n\nFactor Analysis using method =  uls\nCall: fa(r = matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), \n    rotate = \"varimax\", fm = \"uls\", alpha = 0.05)\nStandardized loadings (pattern matrix) based upon correlation matrix\n      ULS1 ULS2 ULS4 ULS3   h2     u2 com\nPW010 0.74 0.10 0.27 0.09 0.65 0.3543 1.3\nPW030 0.39 0.13 0.90 0.11 1.00 0.0045 1.4\nPW040 0.54 0.31 0.22 0.06 0.44 0.5587 2.0\nPW100 0.54 0.13 0.19 0.17 0.38 0.6242 1.6\nPW120 0.25 0.13 0.10 0.95 1.00 0.0045 1.2\nPW160 0.58 0.18 0.04 0.14 0.39 0.6099 1.3\nPW200 0.15 0.71 0.09 0.11 0.55 0.4544 1.2\nPW210 0.25 0.89 0.06 0.05 0.87 0.1317 1.2\n\n                      ULS1 ULS2 ULS4 ULS3\nSS loadings           1.77 1.49 1.00 1.00\nProportion Var        0.22 0.19 0.12 0.12\nCumulative Var        0.22 0.41 0.53 0.66\nProportion Explained  0.34 0.28 0.19 0.19\nCumulative Proportion 0.34 0.62 0.81 1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  28  with the objective function =  2.48 with Chi Square =  33268.91\ndf of  the model are 2  and the objective function was  0.01 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  13443 with the empirical chi square  53.96  with prob &lt;  1.9e-12 \nThe total n.obs was  13443  with Likelihood Chi Square =  111.81  with prob &lt;  5.3e-25 \n\nTucker Lewis Index of factoring reliability =  0.954\nRMSEA index =  0.064  and the 95 % confidence intervals are  0.052 0.076\nBIC =  92.8\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                  ULS1 ULS2 ULS4 ULS3\nCorrelation of (regression) scores with factors   0.83 0.92 0.97 0.99\nMultiple R square of scores with factors          0.69 0.86 0.95 0.98\nMinimum correlation of possible factor scores     0.37 0.71 0.90 0.97\n\nfa.diagram(AFE_varimax)",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#resultados-afe",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#resultados-afe",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Resultados AFE:",
    "text": "Resultados AFE:\nEl resultado usando tanto la rotación oblicua como la ortogonal es similar (en la ortogonal vemos como los factores ya no están correlacionados - porque no se les permite estarlo). En ambos casos, el ajuste, siguiendo los índices de bondad del mismo, es muy bueno: RMSEA y RMSR inferiores a 0,08 y TLI mayor a 0,95.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-1-determinar-un-método-de-extracción-de-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-1-determinar-un-método-de-extracción-de-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 1: determinar un método de extracción de factores:",
    "text": "Paso 1: determinar un método de extracción de factores:\nAl igual que hacíamos con el EFA, con el CFA también debemos determinar un método concreto para extraer los factores. Con el fin de establecer un método u otro, llevaremos a cabo un test de Mardia: si existiera normalidad multivariante, podríamos utilizar Máxima Verosimilitud. Si nuestros datos no fuesen normales, Mínimo Cuadrados No Ponderados sería una buena opción.\n\nmardia(datos_AFC, na.rm = TRUE)\n\n\n\n\n\n\n\n\nCall: mardia(x = datos_AFC, na.rm = TRUE)\n\nMardia tests of multivariate skew and kurtosis\nUse describe(x) the to get univariate tests\nn.obs = 5107   num.vars =  8 \nb1p =  11.59   skew =  9861.08  with probability  &lt;=  0\n small sample skew =  9868.16  with probability &lt;=  0\nb2p =  126.63   kurtosis =  131.71  with probability &lt;=  0\n\nks.test(datos_AFC, pnorm)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  datos_AFC\nD = 0.95624, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nComo podíamos esperar los resultados son idénticos a los obtenidos con la mitad de la muestra destinada al EFA: rechazamos la hipótesis nula de normalidad multivariante (al ser los p-valores inferiores a los niveles de significación habituales). Por ello, tampoco podemos usar Máxima Verosimilitud para el CFA, y usaremos, de nuevo, Mínimos Cuadrados No Ponderados (ULS).",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-2-probar-una-o-varias-estructuras-factoriales",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#paso-2-probar-una-o-varias-estructuras-factoriales",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Paso 2: Probar una o varias estructuras factoriales:",
    "text": "Paso 2: Probar una o varias estructuras factoriales:\nVamos a plantear 3 modelos diferentes: uno que replique la estructura factorial planteada en el EFA, otro de un solo factor y otro de dos factores.\nPara comparar modelos nos vamos a fijar en los índices de bondad del ajuste. Al igual que comentamos con el EFA: la situación óptima es aquella en la que los índices SRMR y RMSEA son inferiores a 0,08 - cuanto más bajos mejor -, y los índices TLI y CFI son superiores a 0,95 - cuanto más altos mejor.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-los-cuatro-factores-que-muestra-el-exploratorio",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-los-cuatro-factores-que-muestra-el-exploratorio",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Modelo con los cuatro factores que muestra el Exploratorio:",
    "text": "Modelo con los cuatro factores que muestra el Exploratorio:\n\n# Especificamos el modelo:\nmodelo_4F_AFE &lt;- \"Factor1 =~ PW010 + PW040 + PW100 + PW160\n                  Factor2 =~ PW200 + PW210\n                  Factor3 =~ PW120\n                  Factor4 =~ PW030\"\n# Realizamos el factorial:\nfactorial_4F_AFE &lt;- cfa(modelo_4F_AFE, datos_AFC, estimator = \"ULS\", orthogonal = FALSE)\nsummary(factorial_4F_AFE, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 31 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n                                                  Used       Total\n  Number of observations                          5107       13440\n\nModel Test User Model:\n                                                      \n  Test statistic                              2798.941\n  Degrees of freedom                                16\n  P-value (Unknown)                                 NA\n\nModel Test Baseline Model:\n\n  Test statistic                            244573.959\n  Degrees of freedom                                28\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.989\n  Tucker-Lewis Index (TLI)                       0.980\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.185\n  90 Percent confidence interval - lower         0.179\n  90 Percent confidence interval - upper         0.190\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 =~                                                            \n    PW010             1.000                               1.230    0.729\n    PW040             0.902    0.006  154.059    0.000    1.109    0.651\n    PW100             0.990    0.006  158.608    0.000    1.217    0.619\n    PW160             0.709    0.005  139.082    0.000    0.872    0.552\n  Factor2 =~                                                            \n    PW200             1.000                               1.705    0.741\n    PW210             0.988    0.008  127.508    0.000    1.685    0.912\n  Factor3 =~                                                            \n    PW120             1.000                               2.341    1.000\n  Factor4 =~                                                            \n    PW030             1.000                               1.936    1.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 ~~                                                            \n    Factor2           1.082    0.008  135.914    0.000    0.516    0.516\n    Factor3           1.427    0.009  153.160    0.000    0.496    0.496\n    Factor4           1.610    0.010  165.798    0.000    0.676    0.676\n  Factor2 ~~                                                            \n    Factor3           1.210    0.011  109.986    0.000    0.303    0.303\n    Factor4           1.058    0.011   98.275    0.000    0.320    0.320\n  Factor3 ~~                                                            \n    Factor4           1.399    0.014   99.975    0.000    0.309    0.309\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PW010             1.336    0.019   69.104    0.000    1.336    0.469\n   .PW040             1.676    0.018   92.327    0.000    1.676    0.577\n   .PW100             2.389    0.019  124.496    0.000    2.389    0.617\n   .PW160             1.738    0.016  105.875    0.000    1.738    0.695\n   .PW200             2.383    0.030   78.692    0.000    2.383    0.450\n   .PW210             0.577    0.030   19.415    0.000    0.577    0.169\n   .PW120             0.000                               0.000    0.000\n   .PW030             0.000                               0.000    0.000\n    Factor1           1.512    0.013  113.445    0.000    1.000    1.000\n    Factor2           2.908    0.027  108.321    0.000    1.000    1.000\n    Factor3           5.478    0.014  391.452    0.000    1.000    1.000\n    Factor4           3.748    0.014  267.783    0.000    1.000    1.000\n\nsemPlot::semPaths(factorial_4F_AFE,\n  whatLabels = \"est\",\n  sizeMan = 3.25,\n  node.width = 1,\n  edge.label.cex = .75,\n  style = \"ram\",\n  mar = c(10, 5, 10, 5)\n)\n\n\n\n\n\n\n\n\nSe nos despliega una gran cantidad de información, pero, para facilitar el análisis, nos fijaremos simplemente en los índices de bondad del ajuste. Siguiendo dichos índices, el ajuste parece bueno: SRMR &lt; 0,08, y TLI y CFI &gt; 0,95. Sin embargo, RMSEA es mayor a 0,8",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-un-solo-factor",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-un-solo-factor",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Modelo con un solo factor:",
    "text": "Modelo con un solo factor:\n\n# Planteamos el modelo:\nmodelo_1F &lt;- \"Factor =~ PW010 + PW030 + PW040 + PW100 + PW120 + PW160 + PW200 + PW210\"\n# Realizamos el factorial:\nfactorial_1F_AFE &lt;- cfa(modelo_1F, datos_AFC, estimator = \"ULS\", orthogonal = FALSE)\nsummary(factorial_1F_AFE, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 35 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n                                                  Used       Total\n  Number of observations                          5107       13440\n\nModel Test User Model:\n                                                       \n  Test statistic                              19921.783\n  Degrees of freedom                                 20\n  P-value (Unknown)                                  NA\n\nModel Test Baseline Model:\n\n  Test statistic                            244573.959\n  Degrees of freedom                                28\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.919\n  Tucker-Lewis Index (TLI)                       0.886\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.441\n  90 Percent confidence interval - lower         0.436\n  90 Percent confidence interval - upper         0.447\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.084\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor =~                                                             \n    PW010             1.000                               1.153    0.683\n    PW030             1.036    0.006  160.123    0.000    1.195    0.617\n    PW040             0.933    0.006  154.647    0.000    1.075    0.631\n    PW100             1.002    0.006  158.516    0.000    1.156    0.588\n    PW120             0.995    0.006  158.139    0.000    1.147    0.490\n    PW160             0.737    0.005  139.432    0.000    0.849    0.537\n    PW200             1.021    0.006  159.425    0.000    1.177    0.512\n    PW210             1.017    0.006  159.238    0.000    1.173    0.635\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PW010             1.518    0.018   84.056    0.000    1.518    0.533\n   .PW030             2.321    0.018  126.206    0.000    2.321    0.619\n   .PW040             1.749    0.017  100.078    0.000    1.749    0.602\n   .PW100             2.535    0.018  140.204    0.000    2.535    0.655\n   .PW120             4.162    0.018  231.056    0.000    4.162    0.760\n   .PW160             1.777    0.016  110.391    0.000    1.777    0.711\n   .PW200             3.905    0.018  213.996    0.000    3.905    0.738\n   .PW210             2.040    0.018  112.034    0.000    2.040    0.597\n    Factor            1.330    0.011  116.555    0.000    1.000    1.000\n\nsemPlot::semPaths(factorial_1F_AFE,\n  whatLabels = \"est\",\n  sizeMan = 3.25,\n  node.width = 1,\n  edge.label.cex = .75,\n  style = \"ram\",\n  mar = c(10, 5, 10, 5)\n)\n\n\n\n\n\n\n\n\nLos índices de bondad del ajuste presentan todos peores valores que el modelo de 4 factores obtenido en el AFE: RMSEA = 0,441; SRMR = 0,084; CFI = 0,919; TLI = 0,886",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-dos-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#modelo-con-dos-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Modelo con dos factores:",
    "text": "Modelo con dos factores:\nPlanteamos un modelo con dos factores: uno que englobe los tres items relacionados con la vivienda, y otro que englobe el resto de ítems.\n\n# Planteamos el modelo:\nmodelo_2F &lt;- \"factor_vivienda =~ PW040 + PW200 + PW210\n              factor_miscelanea =~ PW010 + PW030 + PW100 + PW120 + PW160\"\n# Realizamos el factorial:\nfactorial_2F &lt;- cfa(modelo_2F, datos_AFC, estimator = \"ULS\", orthogonal = FALSE)\nsummary(factorial_2F, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 35 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n\n                                                  Used       Total\n  Number of observations                          5107       13440\n\nModel Test User Model:\n                                                       \n  Test statistic                              12204.349\n  Degrees of freedom                                 19\n  P-value (Unknown)                                  NA\n\nModel Test Baseline Model:\n\n  Test statistic                            244573.959\n  Degrees of freedom                                28\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.950\n  Tucker-Lewis Index (TLI)                       0.927\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.354\n  90 Percent confidence interval - lower         0.349\n  90 Percent confidence interval - upper         0.360\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.073\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                       Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  factor_vivienda =~                                                        \n    PW040                 1.000                               1.121    0.658\n    PW200                 1.300    0.009  146.358    0.000    1.458    0.634\n    PW210                 1.306    0.009  146.386    0.000    1.465    0.792\n  factor_miscelanea =~                                                      \n    PW010                 1.000                               1.231    0.729\n    PW030                 1.025    0.007  157.091    0.000    1.261    0.652\n    PW100                 1.000    0.006  156.148    0.000    1.231    0.626\n    PW120                 0.986    0.006  155.560    0.000    1.214    0.519\n    PW160                 0.718    0.005  137.747    0.000    0.884    0.559\n\nCovariances:\n                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  factor_vivienda ~~                                                      \n    factor_misceln      0.920    0.006  142.840    0.000    0.667    0.667\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PW040             1.649    0.019   88.350    0.000    1.649    0.567\n   .PW200             3.166    0.023  138.155    0.000    3.166    0.598\n   .PW210             1.271    0.023   55.195    0.000    1.271    0.372\n   .PW010             1.333    0.019   69.586    0.000    1.333    0.468\n   .PW030             2.157    0.019  110.858    0.000    2.157    0.576\n   .PW100             2.355    0.019  122.927    0.000    2.355    0.608\n   .PW120             4.005    0.019  210.874    0.000    4.005    0.731\n   .PW160             1.717    0.016  104.214    0.000    1.717    0.687\n    factor_viviend    1.257    0.012  101.809    0.000    1.000    1.000\n    factor_misceln    1.515    0.013  115.828    0.000    1.000    1.000\n\nsemPlot::semPaths(factorial_2F,\n  whatLabels = \"est\",\n  sizeMan = 3.25,\n  node.width = 1,\n  edge.label.cex = .75,\n  style = \"ram\",\n  mar = c(10, 5, 10, 5)\n)\n\n\n\n\n\n\n\n\nLos índices de bondad del ajuste presentan todos peores valores que el modelo de 4 factores obtenido en el AFE: RMSEA = 0,354; SRMR = 0,073; CFI = 0,950; TLI = 0,927",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#resultados-cfa",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/ECV_factorial.html#resultados-cfa",
    "title": "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial",
    "section": "Resultados CFA:",
    "text": "Resultados CFA:\nLos índices de bondad del ajuste muestran que, de los tres modelos planteados, el modelo que refleja la estructura de los cuatro factores del AFE es el que mejor ajuste tiene.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: ECV_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando un Análisis Factorial. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado pisa_factorial.xlsx. Este dataset presenta un conjunto de datos relativos a la la prueba PISA del año 2018. El objetivo es realizar un Análisis Factorial a partir de los microdatos.\nConcretamente en este dataset tenemos las siguientes variables (La escala de respuesta para estas son 1 - Totalmente en desacuerdo, 2 - En desacuerdo, 3 - De acuerdo y 4 - Totalmente de acuerdo):\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST181Q02HA: Disfruto trabajando en situaciones que requieren competir con los demás.\nST181Q03HA: Es importante para mí hacerlo mejor que los demás al realizar una tarea.\nST181Q04HA: Me esfuerzo mucho cuando estoy compitiendo contra los demás.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST182Q03HA: Me siento satisfecho cuando me esfuerzo todo lo que puedo.\nST182Q04HA: Cuando inicio una tarea continúo hasta terminarla.\nST182Q05HA: Cuando hago algo, parte de mi satisfacción se debe a que he mejorado mis resultados anteriores.\nST182Q06HA: Si algo no se me da bien, prefiero seguir esforzándome para mejorar, en lugar de hacer otra cosa que sí se me da bien.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones?\n\n\nST183Q01HA: Cuando me he equivocado, me preocupa lo que otras personas piesen de mí.\nST183Q02HA: Cuando me he equivocado, me preocupa no tener el talento suficiente.\nST183Q03HA: Cuando me he equivocado, dudo sobre mis planes para el futuro.\n\n\n\n\nSe pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Análisis Factorial.\n\nHacer un análisis exploratorio.\nDividir datos en dos partes\nAnálisis Factorial Exploratorio (EFA)\n\nEstimación matriz correlaciones.\nVerificar que la matriz sea factorizable.\nMétodo de extracción de factores.\nNúmero de factores a extraer.\nExtracción factores + rotación.\n\nAnálisis Factorial Confirmatorio (CFA)\n\nMétodo de extracción de factores.\nProbar una o varias estructuras factoriales.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#dataset",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#dataset",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado pisa_factorial.xlsx. Este dataset presenta un conjunto de datos relativos a la la prueba PISA del año 2018. El objetivo es realizar un Análisis Factorial a partir de los microdatos.\nConcretamente en este dataset tenemos las siguientes variables (La escala de respuesta para estas son 1 - Totalmente en desacuerdo, 2 - En desacuerdo, 3 - De acuerdo y 4 - Totalmente de acuerdo):\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST181Q02HA: Disfruto trabajando en situaciones que requieren competir con los demás.\nST181Q03HA: Es importante para mí hacerlo mejor que los demás al realizar una tarea.\nST181Q04HA: Me esfuerzo mucho cuando estoy compitiendo contra los demás.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST182Q03HA: Me siento satisfecho cuando me esfuerzo todo lo que puedo.\nST182Q04HA: Cuando inicio una tarea continúo hasta terminarla.\nST182Q05HA: Cuando hago algo, parte de mi satisfacción se debe a que he mejorado mis resultados anteriores.\nST182Q06HA: Si algo no se me da bien, prefiero seguir esforzándome para mejorar, en lugar de hacer otra cosa que sí se me da bien.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones?\n\n\nST183Q01HA: Cuando me he equivocado, me preocupa lo que otras personas piesen de mí.\nST183Q02HA: Cuando me he equivocado, me preocupa no tener el talento suficiente.\nST183Q03HA: Cuando me he equivocado, dudo sobre mis planes para el futuro.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "",
    "text": "Se pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Análisis Factorial.\n\nHacer un análisis exploratorio.\nDividir datos en dos partes\nAnálisis Factorial Exploratorio (EFA)\n\nEstimación matriz correlaciones.\nVerificar que la matriz sea factorizable.\nMétodo de extracción de factores.\nNúmero de factores a extraer.\nExtracción factores + rotación.\n\nAnálisis Factorial Confirmatorio (CFA)\n\nMétodo de extracción de factores.\nProbar una o varias estructuras factoriales.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerias\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(caret) # train/test data\nlibrary(psych) # test mardia /kmo\nlibrary(ggcorrplot)\nlibrary(GPArotation) # Para poder rotar los factores\nlibrary(lavaan) # cfa\nlibrary(semPlot) # diagrama cfa",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#lectura-datos",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../../files/pisa_factorial.xlsx\", sheet = \"Datos\")\n\nEn primer lugar, cargamos los datos que vamos a utilizar. En este caso, se trata de un conjunto de datos compuesto por 35943 filas y 11 columnas. Las filas corresponden a individuos concertos y las 11 columnas a variables mencionadas previamente.\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores perdidos, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación vemos que si hay valores NA, y por ello los imputaremos con el algoritmo que usemos después.\n\nsum(is.na(datos))\n\n[1] 20958\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [35,943 × 11] (S3: tbl_df/tbl/data.frame)\n $ CNTSTUID  : num [1:35943] 72400490 72401482 72402362 72402959 72403316 ...\n $ ST181Q02HA: num [1:35943] 3 3 4 4 3 3 3 3 4 4 ...\n $ ST181Q03HA: num [1:35943] 3 2 2 2 1 2 3 4 2 2 ...\n $ ST181Q04HA: num [1:35943] 3 3 3 4 2 3 4 3 3 3 ...\n $ ST182Q03HA: num [1:35943] 3 4 4 3 4 4 4 4 4 3 ...\n $ ST182Q04HA: num [1:35943] 3 4 4 3 3 3 3 3 3 2 ...\n $ ST182Q05HA: num [1:35943] 3 4 4 2 3 3 4 4 4 3 ...\n $ ST182Q06HA: num [1:35943] 3 3 3 3 3 3 4 3 3 3 ...\n $ ST183Q01HA: num [1:35943] 2 2 3 4 3 2 3 3 2 3 ...\n $ ST183Q02HA: num [1:35943] 2 2 3 3 2 3 4 2 2 3 ...\n $ ST183Q03HA: num [1:35943] 3 2 3 4 2 2 3 1 2 2 ...\n\n# Descripción de las variables\nsummary(datos)\n\n    CNTSTUID          ST181Q02HA      ST181Q03HA      ST181Q04HA   \n Min.   :72400001   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:72411570   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:3.000  \n Median :72423133   Median :3.000   Median :3.000   Median :3.000  \n Mean   :72423133   Mean   :2.792   Mean   :2.607   Mean   :2.999  \n 3rd Qu.:72434686   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:4.000  \n Max.   :72446162   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n                    NA's   :1835    NA's   :1884    NA's   :1961   \n   ST182Q03HA      ST182Q04HA     ST182Q05HA      ST182Q06HA      ST183Q01HA   \n Min.   :1.000   Min.   :1.00   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:3.00   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :4.000   Median :3.00   Median :3.000   Median :3.000   Median :3.000  \n Mean   :3.468   Mean   :2.97   Mean   :3.275   Mean   :2.908   Mean   :2.517  \n 3rd Qu.:4.000   3rd Qu.:3.00   3rd Qu.:4.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :4.000   Max.   :4.00   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n NA's   :2033    NA's   :2123   NA's   :2208    NA's   :2247    NA's   :2148   \n   ST183Q02HA      ST183Q03HA  \n Min.   :1.000   Min.   :1.00  \n 1st Qu.:2.000   1st Qu.:2.00  \n Median :3.000   Median :2.00  \n Mean   :2.523   Mean   :2.43  \n 3rd Qu.:3.000   3rd Qu.:3.00  \n Max.   :4.000   Max.   :4.00  \n NA's   :2205    NA's   :2314  \n\n\nComo vamos a hacer un doble Análisis Factorial - Exploratorio y Confirmatorio - vamos a dividir la muestra en aproximadamente el 50%, realizando el Exploratorio sobre una mitad, y el Confirmatorio sobre la otra, para ver si los resultados concuerdan.\n\n# Extraer columnas de identificador del alumno y  de los items que(miden la competitividad, la perseverancia y el miedo al fracaso):\n\nsubconjunto &lt;- datos[, c(\"CNTSTUID\", \"ST181Q02HA\", \"ST181Q03HA\", \"ST181Q04HA\", \"ST182Q03HA\", \"ST182Q04HA\", \"ST182Q05HA\", \"ST182Q06HA\", \"ST183Q01HA\", \"ST183Q02HA\", \"ST183Q03HA\")]\n\n# Seed para garantizar la reproducibilidad\nset.seed(2021)\n# Partición de datos\nsubconjunto.ids &lt;- createDataPartition(subconjunto$CNTSTUID, p = 0.5, list = F)\n\n# Para el Exploratorio solo conservamos las observaciones cuyos IDs han sido muestreados\ndatos_AFE &lt;- subconjunto[subconjunto.ids, ]\ndim(datos_AFE) # 17972 observaciones\n\n[1] 17972    11\n\n# Para el Confirmatorio solo conservamos las observaciones cuyos IDs no han sido muestreados\ndatos_AFC &lt;- subconjunto[-subconjunto.ids, ]\ndim(datos_AFC) # 17971 observaciones\n\n[1] 17971    11\n\n# Eliminamos IDs, ya que no hacen falta:\ndatos_AFE$CNTSTUID &lt;- NULL\ndatos_AFC$CNTSTUID &lt;- NULL",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#introducción-1",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Introducción",
    "text": "Introducción\nEl objetivo de la Reducción de la dimensionalidad es reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nEl Análisis Factorial Exploratorio (Exploratory Factorial Analysis) es un método para reducir la dimensionalidad de un conjunto de variables/indicadores, es decir, es un método para resumir la información. En este sentido, puede confundirse con el Análisis de Componentes Principales (PCA).\nEl PCA parte de la varianza total para encontrar combinaciones lineales entre las variables originales que expliquen la mayor parte de dicha varianza total. Así, el primer componente principal sería aquella combinación lineal de variables que explica un mayor porcentaje de varianza total; el segundo componente principal sería aquel que explica la mayor proporción de varianza no explicada por el primero, y así sucesivamente. Al aplicar PCA a un conjunto de datos conseguimos pues resumir la información en unas pocas componentes principales. Dichas componentes principales, al ser combinaciones lineales de variables, resultan, muchas veces, difíciles de interpretar.\nPor su parte, el EFA distingue entre varianza común y varianza específica. La varianza común o compartida es aquella parte de la variabilidad total de los datos que está compartida entre las variables; mientras, la varianza no compartida es la específica de cada variable. Los factores comunes son, por tanto, variables subyacentes - no observadas - que explican la asociación entre las variables.\nAsimismo, desde el punto de vista de las variables, decimos que la comunalidad de una variable es la parte de su varianza explicada por los factores comunes. Por ello, interesa que este dato sea alto. Por el contrario, la especificidad es la parte de variabilidad de una variable específica de dicha variable. Esta última conviene pues que sea baja.\nLos factores sí suelen tener una interpretación más sencilla que los componentes principales. Asimismo, mientras los componentes principales, por definición, no están correlacionados, los factores pueden estarlo.\nEl EFA suele utilizarse con datos procendentes de encuestas sociales, psicológicas o sanitarias, sin embargo, también puede aplicarse a otros tipos de datos. En nuestro caso tenemos datos procedentes de una encuesta de carácter sociodemográfico.\nLos pasos generales son:",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-1-estimación-de-la-matriz-de-correlaciones.",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-1-estimación-de-la-matriz-de-correlaciones.",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 1: Estimación de la matriz de correlaciones.",
    "text": "Paso 1: Estimación de la matriz de correlaciones.\nDependiendo del tipo de datos que tengamos, debemos utilizar un tipo de correlación u otro:\n\nDatos continuos: La matriz de correlaciones de Pearson y la de Spearman son las más apropiadas ante continuos o ante datos ordinales (categóricos) con más de 7 categorías de respuesta (tienden a la continuidad).\nDatos categóricos/ordinales: la matriz de correlaciones policórica es la más recomendada ante datos ordinales de 7 o menos categorías de respuesta. Si tuviéramos datos dicotómicos, la matriz a emplear sería la matriz tetracórica (similar a la policórica pero con datos dicotómicos).\n\nEn nuestro caso, como tenemos datos ordinales con 11 opciones de respuesta (del 0 al 10), usaremos la matriz de correlaciones de Pearson.\n\nmatriz_correlaciones &lt;- cor(datos_AFE, method = \"pearson\", use = \"complete.obs\")\nggcorrplot(matriz_correlaciones, lab = T, type = \"lower\")\n\n\n\n\n\n\n\n\nVemos que las correlaciones son más altas entre los ítems que conforman cada uno de los subgrupos teóricos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-2-verificar-que-la-matriz-sea-factorizable-es-decir-que-tiene-sentido-hacer-un-afe-con-nuestros-datos",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-2-verificar-que-la-matriz-sea-factorizable-es-decir-que-tiene-sentido-hacer-un-afe-con-nuestros-datos",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 2: Verificar que la matriz sea factorizable (es decir, que tiene sentido hacer un AFE con nuestros datos):",
    "text": "Paso 2: Verificar que la matriz sea factorizable (es decir, que tiene sentido hacer un AFE con nuestros datos):\nSi la matriz no fuese factorizable, la relación entre las variables sería tan baja que no se formarían factores.\nEntre los métodos para comprobar si la matriz es factorizable destacan:\n\nTest de esfericidad de Bartlett: se pregunta si la matriz de correlaciones es o no la matriz identidad. Si la matriz de correlaciones es la matriz identidad, las variables no estarían correlacionadas y no habría varianza compartida de la que extraer los factores (no habría factores comunes). Si rechazamos la hipótesis nula (la matriz de correlaciones es la matriz identidad), la matriz será factorizable.\nMedida KMO (Kaiser-Meyer-Olkin): expresa el grado de predicción de las variables a partir de las demás. Su valor oscila entre 0 y 1. Cuanto mayor sea, mejor. Valores a partir de 0,5 se consideran aceptables.\nCalcular el determinante de la matriz de correlaciones: valores cercanos a cero indican que la matriz es factorizable.\n\n\n# Test de esfericidad de Bartlett:\np_esf &lt;- cortest.bartlett(matriz_correlaciones, n = nrow(datos_AFE))\ncat(\"El p-valor del test de esfericidad de Bartlett es: \", p_esf$p.value, \"\\n\")\n\nEl p-valor del test de esfericidad de Bartlett es:  0 \n\n# KMO:\nKMO(matriz_correlaciones)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = matriz_correlaciones)\nOverall MSA =  0.73\nMSA for each item = \nST181Q02HA ST181Q03HA ST181Q04HA ST182Q03HA ST182Q04HA ST182Q05HA ST182Q06HA \n      0.74       0.76       0.72       0.76       0.77       0.77       0.79 \nST183Q01HA ST183Q02HA ST183Q03HA \n      0.74       0.62       0.65 \n\n# Determinante de la matriz:\ncat(\"El valor del determinante de la matriz es: \", det(matriz_correlaciones))\n\nEl valor del determinante de la matriz es:  0.07879726\n\n\n1 . El p-valor es menor a los niveles de significación habituales(10%, 5% y 1%) por lo que rechazamos la hipótesis nula: la matriz de correlaciones no es la matriz identidad.\n\nEl resultado del KMO es bueno: 0,73.\nEl determinante tiene un valor bastante bajo.\n\nLos test muestran que la matriz es factorizable.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-3-determinar-un-método-de-extracción-de-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-3-determinar-un-método-de-extracción-de-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 3: Determinar un método de extracción de factores:",
    "text": "Paso 3: Determinar un método de extracción de factores:\nLa ecuación del modelo factorial es la siguiente:\n\\[X_j = a_{j1}F_1 + a_{j2}F_2+ ... +a_{j n}F_n + u_j\\]\nDonde \\(X_j\\) (variable j) es una combinación lineal de factores comunes y del factor único (especificidad). Los \\(a_{jh}\\) son los pesos factoriales, que representan la importancia que cada factor común tiene en explicar la variable \\(X_j\\).\nEl objetivo de los métodos de extracción de factores es reproducir, con el mínimo error, la matriz de correlaciones a partir de la matriz de pesos factoriales.\nExisten varios métodos de extracción de factores. Los más comunes son:\n\nMáxima Verosimilitud (Maximum Likelihood, ML). Para utilizarlo, nuestros datos deben seguir una normal multivariante (lo que podemos comprobar realizando el test de Mardia, por ejemplo). Estadísticamente es el más adecuado, pues es asintóticamente insesgado, eficiente y consistente.\nMínimos Cuadrados No Ponderados (Unweighted Least Squares, ULS) (También llamado Residuos Mínimos, MinRes). No requiere de la existencia de normalidad multivariante.\nEjes principales. Tampoco requiere de datos que sigan una normal multivariante.\nComponentes principales. Como hemos dicho en la introducción, no es método de análisis factorial propiamente dicho, ya que su objetivo es intentar explicar toda la varianza, y no solo la varianza común o compartida. Sin embargo, su uso, aunque no se recomiende, está bastante extendido, pues es la opción por defecto de muchos programas estadísticos.\n\n\n# Test de Mardia:\nmardia(datos_AFE, na.rm = TRUE)\n\n\n\n\n\n\n\n\nCall: mardia(x = datos_AFE, na.rm = TRUE)\n\nMardia tests of multivariate skew and kurtosis\nUse describe(x) the to get univariate tests\nn.obs = 16232   num.vars =  10 \nb1p =  3.92   skew =  10595.01  with probability  &lt;=  0\n small sample skew =  10597.33  with probability &lt;=  0\nb2p =  140.16   kurtosis =  82.89  with probability &lt;=  0\n\nks.test(datos_AFE, pnorm)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  datos_AFE\nD = 0.88912, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nComo los p-valores son inferiores a los niveles de significación habituales (10%, 5% y 1%) se rechaza la hipótesis nula, es decir, se rechaza la normalidad de los datos.\nTambién podemos comprobar este hecho haciendo uso del Q-Q plot, ya que la línea de puntos está bastante alejada de la recta que refleja la normal.\nComo no existe normalidad, debemos emplear un método de extracción de factores robusto, como es ULS (Mínimos Cuadrados No Ponderados).",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-4-determinamos-el-número-de-factores-a-extraer",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-4-determinamos-el-número-de-factores-a-extraer",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 4: Determinamos el número de factores a extraer:",
    "text": "Paso 4: Determinamos el número de factores a extraer:\nExisten cuatro métodos principales:\n\nRegla de Kaiser: se deben aceptar todos aquellos factores con un autovalor mayor a 1.\nGráfico de sedimentación (scree plot): gráfico donde podemos ver cómo disminuye la varianza explicada o el autovalor a medida que vamos aumentando el número de factores a extraer. Se complementa muy bien con la regla de Kaiser.\nEstablecimiento de un porcentaje de varianza explicada mínimo (por ejemplo, el 75%): cogeríamos todos los factores necesarios para explicar ese porcentaje mínimo de varianza.\nAnálisis paralelo (AP, método recomendado): el AP parte de generar nuevas muestras aleatorias (mínimo 100) con el mismo número de variables y observaciones que la muestra original y, a partir de ello, se queda con aquellos factores cuyo autovalor es superior a los generados por azar.\n\nEmplearemos el Análisis Paralelo al ser el método más recomendado.\n\n# Gráfico de sedimentación con la regla de Kaiser marcada:\nscree(matriz_correlaciones)\n\n\n\n\n\n\n\n# Análisis paralelo:\n\nfa.parallel(matriz_correlaciones, n.obs = nrow(datos_AFE), n.iter = 500, fa = \"fa\", fm = \"uls\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  NA \n\n# uls lleva a cabo un método de minimal residuals\n# fa = \"fa\" show the eigen values for  a principal axis factor analysis\n\nEl Análisis Paralelo sugiere la extracción de tres factores, la regla de Kaiser, de 3, y el gráfico de sedimentación podríamos decir que de 3 ó 4.\nHaremos caso al Análisis Paralelo, por ser la técnica más recomendada actualmente.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-5-extracción-de-factores-rotación",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-5-extracción-de-factores-rotación",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 5: Extracción de factores + rotación:",
    "text": "Paso 5: Extracción de factores + rotación:\nVamos a extraer tres factores, siguiendo lo recomendado por el Análisis Paralelo.\nAsimismo, debemos decidir si vamos a rotar los factores y qué tipo de rotación vamos a emplear. Los métodos de rotación facilitan la interpretación de los factores, ya que sin rotarlos, suele ser muy difíciles de interpretar, por lo que se recomienda el uso de dichas técnicas de rotación.\nExisten dos tipos de rotación:\n\nRotación oblicua: permite que los factores puedan tener correlación entre ellos. Tipo recomendado, ya que es el más cercano a la estructura real que deseamos explorar. Un ejemplo es el método oblimin.\nRotación ortogonal: fuerza a los factores a que no puedan tener correlación entre ellos. Un ejemplo es el método varimax.\n\nProbaremos los dos y nos quedaremos con la solución que mejor resultado arroje. Pero, ¿cómo determinamos qué solución factorial ajusta mejor?\n\nFijándonos en los pesos factoriales: Cuanto mayor sea el peso factorial de un ítem en un factor mejor. Se aceptan pesos factoriales por encima de 0,4. En general, una buena solución presentará variables que pesan mucho en un factor y poco en el resto.\nÍndices de bondad del ajuste. Existen unos cuantos, pero nos podemos fijar en RMSEA, SRMR, TLI y CFI. RMSEA y SRMR cuantos más bajos mejor (se aceptan valores por debajo de 0,08), y TLI y CFI cuantos más altos mejor (valores por encima de 0,95 se consideran muy buenos).\n\n\n# Rotación oblicua oblimin:\nAFE_oblimin &lt;- fa(matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), rotate = \"oblimin\", fm = \"uls\", alpha = 0.05)\nAFE_oblimin\n\nFactor Analysis using method =  uls\nCall: fa(r = matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), \n    rotate = \"oblimin\", fm = \"uls\", alpha = 0.05)\nStandardized loadings (pattern matrix) based upon correlation matrix\n            ULS3  ULS2  ULS1  ULS4   h2     u2 com\nST181Q02HA  0.72 -0.07 -0.04  0.04 0.51 0.4940 1.0\nST181Q03HA  0.68  0.08  0.04 -0.06 0.47 0.5275 1.0\nST181Q04HA  0.79  0.00  0.02  0.01 0.64 0.3636 1.0\nST182Q03HA  0.00  0.00  0.01  0.99 1.00 0.0048 1.0\nST182Q04HA  0.00 -0.01  0.68  0.01 0.48 0.5224 1.0\nST182Q05HA  0.05  0.06  0.51  0.19 0.44 0.5609 1.3\nST182Q06HA  0.00 -0.03  0.67 -0.06 0.41 0.5948 1.0\nST183Q01HA  0.07  0.56 -0.07  0.10 0.34 0.6615 1.1\nST183Q02HA  0.00  0.90  0.02 -0.01 0.81 0.1859 1.0\nST183Q03HA -0.03  0.69 -0.03 -0.02 0.47 0.5324 1.0\n\n                      ULS3 ULS2 ULS1 ULS4\nSS loadings           1.62 1.62 1.23 1.08\nProportion Var        0.16 0.16 0.12 0.11\nCumulative Var        0.16 0.32 0.45 0.56\nProportion Explained  0.29 0.29 0.22 0.19\nCumulative Proportion 0.29 0.58 0.81 1.00\n\n With factor correlations of \n     ULS3 ULS2 ULS1 ULS4\nULS3 1.00 0.12 0.30 0.23\nULS2 0.12 1.00 0.02 0.04\nULS1 0.30 0.02 1.00 0.54\nULS4 0.23 0.04 0.54 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  45  with the objective function =  2.54 with Chi Square =  45651.51\ndf of  the model are 11  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0.01 \n\nThe harmonic n.obs is  17972 with the empirical chi square  29.63  with prob &lt;  0.0018 \nThe total n.obs was  17972  with Likelihood Chi Square =  57.84  with prob &lt;  2.3e-08 \n\nTucker Lewis Index of factoring reliability =  0.996\nRMSEA index =  0.015  and the 95 % confidence intervals are  0.011 0.02\nBIC =  -49.93\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                  ULS3 ULS2 ULS1 ULS4\nCorrelation of (regression) scores with factors   0.89 0.92 0.85 1.00\nMultiple R square of scores with factors          0.79 0.85 0.72 1.00\nMinimum correlation of possible factor scores     0.58 0.71 0.45 0.99\n\nfa.diagram(AFE_oblimin)\n\n\n\n\n\n\n\n\n\nVemos que los items que más pesan en el factor 1 son los correspondientes a la variable Perseverancia - los que comienzan por ST182.\nLos items que más pesan en el factor 2 son los correspondientes a la variable Miedo al fracaso - los que comienzan por ST183.\nLos ítems que más pesan en el factor 3 son los correspondientes a la variable Competitividad - los que comienzan por ST181.\nLos índices de bondad del ajuste son muy buenos: RMSEA y RMSR inferiores a 0,08 y TLI mayor a 0,95\nEn el gráfico vemos que los factores 1 y 3 (perseverancia y competitividad) están ligeramente correlacionados.\n\n\n# Rotación ortogonal varimax:\nAFE_varimax &lt;- fa(matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), rotate = \"varimax\", fm = \"uls\", alpha = 0.05)\nAFE_varimax\n\nFactor Analysis using method =  uls\nCall: fa(r = matriz_correlaciones, nfactors = 4, n.obs = nrow(datos_AFE), \n    rotate = \"varimax\", fm = \"uls\", alpha = 0.05)\nStandardized loadings (pattern matrix) based upon correlation matrix\n            ULS2 ULS3  ULS1  ULS4   h2     u2 com\nST181Q02HA -0.01 0.70  0.08  0.08 0.51 0.4940 1.1\nST181Q03HA  0.12 0.67  0.12 -0.01 0.47 0.5275 1.1\nST181Q04HA  0.06 0.78  0.14  0.06 0.64 0.3636 1.1\nST182Q03HA  0.04 0.10  0.41  0.90 1.00 0.0048 1.4\nST182Q04HA -0.01 0.10  0.68  0.11 0.48 0.5224 1.1\nST182Q05HA  0.06 0.15  0.59  0.25 0.44 0.5609 1.5\nST182Q06HA -0.04 0.09  0.63  0.04 0.41 0.5948 1.1\nST183Q01HA  0.57 0.09 -0.01  0.08 0.34 0.6615 1.1\nST183Q02HA  0.90 0.04  0.04 -0.02 0.81 0.1859 1.0\nST183Q03HA  0.68 0.00 -0.02 -0.03 0.47 0.5324 1.0\n\n                      ULS2 ULS3 ULS1 ULS4\nSS loadings           1.62 1.61 1.41 0.91\nProportion Var        0.16 0.16 0.14 0.09\nCumulative Var        0.16 0.32 0.46 0.56\nProportion Explained  0.29 0.29 0.25 0.16\nCumulative Proportion 0.29 0.58 0.84 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  45  with the objective function =  2.54 with Chi Square =  45651.51\ndf of  the model are 11  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0.01 \n\nThe harmonic n.obs is  17972 with the empirical chi square  29.63  with prob &lt;  0.0018 \nThe total n.obs was  17972  with Likelihood Chi Square =  57.84  with prob &lt;  2.3e-08 \n\nTucker Lewis Index of factoring reliability =  0.996\nRMSEA index =  0.015  and the 95 % confidence intervals are  0.011 0.02\nBIC =  -49.93\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                  ULS2 ULS3 ULS1 ULS4\nCorrelation of (regression) scores with factors   0.92 0.88 0.81 0.96\nMultiple R square of scores with factors          0.85 0.77 0.66 0.93\nMinimum correlation of possible factor scores     0.70 0.54 0.32 0.85\n\nfa.diagram(AFE_varimax)\n\n\n\n\n\n\n\n\nLos resultados son muy similares a los obtenidos con la rotación oblicua:\n\nVemos que los items que más pesan en el factor 1 son los correspondientes a la variable Perseverancia - los que comienzan por ST182.\nLos items que más pesan en el factor 2 son los correspondientes a la variable Miedo al fracaso - los que comienzan por ST183.\nLos ítems que más pesan en el factor 3 son los correspondientes a la variable Competitividad - los que comienzan por ST181.\nLos índices de bondad del ajuste son muy buenos: RMSEA y RMSR inferiores a 0,08 y TLI mayor a 0,95\nEn el gráfico vemos que los factores 1 y 3 ya no están correlacionados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#resultados-afe",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#resultados-afe",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Resultados AFE:",
    "text": "Resultados AFE:\nLa estructura factorial obtenida concuerda con la estructura factorial teórica de tres factores",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-1-determinar-un-método-de-extracción-de-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-1-determinar-un-método-de-extracción-de-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 1: determinar un método de extracción de factores:",
    "text": "Paso 1: determinar un método de extracción de factores:\nAl igual que hacíamos con el EFA, con el CFA también debemos determinar un método concreto para extraer los factores. Con el fin de establecer un método u otro, llevaremos a cabo un test de Mardia: si existiera normalidad multivariante, podríamos utilizar Máxima Verosimilitud. Si nuestros datos no fuesen normales, Mínimo Cuadrados No Ponderados sería una buena opción.\n\nmardia(datos_AFC, na.rm = TRUE)\n\n\n\n\n\n\n\n\nCall: mardia(x = datos_AFC, na.rm = TRUE)\n\nMardia tests of multivariate skew and kurtosis\nUse describe(x) the to get univariate tests\nn.obs = 16255   num.vars =  10 \nb1p =  3.68   skew =  9980.25  with probability  &lt;=  0\n small sample skew =  9982.43  with probability &lt;=  0\nb2p =  140.86   kurtosis =  85.84  with probability &lt;=  0\n\nks.test(datos_AFC, pnorm)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  datos_AFC\nD = 0.88958, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nComo podíamos esperar los resultados son idénticos a los obtenidos con la mitad de la muestra destinada al AFE: rechazamos la hipótesis nula de normalidad multivariante (al ser los p-valores inferiores a los niveles de significación habituales). Por ello, tampoco podemos usar Máxima Verosimilitud para el AFC, y usaremos, de nuevo, Mínimos Cuadrados No Ponderados (ULS).",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-2-probar-una-o-varias-estructuras-factoriales",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#paso-2-probar-una-o-varias-estructuras-factoriales",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Paso 2: Probar una o varias estructuras factoriales:",
    "text": "Paso 2: Probar una o varias estructuras factoriales:\nVamos a plantear 3 modelos diferentes: uno que replique la estructura factorial planteada en el EFA, otro de un solo factor y otro de dos factores.\nPara comparar modelos nos vamos a fijar en los índices de bondad del ajuste. Al igual que comentamos con el EFA: la situación óptima es aquella en la que los índices SRMR y RMSEA son inferiores a 0,08 - cuanto más bajos mejor -, y los índices TLI y CFI son superiores a 0,95 - cuanto más altos mejor.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-tres-factores-teóricos",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-tres-factores-teóricos",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Modelo con tres factores teóricos:",
    "text": "Modelo con tres factores teóricos:\n\n# Especificamos el modelo: en este modelo se respetan los tres factores teóricos:\nmodelo3F &lt;- \"Competitividad =~ ST181Q02HA + ST181Q03HA + ST181Q04HA\n             Perseverancia =~ ST182Q03HA + ST182Q04HA + ST182Q05HA + ST182Q06HA\n             Miedo_al_fracaso =~ ST183Q01HA + ST183Q02HA + ST183Q03HA\"\n# Realizamos el factorial:\nfactorial3F &lt;- cfa(modelo3F, datos_AFC, estimator = \"ULS\", ordered = TRUE, orthogonal = FALSE)\n# ordered=TRUE usamos la matriz policórica\n# orthogonal=FALSE estamos diciendo que puede haber correlación entre factores\n\nsummary(factorial3F, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 30 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        43\n\n                                                  Used       Total\n  Number of observations                         16255       17971\n\nModel Test User Model:\n                                                      \n  Test statistic                              1347.860\n  Degrees of freedom                                32\n  P-value (Unknown)                                 NA\n\nModel Test Baseline Model:\n\n  Test statistic                             69227.035\n  Degrees of freedom                                45\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981\n  Tucker-Lewis Index (TLI)                       0.973\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.050\n  90 Percent confidence interval - lower         0.048\n  90 Percent confidence interval - upper         0.053\n  P-value H_0: RMSEA &lt;= 0.050                    0.410\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.039\n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Competitividad =~                                                        \n    ST181Q02HA           1.000                               0.722    0.722\n    ST181Q03HA           1.007    0.015   66.764    0.000    0.727    0.727\n    ST181Q04HA           1.234    0.019   63.552    0.000    0.890    0.890\n  Perseverancia =~                                                         \n    ST182Q03HA           1.000                               0.707    0.707\n    ST182Q04HA           0.993    0.014   71.418    0.000    0.702    0.702\n    ST182Q05HA           1.151    0.016   71.457    0.000    0.814    0.814\n    ST182Q06HA           0.875    0.013   69.780    0.000    0.618    0.618\n  Miedo_al_fracaso =~                                                      \n    ST183Q01HA           1.000                               0.639    0.639\n    ST183Q02HA           1.507    0.030   49.642    0.000    0.962    0.962\n    ST183Q03HA           1.115    0.019   59.088    0.000    0.712    0.712\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Competitividad ~~                                                      \n    Perseverancia      0.174    0.003   56.878    0.000    0.340    0.340\n    Miedo_al_fracs     0.044    0.002   21.025    0.000    0.095    0.095\n  Perseverancia ~~                                                       \n    Miedo_al_fracs     0.027    0.002   14.295    0.000    0.059    0.059\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ST181Q02HA|t1    -1.396    0.008 -177.950    0.000   -1.396   -1.396\n    ST181Q02HA|t2    -0.434    0.008  -55.353    0.000   -0.434   -0.434\n    ST181Q02HA|t3     0.797    0.008  101.552    0.000    0.797    0.797\n    ST181Q03HA|t1    -1.322    0.008 -168.547    0.000   -1.322   -1.322\n    ST181Q03HA|t2    -0.130    0.008  -16.551    0.000   -0.130   -0.130\n    ST181Q03HA|t3     1.024    0.008  130.573    0.000    1.024    1.024\n    ST181Q04HA|t1    -1.582    0.008 -201.738    0.000   -1.582   -1.582\n    ST181Q04HA|t2    -0.728    0.008  -92.850    0.000   -0.728   -0.728\n    ST181Q04HA|t3     0.549    0.008   70.001    0.000    0.549    0.549\n    ST182Q03HA|t1    -1.960    0.008 -249.928    0.000   -1.960   -1.960\n    ST182Q03HA|t2    -1.423    0.008 -181.399    0.000   -1.423   -1.423\n    ST182Q03HA|t3    -0.206    0.008  -26.304    0.000   -0.206   -0.206\n    ST182Q04HA|t1    -1.953    0.008 -248.995    0.000   -1.953   -1.953\n    ST182Q04HA|t2    -0.714    0.008  -91.039    0.000   -0.714   -0.714\n    ST182Q04HA|t3     0.712    0.008   90.760    0.000    0.712    0.712\n    ST182Q05HA|t1    -2.073    0.008 -264.331    0.000   -2.073   -2.073\n    ST182Q05HA|t2    -1.367    0.008 -174.275    0.000   -1.367   -1.367\n    ST182Q05HA|t3     0.281    0.008   35.824    0.000    0.281    0.281\n    ST182Q06HA|t1    -1.705    0.008 -217.356    0.000   -1.705   -1.705\n    ST182Q06HA|t2    -0.587    0.008  -74.830    0.000   -0.587   -0.587\n    ST182Q06HA|t3     0.734    0.008   93.594    0.000    0.734    0.734\n    ST183Q01HA|t1    -0.944    0.008 -120.363    0.000   -0.944   -0.944\n    ST183Q01HA|t2    -0.098    0.008  -12.494    0.000   -0.098   -0.098\n    ST183Q01HA|t3     1.015    0.008  129.350    0.000    1.015    1.015\n    ST183Q02HA|t1    -1.026    0.008 -130.806    0.000   -1.026   -1.026\n    ST183Q02HA|t2    -0.081    0.008  -10.362    0.000   -0.081   -0.081\n    ST183Q02HA|t3     1.032    0.008  131.540    0.000    1.032    1.032\n    ST183Q03HA|t1    -0.854    0.008 -108.820    0.000   -0.854   -0.854\n    ST183Q03HA|t2     0.075    0.008    9.613    0.000    0.075    0.075\n    ST183Q03HA|t3     1.001    0.008  127.651    0.000    1.001    1.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .ST181Q02HA        0.479                               0.479    0.479\n   .ST181Q03HA        0.472                               0.472    0.472\n   .ST181Q04HA        0.207                               0.207    0.207\n   .ST182Q03HA        0.500                               0.500    0.500\n   .ST182Q04HA        0.507                               0.507    0.507\n   .ST182Q05HA        0.338                               0.338    0.338\n   .ST182Q06HA        0.618                               0.618    0.618\n   .ST183Q01HA        0.592                               0.592    0.592\n   .ST183Q02HA        0.074                               0.074    0.074\n   .ST183Q03HA        0.493                               0.493    0.493\n    Competitividad    0.521    0.010   50.703    0.000    1.000    1.000\n    Perseverancia     0.500    0.009   54.441    0.000    1.000    1.000\n    Miedo_al_fracs    0.408    0.010   41.805    0.000    1.000    1.000\n\n# fijarse en los índices de bondad del ajuste:\n\nsemPlot::semPaths(factorial3F,\n  whatLabels = \"est\",\n  sizeMan = 3.25,\n  node.width = 1,\n  edge.label.cex = .75,\n  style = \"ram\",\n  mar = c(10, 5, 10, 5)\n)\n\n\n\n\n\n\n\n\nEl ajuste es muy bueno: RMSEA y SRMR &lt; 0,08, y TLI y CFI &gt; 0,95",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-un-solo-factor",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-un-solo-factor",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Modelo con un solo factor:",
    "text": "Modelo con un solo factor:\n\n# Especificamos el modelo: todos los items en un único factor:\nmodelo1F &lt;- \"Factor =~ ST181Q02HA + ST181Q03HA + ST181Q04HA + ST182Q03HA + ST182Q04HA + ST182Q05HA + ST182Q06HA + ST183Q01HA + ST183Q02HA + ST183Q03HA\"\n# Realizamos el factorial:\nfactorial1F &lt;- cfa(modelo1F, datos_AFC, estimator = \"ULS\", ordered = TRUE, orthogonal = FALSE)\nsummary(factorial1F, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 24 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n                                                  Used       Total\n  Number of observations                         16255       17971\n\nModel Test User Model:\n                                                       \n  Test statistic                              28114.458\n  Degrees of freedom                                 35\n  P-value (Unknown)                                  NA\n\nModel Test Baseline Model:\n\n  Test statistic                             69227.035\n  Degrees of freedom                                45\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.594\n  Tucker-Lewis Index (TLI)                       0.478\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.222\n  90 Percent confidence interval - lower         0.220\n  90 Percent confidence interval - upper         0.224\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.177\n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor =~                                                             \n    ST181Q02HA        1.000                               0.496    0.496\n    ST181Q03HA        1.034    0.016   62.943    0.000    0.512    0.512\n    ST181Q04HA        1.207    0.018   66.005    0.000    0.598    0.598\n    ST182Q03HA        1.244    0.019   66.499    0.000    0.617    0.617\n    ST182Q04HA        1.195    0.018   65.831    0.000    0.592    0.592\n    ST182Q05HA        1.392    0.020   68.013    0.000    0.690    0.690\n    ST182Q06HA        1.078    0.017   63.849    0.000    0.534    0.534\n    ST183Q01HA        0.375    0.011   33.667    0.000    0.186    0.186\n    ST183Q02HA        0.342    0.011   31.163    0.000    0.170    0.170\n    ST183Q03HA        0.142    0.010   13.782    0.000    0.070    0.070\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ST181Q02HA|t1    -1.396    0.008 -177.950    0.000   -1.396   -1.396\n    ST181Q02HA|t2    -0.434    0.008  -55.353    0.000   -0.434   -0.434\n    ST181Q02HA|t3     0.797    0.008  101.552    0.000    0.797    0.797\n    ST181Q03HA|t1    -1.322    0.008 -168.547    0.000   -1.322   -1.322\n    ST181Q03HA|t2    -0.130    0.008  -16.551    0.000   -0.130   -0.130\n    ST181Q03HA|t3     1.024    0.008  130.573    0.000    1.024    1.024\n    ST181Q04HA|t1    -1.582    0.008 -201.738    0.000   -1.582   -1.582\n    ST181Q04HA|t2    -0.728    0.008  -92.850    0.000   -0.728   -0.728\n    ST181Q04HA|t3     0.549    0.008   70.001    0.000    0.549    0.549\n    ST182Q03HA|t1    -1.960    0.008 -249.928    0.000   -1.960   -1.960\n    ST182Q03HA|t2    -1.423    0.008 -181.399    0.000   -1.423   -1.423\n    ST182Q03HA|t3    -0.206    0.008  -26.304    0.000   -0.206   -0.206\n    ST182Q04HA|t1    -1.953    0.008 -248.995    0.000   -1.953   -1.953\n    ST182Q04HA|t2    -0.714    0.008  -91.039    0.000   -0.714   -0.714\n    ST182Q04HA|t3     0.712    0.008   90.760    0.000    0.712    0.712\n    ST182Q05HA|t1    -2.073    0.008 -264.331    0.000   -2.073   -2.073\n    ST182Q05HA|t2    -1.367    0.008 -174.275    0.000   -1.367   -1.367\n    ST182Q05HA|t3     0.281    0.008   35.824    0.000    0.281    0.281\n    ST182Q06HA|t1    -1.705    0.008 -217.356    0.000   -1.705   -1.705\n    ST182Q06HA|t2    -0.587    0.008  -74.830    0.000   -0.587   -0.587\n    ST182Q06HA|t3     0.734    0.008   93.594    0.000    0.734    0.734\n    ST183Q01HA|t1    -0.944    0.008 -120.363    0.000   -0.944   -0.944\n    ST183Q01HA|t2    -0.098    0.008  -12.494    0.000   -0.098   -0.098\n    ST183Q01HA|t3     1.015    0.008  129.350    0.000    1.015    1.015\n    ST183Q02HA|t1    -1.026    0.008 -130.806    0.000   -1.026   -1.026\n    ST183Q02HA|t2    -0.081    0.008  -10.362    0.000   -0.081   -0.081\n    ST183Q02HA|t3     1.032    0.008  131.540    0.000    1.032    1.032\n    ST183Q03HA|t1    -0.854    0.008 -108.820    0.000   -0.854   -0.854\n    ST183Q03HA|t2     0.075    0.008    9.613    0.000    0.075    0.075\n    ST183Q03HA|t3     1.001    0.008  127.651    0.000    1.001    1.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .ST181Q02HA        0.754                               0.754    0.754\n   .ST181Q03HA        0.737                               0.737    0.737\n   .ST181Q04HA        0.642                               0.642    0.642\n   .ST182Q03HA        0.620                               0.620    0.620\n   .ST182Q04HA        0.649                               0.649    0.649\n   .ST182Q05HA        0.524                               0.524    0.524\n   .ST182Q06HA        0.714                               0.714    0.714\n   .ST183Q01HA        0.965                               0.965    0.965\n   .ST183Q02HA        0.971                               0.971    0.971\n   .ST183Q03HA        0.995                               0.995    0.995\n    Factor            0.246    0.005   45.181    0.000    1.000    1.000\n\n\nEl ajuste no es bueno: RMSEA y SRMR &gt; 0,08, y TLI y CFI &lt; 0,95 (de hecho, es peor que en el modelo con dos factores).",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-dos-factores",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#modelo-con-dos-factores",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Modelo con dos factores:",
    "text": "Modelo con dos factores:\nPlanteamos un modelo con dos factores: uno que englobe los tres items relacionados con la vivienda, y otro que englobe el resto de ítems.\n\n# Especificamos el modelo: en este modelo Competitividad y Perseverancia los introducimos en el mismo factor, ya que con el AFE vimos que tenían correlación:\nmodelo2F &lt;- \"Competitividad_Perseverancia =~ ST181Q02HA + ST181Q03HA + ST181Q04HA + ST182Q03HA + ST182Q04HA + ST182Q05HA + ST182Q06HA\n             Miedo_al_fracaso =~ ST183Q01HA + ST183Q02HA + ST183Q03HA\"\n# Realizamos el factorial:\nfactorial2F &lt;- cfa(modelo2F, datos_AFC, estimator = \"ULS\", ordered = TRUE, orthogonal = FALSE)\nsummary(factorial2F, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.17 ended normally after 29 iterations\n\n  Estimator                                        ULS\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n                                                  Used       Total\n  Number of observations                         16255       17971\n\nModel Test User Model:\n                                                       \n  Test statistic                              11937.402\n  Degrees of freedom                                 34\n  P-value (Unknown)                                  NA\n\nModel Test Baseline Model:\n\n  Test statistic                             69227.035\n  Degrees of freedom                                45\n  P-value                                           NA\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.828\n  Tucker-Lewis Index (TLI)                       0.772\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.147\n  90 Percent confidence interval - lower         0.145\n  90 Percent confidence interval - upper         0.149\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.116\n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                                  Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Competitividad_Perseverancia =~                                             \n    ST181Q02HA                       1.000                               0.497\n    ST181Q03HA                       1.013    0.016   62.248    0.000    0.503\n    ST181Q04HA                       1.195    0.018   65.562    0.000    0.593\n    ST182Q03HA                       1.250    0.019   66.294    0.000    0.621\n    ST182Q04HA                       1.221    0.019   65.925    0.000    0.606\n    ST182Q05HA                       1.407    0.021   67.816    0.000    0.699\n    ST182Q06HA                       1.102    0.017   64.056    0.000    0.547\n  Miedo_al_fracaso =~                                                         \n    ST183Q01HA                       1.000                               0.636\n    ST183Q02HA                       1.517    0.031   49.416    0.000    0.964\n    ST183Q03HA                       1.121    0.019   59.013    0.000    0.713\n  Std.all\n         \n    0.497\n    0.503\n    0.593\n    0.621\n    0.606\n    0.699\n    0.547\n         \n    0.636\n    0.964\n    0.713\n\nCovariances:\n                                  Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Competitividad_Perseverancia ~~                                             \n    Miedo_al_fracs                   0.029    0.001   23.065    0.000    0.093\n  Std.all\n         \n    0.093\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    ST181Q02HA|t1    -1.396    0.008 -177.950    0.000   -1.396   -1.396\n    ST181Q02HA|t2    -0.434    0.008  -55.353    0.000   -0.434   -0.434\n    ST181Q02HA|t3     0.797    0.008  101.552    0.000    0.797    0.797\n    ST181Q03HA|t1    -1.322    0.008 -168.547    0.000   -1.322   -1.322\n    ST181Q03HA|t2    -0.130    0.008  -16.551    0.000   -0.130   -0.130\n    ST181Q03HA|t3     1.024    0.008  130.573    0.000    1.024    1.024\n    ST181Q04HA|t1    -1.582    0.008 -201.738    0.000   -1.582   -1.582\n    ST181Q04HA|t2    -0.728    0.008  -92.850    0.000   -0.728   -0.728\n    ST181Q04HA|t3     0.549    0.008   70.001    0.000    0.549    0.549\n    ST182Q03HA|t1    -1.960    0.008 -249.928    0.000   -1.960   -1.960\n    ST182Q03HA|t2    -1.423    0.008 -181.399    0.000   -1.423   -1.423\n    ST182Q03HA|t3    -0.206    0.008  -26.304    0.000   -0.206   -0.206\n    ST182Q04HA|t1    -1.953    0.008 -248.995    0.000   -1.953   -1.953\n    ST182Q04HA|t2    -0.714    0.008  -91.039    0.000   -0.714   -0.714\n    ST182Q04HA|t3     0.712    0.008   90.760    0.000    0.712    0.712\n    ST182Q05HA|t1    -2.073    0.008 -264.331    0.000   -2.073   -2.073\n    ST182Q05HA|t2    -1.367    0.008 -174.275    0.000   -1.367   -1.367\n    ST182Q05HA|t3     0.281    0.008   35.824    0.000    0.281    0.281\n    ST182Q06HA|t1    -1.705    0.008 -217.356    0.000   -1.705   -1.705\n    ST182Q06HA|t2    -0.587    0.008  -74.830    0.000   -0.587   -0.587\n    ST182Q06HA|t3     0.734    0.008   93.594    0.000    0.734    0.734\n    ST183Q01HA|t1    -0.944    0.008 -120.363    0.000   -0.944   -0.944\n    ST183Q01HA|t2    -0.098    0.008  -12.494    0.000   -0.098   -0.098\n    ST183Q01HA|t3     1.015    0.008  129.350    0.000    1.015    1.015\n    ST183Q02HA|t1    -1.026    0.008 -130.806    0.000   -1.026   -1.026\n    ST183Q02HA|t2    -0.081    0.008  -10.362    0.000   -0.081   -0.081\n    ST183Q02HA|t3     1.032    0.008  131.540    0.000    1.032    1.032\n    ST183Q03HA|t1    -0.854    0.008 -108.820    0.000   -0.854   -0.854\n    ST183Q03HA|t2     0.075    0.008    9.613    0.000    0.075    0.075\n    ST183Q03HA|t3     1.001    0.008  127.651    0.000    1.001    1.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .ST181Q02HA        0.753                               0.753    0.753\n   .ST181Q03HA        0.747                               0.747    0.747\n   .ST181Q04HA        0.648                               0.648    0.648\n   .ST182Q03HA        0.615                               0.615    0.615\n   .ST182Q04HA        0.632                               0.632    0.632\n   .ST182Q05HA        0.512                               0.512    0.512\n   .ST182Q06HA        0.700                               0.700    0.700\n   .ST183Q01HA        0.596                               0.596    0.596\n   .ST183Q02HA        0.070                               0.070    0.070\n   .ST183Q03HA        0.492                               0.492    0.492\n    Cmpttvdd_Prsvr    0.247    0.005   45.016    0.000    1.000    1.000\n    Miedo_al_fracs    0.404    0.010   41.656    0.000    1.000    1.000\n\nsemPlot::semPaths(factorial2F,\n  whatLabels = \"est\",\n  sizeMan = 3.25,\n  node.width = 1,\n  edge.label.cex = .75,\n  style = \"ram\",\n  mar = c(10, 5, 10, 5)\n)\n\n\n\n\n\n\n\n\nEl ajuste no es bueno: RMSEA y SRMR &gt; 0,08, y TLI y CFI &lt; 0,95",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#resultados-cfa",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/pisa_factorial.html#resultados-cfa",
    "title": "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial",
    "section": "Resultados CFA:",
    "text": "Resultados CFA:\nLos índices de bondad del ajuste muestran que el modelo que refleja la estructura de los tres factores teóricos es el que mejor ajusta ajusta. Su ajuste a los datos es muy bueno.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Reducción Dimensionalidad - Análisis Factorial: pisa_factorial"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando el método de Componentes Principales. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado IMCV_pca.xlsx.\nEn el siguiente dataset disponemos de datos por Comunidades Autónomas de las nueve dimensiones relativas a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV), una estadística con carácter experimental. Datos correspondientes al año 2020.\nConcretamente tenemos las siguientes variables:\n\nindice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\ndim1 : Indicador sobre las condiciones materiales de vida.\ndim2 : Indicador sobre el trabajo.\ndim3 : Indicador sobre la salud.\ndim4 : Indicador sobre la educación.\ndim5 : Indicador sobre el ocio y relaciones sociales.\n\ndim6 : Indicador sobre la seguridad física y personal.\n\ndim7 : Indicador sobre la gobernanza y los derechos básicos.\n\ndim8 : Indicador sobre el entorno y el medio ambiente.\ndim9 : Indicador sobre experiencia general de la vida.\nCCAA: Comunidades Autónomas.\n\nLa estadística experimental sobre el Índice Multidimensional de Calidad de Vida (IMCV) se construye a partir de los indicadores de calidad del INE, que ofrecen una visión panorámica (multidimensional) de la calidad de vida en España, mediante la elección de un conjunto amplio pero limitado de indicadores (actualmente 60) que cubren nueve dimensiones usadas para describir la calidad de vida.\nNuestro objetivo es aplicar un procedimiento de PCA para hacer toda esta información más manejable es a través de un análisis de componentes principales, el cual nos permite reducir el número de variables correlacionadas entre sí a otro número igual de componentes no correlacionados.\nEsta técnica de análisis tiene muchas aplicaciones y en esta ocasión la utilizaremos para poder simplificar la tarea de clasificación territorios españoles en base a sus características de calidad vida. El objetivo es reducir el número de variables con las que trabajar sufriendo la menor pérdida posible de información. La agrupación de comunidades autónomas en grupos que compartan unas características similares la realizaremos mediante una técnica de análisis de conglomerados denominada K-medias. El objetivo es agrupar nuestras 17 comunidades y dos ciudades autónomas en un puñado de grupos homogéneos.\n\n\n\nRealizar una Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es reducir el número de variables de los 9 indicadores globales de calidad de vida sin perder mucha información.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#dataset",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#dataset",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado IMCV_pca.xlsx.\nEn el siguiente dataset disponemos de datos por Comunidades Autónomas de las nueve dimensiones relativas a la calidad de vida que componen el Índice Multidimensional de Calidad de Vida (IMCV), una estadística con carácter experimental. Datos correspondientes al año 2020.\nConcretamente tenemos las siguientes variables:\n\nindice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\ndim1 : Indicador sobre las condiciones materiales de vida.\ndim2 : Indicador sobre el trabajo.\ndim3 : Indicador sobre la salud.\ndim4 : Indicador sobre la educación.\ndim5 : Indicador sobre el ocio y relaciones sociales.\n\ndim6 : Indicador sobre la seguridad física y personal.\n\ndim7 : Indicador sobre la gobernanza y los derechos básicos.\n\ndim8 : Indicador sobre el entorno y el medio ambiente.\ndim9 : Indicador sobre experiencia general de la vida.\nCCAA: Comunidades Autónomas.\n\nLa estadística experimental sobre el Índice Multidimensional de Calidad de Vida (IMCV) se construye a partir de los indicadores de calidad del INE, que ofrecen una visión panorámica (multidimensional) de la calidad de vida en España, mediante la elección de un conjunto amplio pero limitado de indicadores (actualmente 60) que cubren nueve dimensiones usadas para describir la calidad de vida.\nNuestro objetivo es aplicar un procedimiento de PCA para hacer toda esta información más manejable es a través de un análisis de componentes principales, el cual nos permite reducir el número de variables correlacionadas entre sí a otro número igual de componentes no correlacionados.\nEsta técnica de análisis tiene muchas aplicaciones y en esta ocasión la utilizaremos para poder simplificar la tarea de clasificación territorios españoles en base a sus características de calidad vida. El objetivo es reducir el número de variables con las que trabajar sufriendo la menor pérdida posible de información. La agrupación de comunidades autónomas en grupos que compartan unas características similares la realizaremos mediante una técnica de análisis de conglomerados denominada K-medias. El objetivo es agrupar nuestras 17 comunidades y dos ciudades autónomas en un puñado de grupos homogéneos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "",
    "text": "Realizar una Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es reducir el número de variables de los 9 indicadores globales de calidad de vida sin perder mucha información.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(factoextra) # fviz_cluster function\nlibrary(ggcorrplot) # Para función ggcorrplot\nlibrary(corrplot) # Para corrplot",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#lectura-datos",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../../files/IMCV_pca.xlsx\", sheet = \"Datos\")\n\nEn primer lugar, cargamos los datos que vamos a utilizar. En este caso, se trata de un conjunto de datos compuesto por 20 filas y 14 columnas. Las filas corresponden a las 19 Comunidades Autónomas de España (17 CCAA, Ceuta y Melilla) y el Total Nacional y las 11 columnas al nombre de dichas CCAA, los 9 indices y el indice total (la media de los anteriores).\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores perdidos, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación, no hay ningún NA en los datos que vamos a utilizar.\n\nsum(is.na(datos))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(datos)\n\n[1] 20 11\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [20 × 11] (S3: tbl_df/tbl/data.frame)\n $ CCAA        : chr [1:20] \"Total nacional\" \"Andalucía\" \"Aragón\" \"Principado de Asturias\" ...\n $ dim1        : num [1:20] 100.1 97.1 105.2 102.2 101.8 ...\n $ dim2        : num [1:20] 100.2 95.4 103.3 99.7 102.6 ...\n $ dim3        : num [1:20] 101.3 99.2 102.1 99.7 102.5 ...\n $ dim4        : num [1:20] 106 101 107 109 103 ...\n $ dim5        : num [1:20] 99.1 96.5 104.9 98 103.9 ...\n $ dim6        : num [1:20] 102 102 106 109 103 ...\n $ dim7        : num [1:20] 100 94.9 97.4 100.2 109.4 ...\n $ dim8        : num [1:20] 102.1 98.9 106.3 105.9 104.1 ...\n $ dim9        : num [1:20] 104 102 111 105 112 ...\n $ indice_total: num [1:20] 101.7 98.5 104.8 103.2 104.7 ...\n\n# Descripción de las variables\nsummary(datos)\n\n     CCAA                dim1             dim2            dim3       \n Length:20          Min.   : 91.12   Min.   : 94.6   Min.   : 98.20  \n Class :character   1st Qu.: 97.49   1st Qu.: 97.6   1st Qu.: 99.74  \n Mode  :character   Median :100.79   Median :100.2   Median :101.03  \n                    Mean   :100.16   Mean   : 99.8   Mean   :101.20  \n                    3rd Qu.:103.41   3rd Qu.:102.5   3rd Qu.:102.39  \n                    Max.   :105.31   Max.   :103.4   Max.   :104.65  \n      dim4             dim5             dim6             dim7       \n Min.   : 97.35   Min.   : 95.45   Min.   : 93.32   Min.   : 91.22  \n 1st Qu.:103.09   1st Qu.: 97.56   1st Qu.:100.15   1st Qu.: 97.03  \n Median :106.18   Median : 99.77   Median :102.76   Median :100.12  \n Mean   :105.95   Mean   :100.38   Mean   :102.94   Mean   : 99.97  \n 3rd Qu.:108.14   3rd Qu.:102.53   3rd Qu.:106.55   3rd Qu.:102.49  \n Max.   :115.15   Max.   :107.28   Max.   :109.48   Max.   :109.35  \n      dim8             dim9         indice_total   \n Min.   : 93.17   Min.   : 93.81   Min.   : 97.37  \n 1st Qu.: 99.04   1st Qu.:103.09   1st Qu.:100.35  \n Median :103.31   Median :106.50   Median :101.95  \n Mean   :102.51   Mean   :105.26   Mean   :102.02  \n 3rd Qu.:105.97   3rd Qu.:107.67   3rd Qu.:103.48  \n Max.   :109.42   Max.   :112.49   Max.   :105.92  \n\n\nVemos que estas variables (a excepción de las CCAA) son todas de tipo numérico, y además, podemos obtener información como la media, desviación típica, los cuartiles y el histograma de cada una.\nCorrelación: El que existan correlaciones muy elevadas en el conjunto de datos nos permitirá resumir la información en un menor número de componentes principales, pues éstas explicarán una mayor cantidad de información.\n\nggcorrplot(cor(datos[, 2:11]), type = \"lower\", lab = T, show.legend = T)\n\n\n\n\n\n\n\n\nEn este caso, se ha generado un gráfico entre variables, sin tener en cuenta la correlación de la variable consigo misma, pues siempre será del 100%, o 1 en tanto por uno.\nEn términos absolutos, vemos lo siguiente:\n\nHay varias correlaciones moderadas/altas. En general la variable indice_total esta muy correlacionada con todas las demás. Esto parece razonable puesto que sabemos que es la media aritmética de las otras nueve dimensiones. Es por ello que no aporta nada de información extra. Además destaca la correlación entre dim1 (materiales), dim2 (trabajo), dim3(salud) y dim4 (educación) entre todas ellas. Esto también podría parecer razonable puesto que una mejor educación suele llevar más satisfacción en los estudios, en el trabajo y en las condiciones materiales (puesto que se suele ganar más dinero) y esto puede derivar en menos problemas de salud debido a una vida “más tranquila. También hay correlaciones altas entre dim1 y dim8, y entre dim7 y dim8.\nHay correlaciones muy bajas entre dim8 (entorno y el medioambiente) y dim9 (satisfacción general).\n\nEn resumen, vemos que hay varias variables con una alta correlación absoluta (tanto positiva como negativa), luego esto nos va a permitir resumir la información en un número de componentes principales menor al número de variables que tenemos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#introducción-1",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de Componentes Principales (PCA) es una técnica para reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nSe busca encontrar nuevas variables (componentes) que estén incorrelacionadas y que capturen la máxima variabilidad de los datos. Esto se logra mediante combinaciones lineales de las variables originales. PCA es útil para entender relaciones, reducir dimensiones y manejar la alta correlación entre variables.\nPara aplicar PCA, se necesitan datos cuantitativos y es crucial escalar las variables (estandarizar = media cero y varianza uno). Esto garantiza que ninguna variable domine el análisis. Además, se puede trabajar con la matriz de correlaciones para abordar fuertes correlaciones entre variables, manteniendo así la información más relevante del conjunto de datos.\nLos pasos generales son:\n\nEstandarización de las variables: Es importante estandarizar las variables numéricas para que tengan media cero y desviación estándar uno. Esto es crucial para que ninguna variable domine el análisis debido a su escala.\nCálculo de la matriz de correlaciones o covarianzas: Dependiendo del enfoque, se puede trabajar con la matriz de correlaciones si se busca abordar fuertes correlaciones entre variables, o con la matriz de covarianzas si se busca la varianza total de las variables.\n\nNOTA: Aconsejable trabajar siempre con la matriz de correlaciones (a no ser que todas variables estén en las mismas unidades, que se podrá usar la matriz de covarianzas). De no seguir esta nota y usar la matriz de covarianzas, las variables que tienen mayores unidades dominarán la estructura de covarianza, lo que llevará a una representación inexacta de la variabilidad real de los datos.\n\nDescomposición de la matriz: Se descompone la matriz de correlaciones en sus vectores y valores propios. Los valores propios representan la cantidad de varianza explicada por cada componente principal, mientras que los vectores propios (autovectores) determinan la dirección de cada componente en el espacio multidimensional original.\nSelección de componentes: Los componentes se ordenan de manera descendente según la cantidad de varianza que explican. Los primeros componentes capturan la mayor variabilidad de los datos y se seleccionan para reducir la dimensionalidad manteniendo la información más relevante.\nTransformación de datos: Proyectar los datos originales en el espacio de los componentes principales para obtener las nuevas variables. Estas son combinaciones lineales de las variables originales y son ortogonales entre sí. Esta transformación lineal conserva la mayor parte de la información en un espacio de menor dimensión, lo que facilita el análisis y la visualización de los datos.\nInterpretación y visualización: Explorar la importancia de cada componente en términos de la variabilidad explicada. Se pueden interpretar los componentes para comprender qué aspectos de los datos capturan. Si es posible, representar gráficamente los datos en el espacio reducido de los componentes principales para obtener una mejor comprensión de las relaciones entre las observaciones.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#modelo",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#modelo",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Modelo",
    "text": "Modelo\nEn las siguientes lineas haremos que la variable CCAA se ponga como nombre de filas y posteriormente eliminaremos esa variable ya que ya la tendremos como nombre de filas.\n\nCCAA &lt;- datos$CCAA\ndatos &lt;- datos[, -1] # Eliminamos ahora\nrownames(datos) &lt;- CCAA # Como nombres de filas las CCAA\n\nEscalamos los datos y calculamos la matriz de varianzas covarianzas, mostramos solo la diagonal (debería ser 1).\n\ndatos2 &lt;- scale(datos)\nsummary(datos2)\n\n      dim1              dim2              dim3               dim4        \n Min.   :-2.1326   Min.   :-1.7625   Min.   :-1.56842   Min.   :-1.9141  \n 1st Qu.:-0.6318   1st Qu.:-0.7483   1st Qu.:-0.76011   1st Qu.:-0.6366  \n Median : 0.1484   Median : 0.1242   Median :-0.08763   Median : 0.0510  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7658   3rd Qu.: 0.9280   3rd Qu.: 0.62421   3rd Qu.: 0.4868  \n Max.   : 1.2135   Max.   : 1.2268   Max.   : 1.80469   Max.   : 2.0459  \n      dim5              dim6               dim7               dim8        \n Min.   :-1.4419   Min.   :-2.18929   Min.   :-1.78315   Min.   :-1.9962  \n 1st Qu.:-0.8263   1st Qu.:-0.63634   1st Qu.:-0.59890   1st Qu.:-0.7407  \n Median :-0.1781   Median :-0.04166   Median : 0.03022   Median : 0.1713  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.6283   3rd Qu.: 0.81996   3rd Qu.: 0.51421   3rd Qu.: 0.7402  \n Max.   : 2.0179   Max.   : 1.48572   Max.   : 1.91434   Max.   : 1.4768  \n      dim9          indice_total     \n Min.   :-2.6466   Min.   :-2.01845  \n 1st Qu.:-0.5007   1st Qu.:-0.72184  \n Median : 0.2878   Median :-0.02952  \n Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.5582   3rd Qu.: 0.63501  \n Max.   : 1.6719   Max.   : 1.69368  \n\ndiag(var(datos2))\n\n        dim1         dim2         dim3         dim4         dim5         dim6 \n           1            1            1            1            1            1 \n        dim7         dim8         dim9 indice_total \n           1            1            1            1 \n\n\nAplicamos funcion PCA, notar que en este caso no haría falta los argumentos SCALE=TRUE y CENTER=TRUE puesto que ya hemos escalado dos datos en un paso previo. Por defecto en la función viene el valor de SCALE=FALSE y CENTER=TRUE.\n\npca &lt;- prcomp(datos2, center = TRUE, scale = TRUE) # Scale=T\n\nCalculamos los coeficientes de la ecuación para cada componente principal\n\npca$rotation\n\n                     PC1         PC2          PC3         PC4         PC5\ndim1          0.41217280  0.14877833 -0.007886089  0.25040422 -0.03495987\ndim2          0.39138679 -0.09690754 -0.285268440  0.02374485  0.30937355\ndim3          0.31866055 -0.26428112 -0.366226773 -0.25600015 -0.27286698\ndim4          0.34816197  0.10046074 -0.432653154 -0.04246203  0.09300467\ndim5          0.21612679 -0.13421375  0.369533348 -0.74071758  0.37307852\ndim6          0.26627260  0.31778339  0.561890598  0.13651894  0.10130691\ndim7         -0.01055075 -0.57258219  0.099090101  0.50180042  0.49810262\ndim8          0.39164780  0.13691741  0.227414824  0.20210276 -0.40340929\ndim9          0.02291434 -0.62614481  0.253810202 -0.05251280 -0.50833075\nindice_total  0.42723809 -0.17573666  0.137902202  0.08178066  0.03554529\n                     PC6         PC7         PC8           PC9       PC10\ndim1          0.15986625 -0.35400851  0.14448659 -7.355855e-01  0.1764945\ndim2          0.02687191 -0.56236950 -0.38092875  4.272942e-01  0.1228933\ndim3          0.45622979  0.46654349 -0.32990432 -1.162118e-01  0.0796614\ndim4         -0.68323743  0.32389336  0.25348956 -8.518163e-03  0.1870813\ndim5          0.07972391 -0.03791973  0.28252371 -5.465610e-02  0.1423732\ndim6         -0.17814501  0.31489323 -0.56481902 -8.173809e-03  0.1830583\ndim7          0.13181815  0.27778546  0.16153493  3.579959e-05  0.2041452\ndim8          0.24817365  0.04247789  0.46564230  5.043772e-01  0.1948073\ndim9         -0.41586550 -0.23231717 -0.13180172 -7.148902e-02  0.1801180\nindice_total -0.09359397  0.04809077  0.04236672 -1.395427e-02 -0.8640333\n\n\nPodemos observar aquí nuestras variables en el nuevo sistema de cordenadas (las componentes principales), dando lugar a ecuaciones de cada eje como combinación lineal del total de variables. Analizar el vector que crea cada componente y cuáles son los pesos que tienen las variables en cada componente, ayuda a interpretar qué tipo de información recoge cada una de ellas.\nPor ejemplo, la primera componente principal (PC1), presenta la siguiente ecuación, como combinación lineal de las 10 variables originales (los 9 indices y el total):\n\\(PC_1 = 0.41 {dim1} + 0.39 {dim2} + 0.31dim3 + 0.34 dim4 + 0.21 dim5 + 0.26 dim6 -0.01 dim7 +0.39dim8 +0.02dim9 +0.42indice_total\\)\nExtraemos las nuevas coordenadas de los individuos (puntuaciones)\nAdemás, podemos ver las puntuaciones, que son las coordenadas de cada observación original (Comunidad Autónoma) sobre los nuevos ejes construidos (componentes principales). Esto corresponde a un cambio de coordenadas bajo el paradigma del Álgebra Lineal.\n\npca$x\n\n                                  PC1         PC2         PC3        PC4\nTotal nacional             -0.1520461  0.07182026 -0.45989066  0.1972086\nAndalucía                  -2.8750017  1.42744067  0.03518304  0.1280892\nAragón                      2.5189323 -0.52042496  0.93792815 -0.7452448\nPrincipado de Asturias      0.8999964  0.89724658  0.68192085  1.2083129\nIlles Balears               1.3747946 -2.73400720  0.95711048  0.2369471\nCanarias                   -3.1596066 -0.04090058 -0.09918370  0.3649717\nCantabria                   2.2014164  0.75598285  1.42109419 -0.8263169\nCastilla y León             0.9996922  2.65361220  0.52989434 -0.6582007\nCastilla-La Mancha         -0.2922882  0.86529256  0.53813056  1.2402982\nCataluña/Catalunya          0.1681743 -0.87429121 -1.52725547  0.8279649\nComunitat Valenciana       -0.1458826 -1.25747939  0.61481441  0.9489872\nExtremadura                -1.0519197  0.16786627  1.68926034 -0.4259641\nGalicia                    -0.4075932  2.79976605 -0.32800120 -0.6218279\nComunidad de Madrid         1.0281924 -0.97693037 -2.43036654 -0.1553581\nRegión de Murcia           -2.5673690 -0.06088548 -0.25201844  1.3020744\nComunidad Foral de Navarra  3.6514886 -0.76057338 -0.38861291  0.2272912\nPaís Vasco/Euskadi          2.3914014  1.16327326 -2.33194237 -0.6865915\nLa Rioja                    2.3224637 -0.83141897  0.63878955  0.5481555\nCiudad Autónoma de Ceuta   -4.8615062 -0.93309263 -0.28920898 -0.2972782\nCiudad Autónoma de Melilla -2.0433389 -1.81229652  0.06235434 -2.8135186\n                                    PC5         PC6         PC7         PC8\nTotal nacional             -0.004590525  0.07740386 -0.03500474 -0.03286903\nAndalucía                  -0.617579706  0.16919700 -0.06676935 -0.10573402\nAragón                     -0.377371489 -0.23123596 -0.92964071 -0.27460172\nPrincipado de Asturias     -0.090833979 -0.90107665  0.27340119 -0.05512076\nIlles Balears               0.429253728  0.44404518 -0.41284040 -0.04953438\nCanarias                   -0.620175044 -0.75909198  0.29915397 -0.54095905\nCantabria                  -0.005966101 -0.70417837 -0.05752270  0.23598297\nCastilla y León             0.161603996  0.46660593 -0.60032019  0.30539596\nCastilla-La Mancha         -0.258461708  1.19428695  0.25999657 -0.08984701\nCataluña/Catalunya         -0.229653846  0.13187983 -0.56300688 -0.25232420\nComunitat Valenciana        0.505122343 -0.53013775  0.04048603  0.36603262\nExtremadura                -1.078612886  0.47481509  0.77326877  0.23250910\nGalicia                     1.557454194  0.06616103  0.24900479 -0.33491570\nComunidad de Madrid         0.247821195  0.33060935  0.55630084  0.22530929\nRegión de Murcia            0.861302303 -0.37552323 -0.05444533  0.34920723\nComunidad Foral de Navarra -0.332575416 -0.13910414  0.70942125  0.02558768\nPaís Vasco/Euskadi         -0.851410145 -0.26279579 -0.25757341  0.15864443\nLa Rioja                    0.483996053  0.46552566  0.05023515 -0.30388831\nCiudad Autónoma de Ceuta   -0.223110508  0.11977479 -0.69124440  0.23765210\nCiudad Autónoma de Melilla  0.443787541 -0.03716081  0.45709952 -0.09652721\n                                    PC9          PC10\nTotal nacional              0.053670256 -6.378465e-16\nAndalucía                  -0.233555298  1.869965e-15\nAragón                     -0.202175835  1.898480e-15\nPrincipado de Asturias      0.097410314  3.122401e-15\nIlles Balears               0.021978584  7.720383e-16\nCanarias                    0.025967087 -8.187941e-16\nCantabria                  -0.021604695 -4.758508e-15\nCastilla y León             0.243709000  9.063783e-16\nCastilla-La Mancha          0.090401694  2.492288e-15\nCataluña/Catalunya          0.438693342 -2.877966e-15\nComunitat Valenciana        0.131431585  1.719796e-15\nExtremadura                 0.004275745 -7.890934e-16\nGalicia                     0.009359994 -2.204684e-15\nComunidad de Madrid        -0.177270792 -1.540965e-15\nRegión de Murcia           -0.185842836  2.708514e-15\nComunidad Foral de Navarra  0.102240565 -1.958570e-15\nPaís Vasco/Euskadi         -0.138765629  1.740773e-15\nLa Rioja                   -0.342414149 -1.628944e-15\nCiudad Autónoma de Ceuta   -0.078644487 -3.452468e-15\nCiudad Autónoma de Melilla  0.161135554  3.472752e-15\n\n\nVarianza explicada por cada componente principal\nUna vez calculadas las componentes principales, es de interés conocer la varianza explicada por cada una, ya que el principal objetivo que se sigue con PCA es maximizar la cantidad de información explicada por las componentes.\n\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.2299 1.3937 1.0954 0.96289 0.61604 0.50849 0.46937\nProportion of Variance 0.4972 0.1942 0.1200 0.09272 0.03795 0.02586 0.02203\nCumulative Proportion  0.4972 0.6915 0.8115 0.90419 0.94214 0.96799 0.99002\n                           PC8     PC9      PC10\nStandard deviation     0.25786 0.18237 2.333e-15\nProportion of Variance 0.00665 0.00333 0.000e+00\nCumulative Proportion  0.99667 1.00000 1.000e+00\n\n\n\nStandard deviation: muestra las desviaciones estándar de cada componente principal. Si elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\nProportion of Variance: es la proporción de la varianza total que explica cada componente principal y quizá, es la fila más importante de nuestros resultados. Como los autovalores están ordenados de mayor a menor y así son construidas las componentes principales, la primera componente principal es la que mayor porcentaje de variabilidad explica, un 49%. Así, la varianza explicada por la componentes van en orden decreciente, teniendo que la segunda componente explica un 19% y la tercera, un 12%.\nCumulative proportion: es la varianza acumulada y se calcula progresivamente sumando la Proportion of Variance anterior. En vista de estos resultados, vemos que la primera componente agrupa el 49% de la variación, y que necesitamos 3 componentes para alcanzar el 80%.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#selección-de-componentes",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#selección-de-componentes",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Selección de componentes",
    "text": "Selección de componentes\nGraficando el valor de la varianza de cada componente principal, podemos observar los resultados comentados anteriormente, que las primeras componentes son las que más varianza explican y que a medida que se añaden más, la varianza explicada por cada una es menor.\n\n# fviz_eig(pca, main=\"Varianza de cada componente\", choice = \"eigenvalue\", addlabels = T)\n\nComo el porcentaje de varianza explicada por cada componente sobre el total.\n\nfviz_screeplot(pca, addlabels = TRUE, main = \"Porcentaje de varianza explicada por cada componente (%)\")\n\n\n\n\n\n\n\n\nA continuación, representamos las varianzas acumuladas:\n\nplot(summary(pca)$importance[3, ], type = \"o\", col = \"darkblue\", lwd = 3, main = \"Porcentaje de varianza acumulada\", xlab = \"Componente Principal\", ylab = \"Porcentaje de varianza acumulada\")\n\n\n\n\n\n\n\n\nDeterminar el número de componentes que elegir para continuar con el análisis no tiene unas normas determinadas a seguir. Respecto a ello, existen varios criterios con sus respectivas propuestas.\n\nUna opción para determinar el número de componentes principales que seleccionar, es coger aquellas tal que expliquemos un % determinado de la variabilidad de los datos que nosotros prefijemos. Generalmente se pone como umbral mínimo un 80%, entonces necesitaríamos elegir 3 componentes.\nOtra posibilidad es seguir el criterio de Kaisser, que escoge aquellas componentes cuyo autovalor sea superior a 1 (cuando las variables han sido generadas a partir de la matriz de correlaciones). Según este criterio y mirando el gráfico anterior de la varianza (igual a eigenvalues), elegiríamos las 3 primeras componentes. Este criterio, cuando p &lt; 20, tiende a ser estricto e incluir pocas componentes.\nPara relajar el criterio de Kaisser, existe la modificación de Jollife, que elige aquellas componentes cuyo autovalor sea superior a 0.7. Esta modificación, nos permite elegir igualmente 4 componentes.\n\nEn este caso, nos podríamos quedar con las 3 primeras componentes principales, ya que es el número en el que coincide el mayor número de criterios. Por tanto, en lugar de trabajar con las 13 variables originales, trabajaremos con 3 variables nuevas, que son combinaciones de ellas.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#interpretación",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#interpretación",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Interpretación",
    "text": "Interpretación\nHemos decidido quedarnos con 3 componentes principales, que explican el 81% de la variabilidad total. Para realizar su interpretación, volvemos a ver los coeficientes de las ecuaciones de los componentes, observando cuáles son los valores más altos (en valor absoluto), para así poder dar una interpretación a cada eje.\nGráficamente, también podemos ver la contribución de las variables a los 3 primeros ejes, señalando en color azul las variables que puntúan positivamente en el eje, y en rojo, las que lo hacen de forma negativa.\n\npca$rotation[, 1:3]\n\n                     PC1         PC2          PC3\ndim1          0.41217280  0.14877833 -0.007886089\ndim2          0.39138679 -0.09690754 -0.285268440\ndim3          0.31866055 -0.26428112 -0.366226773\ndim4          0.34816197  0.10046074 -0.432653154\ndim5          0.21612679 -0.13421375  0.369533348\ndim6          0.26627260  0.31778339  0.561890598\ndim7         -0.01055075 -0.57258219  0.099090101\ndim8          0.39164780  0.13691741  0.227414824\ndim9          0.02291434 -0.62614481  0.253810202\nindice_total  0.42723809 -0.17573666  0.137902202\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nSi nos fijamos en los pesos más altos, podemos darle una interpretación a cada eje. Por ejemplo:\n\nLa primera componente explica un 49% de la variación. Hay valores absolutos bastante similares y elevados, que son los correspondientes con todas las dimensiones excepto la 7 y la 9. Esto parece razonable puesto que antes habíamos visto que todas variables a excepción de la 7 y la 9 estaban bastante correlacionadas entre ellas.\nEn la segunda componente, los pesos más elevados corresponden con las dimensiones 7 y 9, que son las que no habían influido en la primera componente.\nPara la tercera componente, influyen las dimensiones de la 3 a la 6.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#representación-gráfica",
    "href": "notebooks/Reduccion Dimension/PCA/IMCV_pca/IMCV_pca.html#representación-gráfica",
    "title": "Reducción Dimensionalidad - PCA: IMCV_pca",
    "section": "Representación gráfica",
    "text": "Representación gráfica\nGráfico de las variables\nRepresentamos sobre las dos primeras componentes principales las variables originales. En el eje de abscisas se representa la PC1 y en el eje de ordenadas, la PC2. Para interpretar correctamente las variables tenemos que fijarnos en la longitud de la flecha y en el ángulo que forman respecto a los ejes y entre ellos mismos.\n\nÁngulo vector - eje: cuanto más paralelo es un vector al eje, más ha contribuido a dicha componente principal.\nÁngulo entre dos vectores: si es pequeño representa una alta correlación entre las variables implicadas (y por tanto, observaciones con valores altos en una variable, tendrá valores altos en la otra). Si el ángulo es cercano a 90º indica que las variables están incorreladas y los ángulos opuestos indican correlación negativa entre ellas.\nLongitud: cuanto mayor es la longitud de un vector, mayor varianza de la variable está contenida en el biplot, es decir, mejor representada está en el gráfico.\n\nEn el gráfico, diferenciamos por colores las variables según su calidad de representación en las dos primeras componentes. Cuanto más cerca esté una variable del círculo de correlaciones, mejor será su representación, por lo que las variables que estén muy cerca del centro de la gráfica son las menos importantes para las dos primeras componentes.\n\nfviz_pca_var(pca, axes = c(1, 2), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\n\nRESUMEN DE RESULTADOS\nLos gráficos obtenidos muestran una visualización de las variables en función de las componentes principales 1 y 2, y 3 y 4. Las variables que tienen una correlación alta con la primera componente principal, que son todas menos las 7 y la 9, están más cerca del eje horizontal y las variables que tienen una alta correlación con la segunda componente principal están más cerca del eje vertical,. Las variables que se encuentran cerca del centro del gráfico tienen una correlación baja con ambas componentes principales, aún así nos guiaremos con la tabla para decidir qué variables están mejor explicadas por cada una de las componentes. En resumen, estos gráficos proporcionan una representación visual de las relaciones entre las variables en función de las dos primeras componentes principales y las dos segundas, lo que puede ayudar a identificar patrones y tendencias en los datos.\nCOMPONENTE 1\nDe los gráficos extraemos la siguiente conclusión. La componente 1 está muy correlacionada con las condiciones materiales de vida, el medio ambiente y el trabajo. Además está correlación es positiva , por lo que las comunidades que puntúen alto en la componente podremos decir que tendrán altos niveles de bienestar material, satisfacción con el medio ambiente y sus condiciones de trabajo.\nCOMPONENTE 2\nLa segunda componente correlaciona principalmente con las dimensiones de experiencie general con la vida y la gobernanza y derechos básicos.\nEn el siguiente gráfico podemos ver las correlaciones de dichas variables con las componentes principales, como ya hemos comentado.\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nGráfico de los individuos\nTras observar la representación de las variables, en este apartado vemos la representación de los individuos sobre los nuevos ejes, con la idea de que aquellos con características similares, se agrupan cerca al tener puntuaciones parecidas. Las comunidades con valores cercanos a la media se situarán cerca del centro del gráfico (0,0).\nRepresentando los individuos sobre PC1 y PC2, vemos que Comunidades como Ceuta y Melilla, o Cataluña y el Madrid están muy próximas entre sí, indicando que tienden a tener un nivel de Mercado de Hipotécas similar.\n\n# Sobre PC1 y PC2\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(1, 2))",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "IMCV Pca",
      "Reducción Dimensionalidad - PCA: IMCV_pca"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando el método de Componentes Principales. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado MercadoHipotecas.xlsx, este dataset presenta los datos de estudio del mercado de adquisición de viviendas en propiedad en todas las comunidades autónomas durante el ejercicio 2021, relacionando las transmisiones inmobiliarias con las hipotecas constituidas, los índices de precio de vivienda y otras variables económicas y sociodemográficas. Nuestro objetivo es aplicar un procedimiento de PCA para conocer que variables independientes son de interés para estudiar el mercado inmobiliario y ver qué comunidades autónomas son las más parecidas y las más diferentes en términos de variables hipotecarias y sociodemográficas. Concretamente en este dataset tenemos las siguientes variables:\n\nCCAA: Comunidades Autónomas.\nHipotecaMedia: Importe de la hipoteca por cada comunidad autónoma promediadas por su tamaño.\nIndicedeVivienda: Índice del precio de la vivienda (IPV).\nTasaEjecHipotecarias: Número de Hipotecas Ejecutadas.\nHip100H: Número de hipotecas por cada cien mil habitantes en la comunidad autónoma.\nTasaParo: Tasas de paro por distintos grupos de edad, sexo y comunidad autónoma.\nTasaNatalidad: Tasa Bruta de Natalidad.\nNumHip: Cantidad de hipotecas en la comunidad autónoma.\nVV100H: Número de viviendas vacías por cada cien mil habitantes.\nIndEnvej: Porcentaje de personas mayores a 65 años.\nTasaMortalidad: Porcentaje de fallecidos por comunidad autónoma.\nTasaFecundidad: Tasa Global de Fecundidad por comunidad autónoma, según nacionalidad (española/extranjera) de la madre.\nActivos: Persona mayor de 16 años en búsqueda activa de trabajo .\nCompravent: Número de compraventas por comunidad autónoma.\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.\n\n\n\nSe pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar el mercado inmobiliario y ver qué comunidades autónomas son las más parecidas y las más diferentes en términos de variables hipotecarias y sociodemográficas.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#dataset",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#dataset",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado MercadoHipotecas.xlsx, este dataset presenta los datos de estudio del mercado de adquisición de viviendas en propiedad en todas las comunidades autónomas durante el ejercicio 2021, relacionando las transmisiones inmobiliarias con las hipotecas constituidas, los índices de precio de vivienda y otras variables económicas y sociodemográficas. Nuestro objetivo es aplicar un procedimiento de PCA para conocer que variables independientes son de interés para estudiar el mercado inmobiliario y ver qué comunidades autónomas son las más parecidas y las más diferentes en términos de variables hipotecarias y sociodemográficas. Concretamente en este dataset tenemos las siguientes variables:\n\nCCAA: Comunidades Autónomas.\nHipotecaMedia: Importe de la hipoteca por cada comunidad autónoma promediadas por su tamaño.\nIndicedeVivienda: Índice del precio de la vivienda (IPV).\nTasaEjecHipotecarias: Número de Hipotecas Ejecutadas.\nHip100H: Número de hipotecas por cada cien mil habitantes en la comunidad autónoma.\nTasaParo: Tasas de paro por distintos grupos de edad, sexo y comunidad autónoma.\nTasaNatalidad: Tasa Bruta de Natalidad.\nNumHip: Cantidad de hipotecas en la comunidad autónoma.\nVV100H: Número de viviendas vacías por cada cien mil habitantes.\nIndEnvej: Porcentaje de personas mayores a 65 años.\nTasaMortalidad: Porcentaje de fallecidos por comunidad autónoma.\nTasaFecundidad: Tasa Global de Fecundidad por comunidad autónoma, según nacionalidad (española/extranjera) de la madre.\nActivos: Persona mayor de 16 años en búsqueda activa de trabajo .\nCompravent: Número de compraventas por comunidad autónoma.\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "",
    "text": "Se pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar el mercado inmobiliario y ver qué comunidades autónomas son las más parecidas y las más diferentes en términos de variables hipotecarias y sociodemográficas.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(factoextra) # fviz_cluster function\nlibrary(ggcorrplot) # Para funcion ggcorrplot\nlibrary(corrplot) # Para corrplot",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#lectura-datos",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../../files/MercadoHipotecas.xlsx\", sheet = \"Datos\")\n\nEn primer lugar, cargamos los datos que vamos a utilizar. En este caso, se trata de un conjunto de datos compuesto por 19 filas y 14 columnas. Las filas corresponden a las 19 Comunidades Autónomas de España y las 14 columnas a variables que se han medido para cada una de ellas.\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores perdidos, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación, no hay ningún NA en los datos que vamos a utilizar.\n\nsum(is.na(datos))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(datos)\n\n[1] 19 14\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [19 × 14] (S3: tbl_df/tbl/data.frame)\n $ CCAA                : chr [1:19] \"Andalucía\" \"Aragón\" \"Asturias\" \"Baleares\" ...\n $ HipotecaMedia       : num [1:19] 125016 124092 104138 249109 137260 ...\n $ IndicedeVivienda    : num [1:19] 0.952 0.94 0.901 1.09 0.953 ...\n $ TasaEjecHipotecarias: num [1:19] 0.2299 0.0195 0.0163 0.0108 0.044 ...\n $ Hip100H             : num [1:19] 1297 1167 1087 1109 808 ...\n $ TasaParo            : num [1:19] 21.7 10.2 12.5 14.9 23.2 ...\n $ TasaNatalidad       : num [1:19] 7.72 7.21 4.74 7.81 5.67 5.63 5.5 7.1 7.52 7.06 ...\n $ NumHip              : num [1:19] 109869 15475 10994 13013 17547 ...\n $ VV100H              : num [1:19] 7521 7565 8189 6075 6363 ...\n $ IndEnvej            : num [1:19] 108 148 231 106 126 ...\n $ TasaMortalidad      : num [1:19] 9.33 11.06 13.25 7.22 7.63 ...\n $ TasaFecundidad      : num [1:19] 34.4 34.8 24.7 32.7 23.9 ...\n $ Activos             : num [1:19] 4023 645 447 649 1121 ...\n $ Compravent          : num [1:19] 117987 14935 9406 14168 20801 ...\n\n# Descripción de las variables\nsummary(datos)\n\n     CCAA           HipotecaMedia    IndicedeVivienda TasaEjecHipotecarias\n Length:19          Min.   : 91788   Min.   :0.8245   Min.   :0.0001933   \n Class :character   1st Qu.:105374   1st Qu.:0.9042   1st Qu.:0.0094955   \n Mode  :character   Median :120525   Median :0.9390   Median :0.0195226   \n                    Mean   :133389   Mean   :0.9559   Mean   :0.0526316   \n                    3rd Qu.:144501   3rd Qu.:1.0065   3rd Qu.:0.0489272   \n                    Max.   :249109   Max.   :1.1158   Max.   :0.2298734   \n    Hip100H          TasaParo      TasaNatalidad        NumHip      \n Min.   : 393.9   Min.   : 9.838   Min.   : 4.740   Min.   :   329  \n 1st Qu.: 993.0   1st Qu.:11.494   1st Qu.: 6.115   1st Qu.:  8910  \n Median :1109.4   Median :12.515   Median : 7.100   Median : 17126  \n Mean   :1074.0   Mean   :14.945   Mean   : 7.168   Mean   : 29379  \n 3rd Qu.:1231.6   3rd Qu.:17.740   3rd Qu.: 7.750   3rd Qu.: 26166  \n Max.   :1407.5   Max.   :26.663   Max.   :11.170   Max.   :109869  \n     VV100H         IndEnvej      TasaMortalidad   TasaFecundidad \n Min.   : 1585   Min.   : 46.23   Min.   : 6.760   Min.   :23.90  \n 1st Qu.: 5568   1st Qu.:110.60   1st Qu.: 8.250   1st Qu.:30.71  \n Median : 7565   Median :126.11   Median : 9.830   Median :32.91  \n Mean   : 7214   Mean   :134.11   Mean   : 9.724   Mean   :32.89  \n 3rd Qu.: 9907   3rd Qu.:153.13   3rd Qu.:10.930   3rd Qu.:34.62  \n Max.   :11177   Max.   :231.07   Max.   :13.250   Max.   :47.19  \n    Activos         Compravent    \n Min.   :  36.3   Min.   :   308  \n 1st Qu.: 381.9   1st Qu.:  8530  \n Median : 748.3   Median : 19785  \n Mean   :1221.2   Mean   : 29815  \n 3rd Qu.:1174.6   3rd Qu.: 24230  \n Max.   :4022.9   Max.   :117987  \n\n\nVemos que estas variables (a excepción de las CCAA) son todas de tipo numérico, y además, podemos obtener información como la media, desviación típica, los cuartiles y el histograma de cada una.\nCorrelación: El que existan correlaciones muy elevadas en el conjunto de datos nos permitirá resumir la información en un menor número de componentes principales, pues éstas explicarán una mayor cantidad de información.\n\nggcorrplot(cor(datos[, 2:14]), type = \"lower\", lab = T, show.legend = T)\n\n\n\n\n\n\n\n\nEn este caso, se ha generado un gráfico entre variables, sin tener en cuenta la correlación de la variable consigo misma, pues siempre será del 100%.\nEn términos absolutos, vemos que hay varias correlaciones moderadas/altas como entre las variables Activos y Compravent (del 98%) o entre IndEnvej y TasaMortalidad (del 78%). En ambos casos, la correlación es positiva, es decir, que crecen proporcionalmente. Respecto a la correlación negativa, encontramos valores muy altos para TasaNatalidad e IndEnvej. Las correlaciones más bajas corresponden a los pares TasaNatalidad y Compravent (2%) o HipotecaMedia y TasaFecundidad (-1%).\nEn resumen, vemos que hay varias variables con una alta correlación absoluta(tanto positiva como negativa), luego esto nos va a permitir resumir la información en un número de componentes principales menor al número de variables que aquí tenemos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#introducción-1",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de Componentes Principales (PCA) es una técnica para reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nSe busca encontrar nuevas variables (componentes) que estén incorrelacionadas y que capturen la máxima variabilidad de los datos. Esto se logra mediante combinaciones lineales de las variables originales. PCA es útil para entender relaciones, reducir dimensiones y manejar la alta correlación entre variables.\nPara aplicar PCA, se necesitan datos cuantitativos y es crucial escalar las variables (estandarizar = media cero y varianza uno). Esto garantiza que ninguna variable domine el análisis. Además, se puede trabajar con la matriz de correlaciones para abordar fuertes correlaciones entre variables, manteniendo así la información más relevante del conjunto de datos.\nLos pasos generales son:\n\nEstandarización de las variables: Es importante estandarizar las variables numéricas para que tengan media cero y desviación estándar uno. Esto es crucial para que ninguna variable domine el análisis debido a su escala.\nCálculo de la matriz de correlaciones o covarianzas: Dependiendo del enfoque, se puede trabajar con la matriz de correlaciones si se busca abordar fuertes correlaciones entre variables, o con la matriz de covarianzas si se busca la varianza total de las variables.\n\n\nNOTA: Aconsejable trabajar siempre con la matriz de correlaciones (a no ser que todas variables estén en las mismas unidades, que se podrá usar la matriz de covarianzas). De no seguir esta nota y usar la matriz de covarianzas, las variables que tienen mayores unidades dominarán la estructura de covarianza, lo que llevará a una representación inexacta de la variabilidad real de los datos.\n\n\nDescomposición de la matriz: Se descompone la matriz de correlaciones en sus vectores y valores propios. Los valores propios representan la cantidad de varianza explicada por cada componente principal, mientras que los vectores propios (autovectores) determinan la dirección de cada componente en el espacio multidimensional original.\nSelección de componentes: Los componentes se ordenan de manera descendente según la cantidad de varianza que explican. Los primeros componentes capturan la mayor variabilidad de los datos y se seleccionan para reducir la dimensionalidad manteniendo la información más relevante.\nTransformación de datos: Proyectar los datos originales en el espacio de los componentes principales para obtener las nuevas variables. Estas son combinaciones lineales de las variables originales y son ortogonales entre sí. Esta transformación lineal conserva la mayor parte de la información en un espacio de menor dimensión, lo que facilita el análisis y la visualización de los datos.\nInterpretación y visualización: Explorar la importancia de cada componente en términos de la variabilidad explicada. Se pueden interpretar los componentes para comprender qué aspectos de los datos capturan. Si es posible, representar gráficamente los datos en el espacio reducido de los componentes principales para obtener una mejor comprensión de las relaciones entre las observaciones.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#modelo",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#modelo",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Modelo",
    "text": "Modelo\nEn las siguientes lineas haremos que la variable CCAA se ponga como nombre de filas y posteriormente eliminaremos esa variable ya que ya la tendremos como nombre de filas.\n\nCCAA &lt;- datos$CCAA\ndatos &lt;- datos[, -1] # Eliminamos ahora\nrownames(datos) &lt;- CCAA # Como nombres de filas las CCAA\n\nEscalamos los datos y calculamos la matriz de varianzas covarianzas, mostramos solo la diagonal (debería ser 1).\n\ndatos2 &lt;- scale(datos)\nsummary(datos2)\n\n HipotecaMedia     IndicedeVivienda  TasaEjecHipotecarias    Hip100H       \n Min.   :-0.9751   Min.   :-1.5436   Min.   :-0.71140     Min.   :-2.9025  \n 1st Qu.:-0.6566   1st Qu.:-0.6068   1st Qu.:-0.58520     1st Qu.:-0.3455  \n Median :-0.3015   Median :-0.1983   Median :-0.44917     Median : 0.1511  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000     Mean   : 0.0000  \n 3rd Qu.: 0.2604   3rd Qu.: 0.5942   3rd Qu.:-0.05025     3rd Qu.: 0.6728  \n Max.   : 2.7123   Max.   : 1.8782   Max.   : 2.40454     Max.   : 1.4235  \n    TasaParo       TasaNatalidad          NumHip             VV100H       \n Min.   :-1.0305   Min.   :-1.62684   Min.   :-0.86162   Min.   :-1.8643  \n 1st Qu.:-0.6963   1st Qu.:-0.70550   1st Qu.:-0.60711   1st Qu.:-0.5451  \n Median :-0.4903   Median :-0.04549   Median :-0.36343   Median : 0.1162  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.5640   3rd Qu.: 0.39005   3rd Qu.:-0.09531   3rd Qu.: 0.8918  \n Max.   : 2.3643   Max.   : 2.68165   Max.   : 2.38726   Max.   : 1.3124  \n    IndEnvej       TasaMortalidad    TasaFecundidad         Activos        \n Min.   :-1.8856   Min.   :-1.5718   Min.   :-1.687729   Min.   :-0.92324  \n 1st Qu.:-0.5044   1st Qu.:-0.7817   1st Qu.:-0.408172   1st Qu.:-0.65392  \n Median :-0.1716   Median : 0.0561   Median : 0.003953   Median :-0.36848  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000000   Mean   : 0.00000  \n 3rd Qu.: 0.4082   3rd Qu.: 0.6394   3rd Qu.: 0.324077   3rd Qu.:-0.03633  \n Max.   : 2.0806   Max.   : 1.8696   Max.   : 2.685109   Max.   : 2.18293  \n   Compravent     \n Min.   :-0.8525  \n 1st Qu.:-0.6150  \n Median :-0.2898  \n Mean   : 0.0000  \n 3rd Qu.:-0.1614  \n Max.   : 2.5476  \n\ndiag(var(datos2))\n\n       HipotecaMedia     IndicedeVivienda TasaEjecHipotecarias \n                   1                    1                    1 \n             Hip100H             TasaParo        TasaNatalidad \n                   1                    1                    1 \n              NumHip               VV100H             IndEnvej \n                   1                    1                    1 \n      TasaMortalidad       TasaFecundidad              Activos \n                   1                    1                    1 \n          Compravent \n                   1 \n\n\nAplicamos funcion PCA, notar que en este caso no haría falta los argumentos SCALE=TRUE y CENTER=TRUE puesto que ya hemos escalado dos datos en un paso previo. Por defecto en la función viene el valor de SCALE=FALSE y CENTER=TRUE.\n\npca &lt;- prcomp(datos2, center = TRUE, scale = TRUE) # Scale=T\n\nCalculamos los coeficientes de la ecuación para cada componente principal\n\npca$rotation\n\n                             PC1         PC2         PC3         PC4\nHipotecaMedia         0.23447260 -0.06310254  0.60036701 -0.14352759\nIndicedeVivienda      0.35056462  0.03492200  0.33265184 -0.21595433\nTasaEjecHipotecarias  0.18895585 -0.38696171 -0.29859399 -0.04395518\nHip100H               0.02384814 -0.32139415  0.21029632  0.52700520\nTasaParo              0.17638635  0.16978548 -0.45544384 -0.50048376\nTasaNatalidad         0.34697038  0.22377034 -0.16525154  0.33885102\nNumHip                0.23253409 -0.41264772 -0.06210480 -0.03592632\nVV100H               -0.31862705 -0.19954134 -0.24466170  0.14612450\nIndEnvej             -0.38398575 -0.20108861  0.16993058 -0.09362279\nTasaMortalidad       -0.39081025 -0.14283197 -0.08256307  0.03905432\nTasaFecundidad        0.27908332  0.21917010 -0.19293982  0.49563579\nActivos               0.22264109 -0.41490535 -0.05417093 -0.08779747\nCompravent            0.22610583 -0.41442757 -0.14564495 -0.03868128\n                             PC5         PC6         PC7         PC8\nHipotecaMedia         0.22721287  0.25793850  0.52980535  0.30139664\nIndicedeVivienda     -0.29883955  0.25484457 -0.69422079  0.26266824\nTasaEjecHipotecarias -0.04578494  0.09016705 -0.14237630 -0.14450223\nHip100H               0.38422611 -0.46625149 -0.28074363  0.33169721\nTasaParo              0.26991529 -0.24907203  0.02930616  0.52896695\nTasaNatalidad        -0.10947984  0.16047629  0.05441172  0.12043767\nNumHip               -0.11719298 -0.06702702  0.13681214  0.03619294\nVV100H                0.39357805  0.72591201 -0.13624112  0.21415518\nIndEnvej             -0.29873692 -0.04475899 -0.01138938  0.04181744\nTasaMortalidad       -0.51193330 -0.04229536  0.15474088  0.56363472\nTasaFecundidad       -0.29559220  0.11660486  0.20014738  0.16672803\nActivos              -0.12111322  0.07773141  0.16598060 -0.14274555\nCompravent           -0.05002690 -0.01843676  0.07412227  0.01306008\n                              PC9          PC10        PC11          PC12\nHipotecaMedia        -0.262791756  0.0251241391 -0.01163925 -0.0004333459\nIndicedeVivienda      0.070224200 -0.0399012682  0.01986345 -0.0797306119\nTasaEjecHipotecarias -0.775723474 -0.0024548350 -0.13676702  0.0096868413\nHip100H              -0.009941869  0.0447484933 -0.12820466 -0.0421913182\nTasaParo              0.030071820  0.2166825934 -0.06628922 -0.1170645772\nTasaNatalidad         0.036325847  0.3089459905 -0.21757169  0.6953808523\nNumHip                0.366200530 -0.0152004504  0.42093030  0.1793240415\nVV100H                0.153187336  0.0443334585  0.05124612 -0.0468669300\nIndEnvej             -0.054795835  0.8212580823  0.03498288 -0.0514083908\nTasaMortalidad       -0.085630316 -0.4010667812 -0.13596976  0.1584267923\nTasaFecundidad       -0.050617589  0.1249734888  0.15590775 -0.6136362733\nActivos               0.389029556  0.0153261020 -0.69630375 -0.2279291148\nCompravent            0.011356353  0.0002626747  0.45158656  0.0625588127\n                              PC13\nHipotecaMedia         0.0219537496\nIndicedeVivienda     -0.0113296319\nTasaEjecHipotecarias  0.2270146192\nHip100H              -0.0002262752\nTasaParo              0.0252507795\nTasaNatalidad        -0.0674299774\nNumHip                0.6233573899\nVV100H                0.0302959223\nIndEnvej              0.0041371857\nTasaMortalidad       -0.0354555546\nTasaFecundidad        0.0577103822\nActivos              -0.1031295200\nCompravent           -0.7334418197\n\n\nPodemos observar aquí nuestras variables en el nuevo sistema de coordenadas (las componentes principales), dando lugar a ecuaciones de cada eje como combinación lineal del total de variables. Analizar el vector que crea cada componente y cuáles son los pesos que tienen las variables en cada componente, ayuda a interpretar qué tipo de información recoge cada una de ellas.\nPor ejemplo, la primera componente principal (PC1), presenta la siguiente ecuación, como combinación lineal de las siete variables originales:\n$ PC_1 = 0.23 {HipotecaMedia} + 0.35 {IndicedeVivienda} + 0.18TasaEjecHipotecarias + 0.023 Hip100H + 0.17 TasaParo + 0.34 TasaNatalidad + 0.23 NumHip -0.31VV100H -0.38IndEnvej -0.39TasaMortalidad +0.27TasaFecundidad + 0.22Activos +0.22Compravent$\nExtraemos las nuevas coordenadas de los individuos (puntuaciones)\nAdemás, podemos ver las puntuaciones, que son las coordenadas de cada observación original (Comunidad Autónoma) sobre los nuevos ejes construidos (componentes principales). Esto corresponde a un cambio de coordenadas bajo el paradigma del Álgebra Lineal.\n\npca$x\n\n                          PC1         PC2         PC3         PC4          PC5\nAndalucía           2.7455023 -3.67317229 -2.12660745 -0.29682024  0.110706075\nAragón             -0.9699327  0.32725095  0.46486382  1.06149710 -0.472663618\nAsturias           -3.5869740 -0.61984654  0.59178552 -0.75737644 -0.703643431\nBaleares            1.7825614  1.10268112  2.51261640 -0.46389719  1.035908384\nCanarias           -0.1301632  0.62843489 -0.23431304 -2.65430954  1.230373539\nCantabria          -1.9979724 -0.01197302  0.82267569  0.26001624  0.450369088\nCastilla León      -2.7026848 -1.01136738 -0.11497356 -0.16914178 -0.536890298\nCastilla La Mancha -0.9290027  0.21256139 -0.74011673  0.53011163  0.848851231\nCataluña            2.7389745 -2.91486340  0.52836881 -0.26063991 -0.837474574\nCvalenciana         0.6478324 -2.73565870 -1.70915179  0.36886681  0.314182432\nExtremadura        -1.9681281  0.80998146 -1.49001512 -0.04715111  0.276129920\nGalicia            -2.8544978 -0.59320383 -0.13237242 -0.89058464 -0.786696609\nMadrid              3.3662356 -2.41685601  2.02910270  0.02964115 -0.009936487\nMurcia              0.6578774  1.18636724 -1.21821799  1.72054150  0.373052356\nNavarra            -0.1447226  1.41272907  0.68283945  0.99630793  0.118156816\nPaisVasco          -0.3644187 -0.15990677  1.81164513  0.51836980 -0.407434600\nLa Rioja           -1.8046126  0.53640665 -0.04130292  1.11905222  0.441014002\nCeuta               2.1698869  3.93962047 -0.87475999 -2.28618673 -0.795605090\nMelilla             3.3442391  3.98081471 -0.76206651  1.22170320 -0.648399135\n                            PC6         PC7         PC8         PC9        PC10\nAndalucía          -0.464167091  0.27301087  0.39570536  0.03036640  0.10008293\nAragón              0.000578174 -0.05973468  0.11517510 -0.04716968 -0.17316784\nAsturias           -0.667474209 -0.26393758  0.26901900 -0.14543133  0.18368390\nBaleares            0.863799139  0.07742047  0.58825882 -0.41385644  0.15314700\nCanarias           -0.366682515 -0.15143184 -0.55567081  0.07533031  0.09050859\nCantabria          -0.657556125 -0.93524427 -0.13942883  0.05217030 -0.13731336\nCastilla León       0.432311346 -0.07605104  0.01978793  0.02206576  0.14977190\nCastilla La Mancha  0.104829825  0.59710033  0.01973866  0.32991103 -0.18684983\nCataluña            0.570424085  0.03427750 -0.30040647 -0.10886475 -0.19763763\nCvalenciana         0.076833291 -0.42483946 -0.13046479 -0.67979305 -0.02854897\nExtremadura        -0.127527331  0.63183605  0.55695797  0.18377774  0.01817693\nGalicia             0.974657578  0.15586395 -0.28482319  0.23684134  0.15868031\nMadrid             -0.201133176 -0.16638968  0.00496925  0.68770653  0.05211501\nMurcia              0.284400425 -0.17269157 -0.35486145  0.14631769  0.08309741\nNavarra            -0.145413390  0.82387546 -0.70187692 -0.27220149  0.05649532\nPaisVasco          -0.941793203  0.63362573  0.04668404 -0.22152370 -0.09684373\nLa Rioja            0.492628649 -0.52220202  0.26539476  0.12958325 -0.21612799\nCeuta               0.047827143 -0.03390410  0.13400159 -0.05399996 -0.29299263\nMelilla            -0.276542616 -0.42058411  0.05183999  0.04877006  0.28372268\n                           PC11          PC12         PC13\nAndalucía           0.073488769 -0.0079224783 -0.029748511\nAragón             -0.011186846 -0.0039848793 -0.025819489\nAsturias           -0.037865075  0.0905049079 -0.004029230\nBaleares            0.002150701  0.0066804364 -0.010156402\nCanarias           -0.075883351 -0.0244823013 -0.001124104\nCantabria           0.020726590 -0.0290358175 -0.015005169\nCastilla León       0.069655256 -0.1046768332  0.010952963\nCastilla La Mancha -0.007564225  0.0149590302 -0.033822596\nCataluña           -0.166811234 -0.0099003952 -0.002594989\nCvalenciana         0.037928332  0.0171151462  0.023516428\nExtremadura        -0.098492637 -0.0145216149  0.038712801\nGalicia             0.027262055  0.0230609545 -0.015291536\nMadrid              0.068182987  0.0235482170  0.028696809\nMurcia             -0.025861394  0.0501499420  0.013033339\nNavarra             0.092609193  0.0121100316  0.005740842\nPaisVasco          -0.022083454 -0.0305590536  0.007893858\nLa Rioja            0.019684769  0.0004476988  0.013228807\nCeuta               0.092598574  0.0187605584  0.009238707\nMelilla            -0.058539011 -0.0322535498 -0.013422528\n\n\nVarianza explicada por cada componente principal\nUna vez calculadas las componentes principales, es de interés conocer la varianza explicada por cada una, ya que el principal objetivo que se sigue con PCA es maximizar la cantidad de información explicada por las componentes.\n\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5    PC6     PC7\nStandard deviation     2.2074 2.0355 1.2656 1.12100 0.64943 0.5175 0.44767\nProportion of Variance 0.3748 0.3187 0.1232 0.09667 0.03244 0.0206 0.01542\nCumulative Proportion  0.3748 0.6935 0.8167 0.91340 0.94584 0.9664 0.98186\n                           PC8     PC9    PC10    PC11    PC12    PC13\nStandard deviation     0.34381 0.29028 0.16368 0.06853 0.03917 0.01970\nProportion of Variance 0.00909 0.00648 0.00206 0.00036 0.00012 0.00003\nCumulative Proportion  0.99095 0.99743 0.99949 0.99985 0.99997 1.00000\n\n\n\nStandard deviation: muestra las desviaciones estándar de cada componente principal. Si elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\nProportion of Variance: es la proporción de la varianza total que explica cada componente principal y quizá, es la fila más importante de nuestros resultados. Como los autovalores están ordenados de mayor a menor y así son construidas las componentes principales, la primera componente principal es la que mayor porcentaje de variabilidad explica, un 37%. Así, la varianza explicada por la componentes van en orden decreciente, teniendo que la segunda componente explica un 31% y la tercera, un 12%.\nCumulative proportion: es la varianza acumulada y se calcula progresivamente sumando la Proportion of Variance anterior. En vista de estos resultados, vemos que la primera componente agrupa el 37% de la variación, y que necesitamos 3 componentes para alcanzar el 80%.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#selección-de-componentes",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#selección-de-componentes",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Selección de componentes",
    "text": "Selección de componentes\nGraficando el valor de la varianza de cada componente principal, podemos observar los resultados comentados anteriormente, que las primeras componentes son las que más varianza explican y que a medida que se añaden más, la varianza explicada por cada una es menor.\n\n# fviz_eig(pca, main=\"Varianza de cada componente\", choice = \"eigenvalue\", addlabels = T)\n\no como el porcentaje de varianza explicada por cada componente sobre el total.\n\nfviz_screeplot(pca, addlabels = TRUE, main = \"Porcentaje de varianza explicada por cada componente (%)\")\n\n\n\n\n\n\n\n\nA continuación, representamos las varianzas acumuladas:\n\nplot(summary(pca)$importance[3, ], type = \"o\", col = \"darkblue\", lwd = 3, main = \"Porcentaje de varianza acumulada\", xlab = \"Componente Principal\", ylab = \"Porcentaje de varianza acumulada\")\n\n\n\n\n\n\n\n\nDeterminar el número de componentes que elegir para continuar con el análisis no tiene unas normas determinadas a seguir. Respecto a ello, existen varios criterios con sus respectivas propuestas.\n\nUna opción para determinar el número de componentes principales que seleccionar, es coger aquellas tal que expliquemos un % determinado de la variabilidad de los datos que nosotros prefijemos. Generalmente se pone como umbral mínimo un 80%, entonces necesitaríamos elegir 3 componentes.\nOtra posibilidad es seguir el criterio de Kaisser, que escoge aquellas componentes cuyo autovalor sea superior a 1 (cuando las variables han sido generadas a partir de la matriz de correlaciones). Según este criterio y mirando el gráfico anterior de la varianza (igual a eigenvalues), eligiríamos las cuatro primeras componentes. Este criterio, cuando p &lt; 20, tiende a ser estricto e incluir pocas componentes.\nPara relajar el criterio de Kaisser, existe la modificación de Jollife, que elige aquellas componentes cuyo autovalor sea superior a 0.7. Esta modificación, nos permite elegir igualmente 4 componentes.\n\nEn este caso, nos podríamos quedar con las 4 primeras componentes principales, ya que es el número en el que coincide el mayor número de criterios. Por tanto, en lugar de trabajar con las 13 variables originales, trabajaremos con 4 variables nuevas, que son combinaciones de ellas.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#interpretación",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#interpretación",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Interpretación",
    "text": "Interpretación\nHemos decidido quedarnos con 4 componentes principales, que explican el 91% de la variabilidad total. Para realizar su interpretación, volvemos a ver los coeficientes de las ecuaciones de los componentes, observando cuáles son los valores más altos (en valor absoluto), para así poder dar una interpretación a cada eje.\n\n# Autovectores de las primeras 3 componentes\ncov(pca$rotation[, 1:4])\n\n             PC1          PC2           PC3           PC4\nPC1  0.074284136  0.014541400  0.0029560973 -0.0029453827\nPC2  0.014541400  0.059966364 -0.0047502326  0.0047330151\nPC3  0.002956097 -0.004750233  0.0823676664  0.0009621668\nPC4 -0.002945383  0.004733015  0.0009621668  0.0823746540\n\n\nGráficamente, también podemos ver la contribución de las variables a los 3 primeros ejes, señalando en color azul las variables que puntúan positivamente en el eje, y en rojo, las que lo hacen de forma negativa.\n\npca$rotation[, 1:4]\n\n                             PC1         PC2         PC3         PC4\nHipotecaMedia         0.23447260 -0.06310254  0.60036701 -0.14352759\nIndicedeVivienda      0.35056462  0.03492200  0.33265184 -0.21595433\nTasaEjecHipotecarias  0.18895585 -0.38696171 -0.29859399 -0.04395518\nHip100H               0.02384814 -0.32139415  0.21029632  0.52700520\nTasaParo              0.17638635  0.16978548 -0.45544384 -0.50048376\nTasaNatalidad         0.34697038  0.22377034 -0.16525154  0.33885102\nNumHip                0.23253409 -0.41264772 -0.06210480 -0.03592632\nVV100H               -0.31862705 -0.19954134 -0.24466170  0.14612450\nIndEnvej             -0.38398575 -0.20108861  0.16993058 -0.09362279\nTasaMortalidad       -0.39081025 -0.14283197 -0.08256307  0.03905432\nTasaFecundidad        0.27908332  0.21917010 -0.19293982  0.49563579\nActivos               0.22264109 -0.41490535 -0.05417093 -0.08779747\nCompravent            0.22610583 -0.41442757 -0.14564495 -0.03868128\n\n\nSi nos fijamos en los pesos más altos, podemos darle una interpretación a cada eje. Por ejemplo:\n\nLa primera componente explica un 40% de la variación. Hay valores absolutos bastante similares y elevados, que son los correspondientes con las variables Ind_envej, T_mort, T_nat y Tasa_enf. Por lo tanto, parece que la primera componente recoge información demográfica. Teniendo en cuenta los signos podemos concluir que las CC.AA. que se sitúen a la derecha del eje serán aquellas con mayor Tasa de mortalidad, mayor Índice de envejecimiento, mayor Tasa de incidencia de enfermedades en la población, y en contraposición, menor Tasa de natalidad.\nEn la segunda componente, los pesos más elevados corresponden con las variables Médicos y Enfermeros, representando de alguna forma, los recursos sanitarios de las CCAA. Ambas variables contribuyen de forma positiva al eje, por lo que cuanto más a la derecha del eje se sitúe una CC.AA., mayores recursos de personal sanitario posee.\nPara la tercera componente, el peso más elevado y con gran diferencia sobre el resto, corresponde a la variable medidora de la inaccesibilidad de la población a los medicamentos recetados. La variable puntúa negativamente en el eje, de forma que las Comunidades con mayor valor en esta componente, son aquellas con menor inaccesibilidad a los medicamentos.\nEn la cuarta componente",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#representación-gráfica",
    "href": "notebooks/Reduccion Dimension/PCA/MercadoHipotecas/MercadoHipotecas.html#representación-gráfica",
    "title": "Reducción Dimensionalidad - PCA: MercadoHipotecas",
    "section": "Representación gráfica",
    "text": "Representación gráfica\nGráfico de las variables\nRepresentamos sobre las dos primeras componentes principales las variables originales. En el eje de abscisas se representa la PC1 y en el eje de ordenadas, la PC2. Para interpretar correctamente las variables tenemos que fijarnos en la longitud de la flecha y en el ángulo que forman respecto a los ejes y entre ellos mismos.\n\nÁngulo vector - eje: cuanto más paralelo es un vector al eje, más ha contribuido a dicha componente principal.\nÁngulo entre dos vectores: si es pequeño representa una alta correlación entre las variables implicadas (y por tanto, observaciones con valores altos en una variable, tendrá valores altos en la otra). Si el ángulo es cercano a 90º indica que las variables están incorreladas y los ángulos opuestos indican correlación negativa entre ellas.\nLongitud: cuanto mayor es la longitud de un vector, mayor varianza de la variable está contenida en el biplot, es decir, mejor representada está en el gráfico.\n\nEn el gráfico, diferenciamos por colores las variables según su calidad de representación en las dos primeras componentes. Cuanto más cerca esté una variable del círculo de correlaciones, mejor será su representación, por lo que las variables que estén muy cerca del centro de la gráfica son las menos importantes para las dos primeras componentes.\n\nfviz_pca_var(pca, axes = c(1, 2), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\nfviz_pca_var(pca, axes = c(3, 4), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\n\nRESUMEN DE RESULTADOS\nLos gráficos obtenidos muestran una visualización de las variables en función de las componentes principales 1 y 2, y 3 y 4. Las variables que tienen una correlación alta con la primera (y tercera) componente principal están más cerca del eje horizontal y las variables que tienen una alta correlación con la segunda componente principal(y cuarta) están más cerca del eje vertical. Las variables que se encuentran cerca del centro del gráfico tienen una correlación baja con ambas componentes principales, aún así nos guiaremos con la tabla para decidir qué variables están mejor explicadas por cada una de las componentes. En resumen, estos gráficos proporcionan una representación visual de las relaciones entre las variables en función de las dos primeras componentes principales y las dos segundas, lo que puede ayudar a identificar patrones y tendencias en los datos.\nCOMPONENTE 1\nEl índice del precio de la vivienda es un indicador importante para el mercado hipotecario ya que afecta el costo de las hipotecas y la capacidad de los compradores para obtener financiamiento. Si el precio de la vivienda es alto, es posible que las personas tengan dificultades para pagar su hipoteca y, por lo tanto, se reducirá la demanda de préstamos hipotecarios.\nEl número de viviendas vacías por comunidad autónoma por cada 100 mil habitantes también es un factor importante en el mercado hipotecario. Si hay muchas viviendas vacías en una determinada área, es posible que el valor de las propiedades disminuya, lo que puede dificultar la venta de propiedades y la obtención de préstamos hipotecarios.\nEl índice de envejecimiento, por su parte, puede tener un impacto en la demanda de viviendas en el mercado hipotecario. Si hay una población envejecida en una determinada área, es posible que haya menos demanda de viviendas, lo que puede disminuir el valor de las propiedades y hacer que sea más difícil obtener financiamiento.\nEn conjunto, la primera componente parece estar relacionada con la oferta y la demanda de viviendas en el mercado hipotecario y, por lo tanto, podría ser importante para entender las condiciones del mercado inmobiliario en una determinada región. El nombre de la componente podría ser “Indicadores del mercado hipotecario”.\nCOMPONENTE 2\nTasa de ejecuciones hipotecarias: Este indicador mide la cantidad de ejecuciones hipotecarias que se han llevado a cabo en una determinada región en un periodo de tiempo específico. La tasa de ejecuciones hipotecarias puede reflejar la salud del mercado inmobiliario y la capacidad de los prestatarios para cumplir con sus pagos hipotecarios. Si la tasa de ejecuciones hipotecarias es alta, esto puede indicar que hay un mayor riesgo crediticio en la región y puede afectar la percepción de los prestamistas y compradores de vivienda.\nNúmero de hipotecas por comunidad autónoma: Este indicador mide la cantidad de hipotecas concedidas en una determinada región en un periodo de tiempo específico. El número de hipotecas puede reflejar la salud del mercado inmobiliario y la demanda de vivienda en la región. Si el número de hipotecas es alto, esto puede indicar una mayor actividad en el mercado hipotecario y una mayor demanda de vivienda en la región.\nNúmero de personas activas (Encuesta EPA): La Encuesta de Población Activa (EPA) es una encuesta que mide el número de personas en edad laboral que están trabajando o buscando trabajo. El número de personas activas puede tener un impacto en el mercado hipotecario, ya que si hay muchas personas empleadas en una determinada región, es posible que haya una mayor capacidad de los prestatarios para cumplir con sus pagos hipotecarios y una mayor demanda de vivienda en la región.\nNúmero de viviendas adquiridas en compraventa en el año 2021 por comunidad autónoma: Este indicador mide la cantidad de viviendas que se han comprado y vendido en una determinada región durante el año 2021. Este dato puede reflejar la actividad en el mercado inmobiliario y la demanda de vivienda en la región. Si el número de viviendas adquiridas en compraventa es alto, esto puede indicar una mayor actividad en el mercado hipotecario y una mayor demanda de vivienda en la región.\nTeniendo en cuenta las variables que componen la segunda componente, un posible nombre resumen podría ser “Riesgo Hipotecario”. Esta componente refleja tanto la actividad en el mercado hipotecario, medida por el número de hipotecas y viviendas adquiridas en compraventa, como la estabilidad del mercado, medida por la tasa de ejecuciones hipotecarias y el número de personas activas en la región.\nCOMPONENTE 3\nValor medio de las hipotecas: este indicador mide el valor promedio de las hipotecas concedidas en una determinada región en un periodo de tiempo específico. El valor medio de las hipotecas puede ser un indicador de la capacidad de los prestatarios para obtener financiación y adquirir una vivienda en la región.\nTasa de desempleo: este indicador mide la proporción de personas en edad laboral que están desempleadas en una determinada región en un periodo de tiempo específico. La tasa de desempleo puede tener un impacto en el mercado hipotecario, ya que si hay muchas personas desempleadas en la región, es posible que haya una menor capacidad de los prestatarios para cumplir con sus pagos hipotecarios.\nEn conjunto, estas variables pueden proporcionar información sobre la salud (estabilidad) financiera de los prestatarios en la región y su capacidad para afrontar los pagos hipotecarios.\nCOMPONENTE 4\nNúmero de hipotecas por cada 100.000 habitantes: este indicador mide el número de hipotecas concedidas en una determinada región en relación con el número de habitantes de la misma. Este indicador puede proporcionar información sobre la actividad del mercado hipotecario en la región y la demanda de viviendas.\nTasa de natalidad: este indicador mide la proporción de nacimientos en una determinada región en relación con el número total de habitantes. La tasa de natalidad puede ser un indicador de la demanda futura de viviendas, ya que las parejas jóvenes que tienen hijos suelen buscar viviendas más grandes y espaciosas para acomodar a sus familias.\nTasa de fecundidad: este indicador mide el número promedio de hijos por mujer en una determinada región. La tasa de fecundidad también puede ser un indicador de la demanda futura de viviendas, ya que si la tasa de fecundidad es alta, puede haber una mayor demanda de viviendas más grandes y espaciosas.\nEn conjunto, estas variables pueden proporcionar información sobre la actividad del mercado hipotecario en la región y la demanda futura de viviendas.\nEn el siguiente gráfico podemos ver las correlaciones de dichas variables con las componentes principales, como ya hemos comentado.\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nEs llamativo como las tres o cuatro primeras componentes son las más importantes en el PCA, sobre todo, la PC1.\nEn resumen, las nuevas componentes han permitido identificar patrones y características de las comunidades autónomas en términos de mercado hipotecario, riesgo hipotecario, estabilidad financiera y demanda futura. Este análisis proporciona información valiosa para comprender mejor las diferencias y similitudes entre las comunidades autónomas y puede ser útil para tomar decisiones en términos de políticas públicas y estrategias empresariales.\nGráfico de los individuos\nTras observar la representación de las variables, en este apartado vemos la representación de los individuos sobre los nuevos ejes, con la idea de que aquellos con características similares, se agrupan cerca al tener puntuaciones parecidas. Las comunidades con valores cercanos a la media se situarán cerca del centro del gráfico (0,0).\nRepresentando los individuos sobre PC1 y PC2, vemos que Comunidades como Ceuta y Melilla, o Cataluña y el Madrid están muy próximas entre sí, indicando que tienden a tener un nivel de Mercado de Hipotecas similar.\n\n# Sobre PC1 y PC2\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(1, 2))\n\n\n\n\n\n\n\n\n\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(3, 4))\n\n\n\n\n\n\n\n\nPara poder extraer fácilmente los perfiles, podemos combinar las variables e individuos en un solo gráfico que nos permita identificar qué Comunidades se encuentran en una situación parecida y además, que nos permita identificar sus características.\nBiplot\nEl biplot permite la representación conjunta de los individuos y las variables sobre los nuevos ejes. Para que el resultado sea fácilmente interpretable, debemos tener pocas variables e individuos en el conjunto de datos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "MercadoHipotecas",
      "Reducción Dimensionalidad - PCA: MercadoHipotecas"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html",
    "href": "notebooks/Logistic Regression/salud/salud.html",
    "title": "Regresión Logística: salud",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos mediante una Regresión Logística para discriminar el sexo de una persona a partir de los microdatos de la Encuesta Nacional de Salud. Resultados. Concretamente, se han tomado los datos relativos a 2017.\n\n\nEn este cuaderno vamos a analizar el dataset llamado salud.xlsx. Las variables de interés son las siguientes:\n\nEDAD: Identificación del adulto seleccionado: Edad.\nSEXO: Identificación del adulto seleccionado: Sexo. (1=Hombre, 2=Mujer)\nAltura: Altura en cm.\nPeso: Peso en kg.\n\n\n\n\nSe pretende hacer una regresión logística que clasifique la variable respuesta Sexo en función de varios predictores, todos ellos continuos: Edad, Altura, Peso.\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#dataset",
    "href": "notebooks/Logistic Regression/salud/salud.html#dataset",
    "title": "Regresión Logística: salud",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado salud.xlsx. Las variables de interés son las siguientes:\n\nEDAD: Identificación del adulto seleccionado: Edad.\nSEXO: Identificación del adulto seleccionado: Sexo. (1=Hombre, 2=Mujer)\nAltura: Altura en cm.\nPeso: Peso en kg.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Logistic Regression/salud/salud.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Logística: salud",
    "section": "",
    "text": "Se pretende hacer una regresión logística que clasifique la variable respuesta Sexo en función de varios predictores, todos ellos continuos: Edad, Altura, Peso.\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#cargar-librerías",
    "href": "notebooks/Logistic Regression/salud/salud.html#cargar-librerías",
    "title": "Regresión Logística: salud",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(Epi) # Para la ROC curve\nlibrary(caret) # para la confusion matrix",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#lectura-de-datos",
    "href": "notebooks/Logistic Regression/salud/salud.html#lectura-de-datos",
    "title": "Regresión Logística: salud",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/salud.xlsx\", sheet = \"Datos\")\n\n\n# Creamos un gráfico de quesito o de sectores para ver qué proporción de personas ha mejorado su situación económica:\n\n\ndatos_quesito &lt;- c(nrow(filter(datos, Sexo == \"1\")), nrow(filter(datos, Sexo == \"2\")))\netiquetas &lt;- c(\"Hombre\", \"Mujer\")\n\nporcentajes &lt;- paste0(round(datos_quesito / sum(datos_quesito) * 100, 1), \"%\")\ncolores &lt;- c(\"lightgreen\", \"lightcoral\")\n\npie(datos_quesito, labels = porcentajes, col = colores, radius = 1.05, cex = 1)\nlegend(\"topleft\", etiquetas, cex = 0.7, fill = colores)\n\n\n\n\n\n\n\n\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Peso)) +\n  geom_density(aes(group = Sexo, colour = Sexo, fill = Sexo),\n    alpha = 0.2\n  )\n\n\n\n\n\n\n\n\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Altura)) +\n  geom_density(\n    aes(\n      group = Sexo,\n      colour = Sexo,\n      fill = Sexo\n    ),\n    alpha = 0.2\n  )\n\n\n\n\n\n\n\n\nVemos la distribución de la altura y el peso distinguiendo entre sexos, se observa como parte de su rango es común pero en los extremos de cada uno se puede distinguir con cierta facilidad si es Hombre/Mujer.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#introducción-1",
    "href": "notebooks/Logistic Regression/salud/salud.html#introducción-1",
    "title": "Regresión Logística: salud",
    "section": "Introducción",
    "text": "Introducción\nUn análisis de regresión logística es una técnica estadística multivariante que tiene como finalidad clasificar las observaciones de una variable dependiente categórica a partir de una o varias variables independientes categóricas o continuas. Dichas variables independientes reciben el nombre de covariables. Asimismo, a diferencia de lo que suele hacerse cuando tenemos una variable respuesta continua, cuando esta es categórica, no interesa describir o pronosticar los valores concretos de dicha variable, sino la probabilidad de pertenecer a cada una de las categorías de la misma.\nAunque matemáticamente se pueda ajustar un modelo de regresión lineal clásico a la relación entre una variable dependiente categórica y una o varias covariables, cuando la variable dependiente es dicotómica (regresión logística binaria, caso más sencillo de regresión logística) no es apropiado utilizar un modelo de regresión lineal porque una variable dicotómica no se ajusta a una distribución normal, sino a una binomial. Ignorar esta cuestión podría llevar a obtener probabilidades imposibles: menores que cero o mayores que uno.\nPara evitar este problema, es preferible utilizar funciones que realicen predicciones comprendidas entre un máximo y un mínimo. Una de estas funciones - posiblemente la más empleada - es la curva logística o función sigmoide:\n\\[\\begin{align}\n\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\n\\end{align}\\]\nEs decir, estamos estimando con una regresión lineal el valor de \\(\\eta\\), que sí es una v.a. continua - a diferencia de Y que es binaria-.\nEsto es, \\(p=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\). De esta forma, para valores positivos muy grandes de \\(\\eta\\) llamado odds, \\(e^{-\\eta}\\) es aproximadamente cero, por lo que el valor de la función es 1; mientras que para valores negativos muy grandes de \\(\\eta\\), \\(e^{-\\eta}\\) tiende a infinito, haciendo que el valor de la función sea 0.\nA continuación, para simplificar un poco las cosas, consideremos el modelo de regresión logística más sencillo: regresión logística binaria simple (una sola covariable):\n[ P(Y=1)= ]\nLa interpretación de esta función es muy similar a la de una regresión lineal: el coeficiente \\(\\beta_0\\) representa la posición de la curva sobre el eje horizontal o de abscisas (más hacia la izquierda o más hacia la derecha); mientras que \\(\\beta_1\\) representa la pendiente de la curva, es decir, cuán inclinada está en su parte central (cuanto más inclinada, mayor capacidad de discriminar entre los dos valores de la variable dependiente).\nEjemplo sencillo Vamos a mostrar como una variable binaria no tiene sentido predecirla con una Regresión Lineal sino Logística.\n\n# Generación de datos para el ejemplo\nset.seed(123)\nn &lt;- 200\nAltura &lt;- rnorm(n, mean = 165, sd = 10)\n\n# Crear una variable binaria 'Sexo' en función de Altura\nSexo &lt;- as.factor(ifelse(Altura + rnorm(n) &gt; 165, 1, 0))\ndatos_ejemplo &lt;- data.frame(Altura, Sexo)\n\n# Regresión lineal\nmodelo_lineal &lt;- lm(Sexo ~ Altura, data = datos_ejemplo)\n\n# Regresión logística\nmodelo_logistico &lt;- glm(Sexo ~ Altura, data = datos_ejemplo, family = binomial)\n\n\n# Regresión Lineal\npar(mfrow = c(1, 2))\nplot(datos_ejemplo$Altura, datos_ejemplo$Sexo, col = \"lightblue\", main = \"Ajuste por Regresión Lineal\")\nabline(modelo_lineal, col = \"navy\")\n\n# Regresión Logística\n\nplot(datos_ejemplo$Altura, as.numeric(datos_ejemplo$Sexo) - 1, col = \"lightblue\", main = \"Regresión Logística\", xlab = \"Altura\", ylab = \"Sexo\")\ncurve(predict(modelo_logistico, data.frame(Altura = x), type = \"response\"), add = TRUE, col = \"navy\", lwd = 2)\n\n\n\n\n\n\n\n\nEn este ejemplo, se muestra cómo un ajuste por regresión lineal no se adapta bien a datos binarios, produciendo predicciones que pueden ser mayores que 1 o menores que 0. En cambio, la regresión logística produce una curva en forma de S que se adapta mejor a los datos, con predicciones que están siempre entre 0 y 1. Esto demuestra que para problemas de clasificación binaria, la regresión logística es una mejor opción que la regresión lineal.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#bondad-de-ajuste-e-interpretación-modelo",
    "href": "notebooks/Logistic Regression/salud/salud.html#bondad-de-ajuste-e-interpretación-modelo",
    "title": "Regresión Logística: salud",
    "section": "Bondad de Ajuste e Interpretación Modelo",
    "text": "Bondad de Ajuste e Interpretación Modelo\n\nInterpretación Modelo\nRecordar que el modelo tomaba la forma \\[\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\\], es decir, estamos estimando el log(odds). Esto nos lleva a las siguientes apreciaciones:\nAunque tanto \\(P(Y=1)\\), como \\(Odds(Y=1)\\), como \\(Logit(Y=1)\\) expresan la misma idea, están en distinta escala:\n\nLa probabilidad toma valores comprendidos entre 0 y 1.\nLa odds tiene un valor mínimo de cero y no tiene máximo.\nLa logit o log(odds) no tiene ni mínimo ni máximo.\n\nPor ejemplo, a una probabilidad de 0,5, le corresponde una odds de 1 y un logit de 0. Ahora bien, es cierto que razonar en términos de cambios en los logaritmos resulta poco intuitivo. Por ello, es preferible interpretar el cambio en las odds o en la razón de ventajas (también llamada odds ratio, razón de probabilidades o razón de momios).\nLa interpretación más frecuente es interpretar los signos de los coeficientes del modelo, es decir, los signos de \\(\\beta_1, \\ldots , \\beta_k\\).\n\nSi \\(\\beta_i &gt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en un aumento de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) aumenta, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\nSi \\(\\beta_i &lt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en una disminución de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) disminuye, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\n\nUna pregunta importante en cualquier análisis de regresión es si el modelo propuesto se ajusta adecuadamente a los datos, lo que conduce naturalmente a la noción de una prueba formal para la falta de ajuste (o bondad de ajuste).\n\n\nMedidas Especifidad y Sensibilidad\nLa especificidad y la sensibilidad son medidas utilizadas para evaluar el rendimiento de un modelo predictivo, especialmente en problemas de clasificación binaria (donde solo hay dos clases). Las definimos como:\n\nSensibilidad (Sensitivity): Es la proporción de verdaderos positivos (casos positivos correctamente identificados) respecto al total de casos positivos reales. Es la capacidad del modelo para identificar correctamente los casos positivos.\nEspecificidad (Specificity): Es la proporción de verdaderos negativos (casos negativos correctamente identificados) respecto al total de casos negativos reales. Representa la capacidad del modelo para identificar correctamente los casos negativos.\n\nUn equilibrio entre ambas es deseable, pero depende del contexto específico del problema y de las consecuencias de los falsos positivos y falsos negativos. En el caso, por ejemplo, de detectar si un paciente tiene cáncer o no, parece más razonable centrarse en los Falsos Negativos, ya que un paciente que tiene cáncer no lo estamos detectando, lo que lleva un riesgo implícito muy alto.\n\n\n\n\n\n\n\n\n\n\nClasificado como Positivo\nClasificado como Negativo\nTotal\n\n\n\n\nRealmente Positivo\nVerdadero Positivo (VP)\nFalso Negativo (FN)\nVP + FN\n\n\nRealmente Negativo\nFalso Positivo (FP)\nVerdadero Negativo (VN)\nFP + VN\n\n\nTotal\nVP + FP\nFN + VN\n\n\n\n\nSensibilidad ( )\nEspecificidad: ( )\n\n\nCurva ROC\nLa curva ROC es una representación gráfica de la sensibilidad frente a la tasa de falsos positivos a varios umbrales de clasificación. Se utiliza comúnmente en análisis de clasificación para evaluar el rendimiento de un modelo.\nPara calcular el área bajo la curva ROC (AUC-ROC), se utiliza la tasa de falsos positivos y de falsos negativos. Cuanto más cerca esté el AUC-ROC de 1, mejor será el rendimiento del modelo, ya que indica una mayor capacidad de distinguir entre clases.\nEs una medida de bondad porque evalúa qué tan bien puede discriminar un modelo entre las clases positivas y negativas. Cuanto más se acerque el AUC a 1, mejor será la capacidad del modelo para distinguir entre las clases. Se utiliza para comparar y seleccionar modelos, donde un AUC mayor indica un mejor rendimiento predictivo.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#formulación",
    "href": "notebooks/Logistic Regression/salud/salud.html#formulación",
    "title": "Regresión Logística: salud",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE: Convertir a factor las variables que tengan que ser tratadas como tal, de lo contrario R las tratará como numéricas. Además, la variable respuesta debe tener los niveles codificados como \\(0\\) y \\(1\\) para poder usar la función glm. El resto de variables convertirlas a numéricas en caso de que aplique.\n\ndatos$Edad &lt;- as.numeric(datos$Edad)\ndatos$Altura &lt;- as.numeric(datos$Altura)\ndatos$Peso &lt;- as.numeric(datos$Peso)\n\n# Pasar factores a 0=Hombre y 1=Mujer\ndatos$Sexo &lt;- ifelse(datos$Sexo == \"1\", \"0\", \"1\")\ndatos$Sexo &lt;- as.factor(datos$Sexo)\n\n# Ver resumen de datos y ver si hay NA\nsummary(datos)\n\n      Edad        Sexo          Altura           Peso       \n Min.   : 15.00   0:10318   Min.   :120.0   Min.   : 26.00  \n 1st Qu.: 39.00   1:11701   1st Qu.:160.0   1st Qu.: 62.00  \n Median : 52.00             Median :166.0   Median : 71.00  \n Mean   : 52.84             Mean   :166.7   Mean   : 72.66  \n 3rd Qu.: 67.00             3rd Qu.:173.0   3rd Qu.: 81.00  \n Max.   :103.00             Max.   :204.0   Max.   :180.00  \n\nsum(is.na(datos))\n\n[1] 0\n\ndatos &lt;- na.omit(datos)\n\nComo hemos visto que había na’s en el conjunto de datos, los quitamos para que no haya posibles problemas (eliminamos esas observaciones).\nVemos que si hay algún\nA continuación presentamos tres posibles modelos y posteriormente elegiremos uno de ellos.\n\nlmod1 : Queremos clasificar la Sexo en función de la edad de la persona (numérica).\nlmod2 : Queremos clasificar la Sexo en función de la edad de la persona (numérica) y el Altura (numérica).\nlmod3 : Queremos clasificar la Sexo en función de la edad de la persona (numérica), el Altura (numérica) y el Peso (numérica).\n\n\n# lmod1\nlmod1 &lt;- glm(formula = Sexo ~ Edad, family = binomial(link = logit), data = datos)\nsummary(lmod1)\n\n\nCall:\nglm(formula = Sexo ~ Edad, family = binomial(link = logit), data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.1957973  0.0407321  -4.807 1.53e-06 ***\nEdad         0.0060933  0.0007289   8.359  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30438  on 22018  degrees of freedom\nResidual deviance: 30368  on 22017  degrees of freedom\nAIC: 30372\n\nNumber of Fisher Scoring iterations: 3\n\n# lmod2\nlmod2 &lt;- glm(formula = Sexo ~ Edad + Altura, family = binomial(link = logit), data = datos)\nsummary(lmod2)\n\n\nCall:\nglm(formula = Sexo ~ Edad + Altura, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 49.432554   0.659059   75.00   &lt;2e-16 ***\nEdad        -0.032589   0.001136  -28.68   &lt;2e-16 ***\nAltura      -0.285114   0.003782  -75.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30438  on 22018  degrees of freedom\nResidual deviance: 17225  on 22016  degrees of freedom\nAIC: 17231\n\nNumber of Fisher Scoring iterations: 5\n\n# lmod3\nlmod3 &lt;- glm(formula = Sexo ~ Edad + Altura + Peso, family = binomial(link = logit), data = datos)\nsummary(lmod3)\n\n\nCall:\nglm(formula = Sexo ~ Edad + Altura + Peso, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 47.518534   0.667448   71.19   &lt;2e-16 ***\nEdad        -0.027013   0.001172  -23.04   &lt;2e-16 ***\nAltura      -0.258429   0.003886  -66.50   &lt;2e-16 ***\nPeso        -0.039144   0.001624  -24.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30438  on 22018  degrees of freedom\nResidual deviance: 16605  on 22015  degrees of freedom\nAIC: 16613\n\nNumber of Fisher Scoring iterations: 6\n\n\nEn este caso, el Modelo 3 tiene el AIC más bajo, lo que sugiere que podría ser el mejor ajuste entre los tres modelos. Sin embargo, es importante considerar otros aspectos y realizar pruebas adicionales si es necesario para validar el modelo seleccionado. Por otro lado, en términos de la Deviance podemos ver cosas parecidas.\nPara este modelo vamos a calcular la matriz de confusión y el área ROC. Hemos calculado la matriz de confusión utilizando un threshold de 0.5. Es decir, si hay mas de un 0.51 de probabilidad de que una observación pertenezca a la clase 1 (Mujer), entonces lo clasificamos como tal.\nLuego veremos el valor óptimo para este threshold.\n\n# confusion matrices\npredicted2 &lt;- predict(lmod3, datos[, c(\"Edad\", \"Altura\", \"Peso\")], type = \"response\")\nconfusionMatrix(data = as.factor(ifelse(predicted2 &gt; 0.5, 1, 0)), reference = datos$Sexo, positive = \"1\")\n\nRealicemos ahora la curva ROC con la función ROC del paquete Epi.\n\nROC(form = Sexo ~ Edad + Altura + Peso, data = datos, plot = \"ROC\", lwd = 3, cex = 1.5)\n\n\n\n\n\n\n\n\nObservamos una Especifidad del 82% y una Sensibilidad del 84%. Esto quiere decir que nuestro modelo es mejor evitando falsos negativos, que falsos positivos.\nDestacar que el elemento Ir.eta que aparece arriba, es el punto de corte óptimo (threshold óptimo) de la probabilidad. Es decir, si nuestra regresión logística predice que hay una probabilidad mayor de \\(0.516\\) de que una observación sea mujer, entonces la clasificaremos como tal.\nPodemos usar el presente modelo para predecir la probabilidad de ser mujer en función de las variables predictoras de nuevas observaciones.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/salud.html#interpretación-coeficientes",
    "href": "notebooks/Logistic Regression/salud/salud.html#interpretación-coeficientes",
    "title": "Regresión Logística: salud",
    "section": "Interpretación coeficientes",
    "text": "Interpretación coeficientes\nVamos a volver a sacar el summary del modelo para proceder a explicar todo bien de nuevo.\n\nsummary(lmod3)\n\n\nCall:\nglm(formula = Sexo ~ Edad + Altura + Peso, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 47.518534   0.667448   71.19   &lt;2e-16 ***\nEdad        -0.027013   0.001172  -23.04   &lt;2e-16 ***\nAltura      -0.258429   0.003886  -66.50   &lt;2e-16 ***\nPeso        -0.039144   0.001624  -24.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30438  on 22018  degrees of freedom\nResidual deviance: 16605  on 22015  degrees of freedom\nAIC: 16613\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nEdad: Por cada incremento unitario en la edad, el logaritmo de odds de éxito en Sexo disminuye aproximadamente en 0.027, manteniendo constante el resto de variables.\nAltura: Por cada incremento unitario en la altura, el logaritmo de odds de éxito en Sexo disminuye aproximadamente en 0.25, manteniendo constante el resto de variables.\nPeso: Por cada incremento unitario en el peso, el logaritmo de odds de éxito en Sexo disminuye aproximadamente en 0.039, manteniendo constante el resto de variables.\n\nInterpretación coeficiente Edad\nVemos que este coeficiente es relativamente pequeño, con lo cual no parece tener impacto en el \\(Sexo\\) de la persona. Esto tiene sentido ya que el número de niños/niñas nacidos cada año tiende al 50%-50% con lo cual no debería haber más elementos de una subpoblación conforme aumente/disminuya la edad.\nEs verdad, que conforme aumenta la edad, la mortalidad es más grave en hombres, pero en ningún caso para producir tantas defunciones como para posibilitar la discriminación del Sexo en función de la edad.\nInterpretación coeficiente Peso\nEl coeficiente para Peso, tiene un valor negativo. Esto implica que la probabilidad de clasificar a una observación como mujer disminuye conforme aumenta la variable Peso (manteniendo el resto de variables constantes). Esto parece razonable ya que dentro de un mismo grupo de edad, incluso de altura, los hombres tienden generalmente a ser más corpulentos que las mujeres y por tanto a pesar más. De todas maneras sería la combinación Peso/Altura la que nos ayudaría a discriminar bien.\nInterpretación coeficiente Altura\nEl coeficiente para Altura, tiene un valor negativo. Esto implica que la probabilidad de clasificar a una observación como mujer disminuye conforme aumenta la variable altura (manteniendo el resto de variables constantes). Esto parece razonable ya que dentro de un mismo grupo de edad, incluso de peso, los hombres tienden generalmente a ser más altos que las mujeres. De todas maneras sería la combinación Peso/Altura la que nos ayudaría a discriminar bien.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Regresión Logística: salud"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/Dataset_cleaning.html",
    "href": "notebooks/Logistic Regression/ECV/Dataset_cleaning.html",
    "title": "Procesamos el dataset para adaptarlo a lo de arriba",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente analizar la existencia del llamado “ascensor social” en España a partir de los microdatos de la Encuesta de Condiciones de Vida (ECV) del año 2019. Dicho análisis nos sirve como excusa para tratar de mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\nEn primer lugar, debemos definir qué es eso del “ascensor social” y cómo vamos a tratar de analizarlo nosostros. Éste se define como la posibilidad de ascender o descender de clase social. Aunque podemos considerar que la pertenencia a una determinada clase social - si es que éstas existen de forma estanca y perfectamente distinguible - se explica por una combinación de aspectos: nivel económico, cultural, de estudios, etc, nosotros nos vamos a centrar simplemente en el nivel económico.\nPara ello, usaremos datos transversales de la ECV del año 2019. Este es el último año - hasta la fecha (2021) - que la ECV ha contado con el modulo temático Transmisión intergeneracional de la pobreza - batería de preguntas que nos permiten realizar el análisis deseado.\nLos microdatos de la ECV 2019 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176807&menu=resultados&idp=1254735976608#!tabs-1254736195153. Dichos microdatos están formados por cuatro archivos:\nPara poder unir los ficheros contamos con una variable de identificación por fichero. Las variables de identificación de los ficheros de hogares (DB030 y HB030) son idénticas; lo mismo para las variables de identificación de los ficheros de personas (RB030 y PB030). Éstas últimas se componen del identificador del hogar y el nº de orden, a dos dígitos, de la persona dentro del hogar.\nAsimismo, al descargarse los microdatos, en la misma carpeta comprimida, encontramos un documento Word que contiene la explicación de todas las variables de la encuesta.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "¿Cómo determinamos si una persona ha mejorado o no su situación económica?"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/Dataset_cleaning.html#cómo-determinamos-si-una-persona-ha-mejorado-o-no-su-situación-económica",
    "href": "notebooks/Logistic Regression/ECV/Dataset_cleaning.html#cómo-determinamos-si-una-persona-ha-mejorado-o-no-su-situación-económica",
    "title": "Procesamos el dataset para adaptarlo a lo de arriba",
    "section": "¿Cómo determinamos si una persona ha mejorado o no su situación económica?",
    "text": "¿Cómo determinamos si una persona ha mejorado o no su situación económica?\nPara responder a esta pregunta vamos a utilizar las respuestas de los encuestados a dos cuestiones concretas:\n\nSituación económica del hogar cuando el adulto era adolescente. Las opciones de respuesta son:\n\n\nMuy mala\nMala\nModeradamente mala\nModeradamente buena\nBuena\nMuy buena\n\nEsta pregunta pertenece al módulo Trasmisión intergeneracional de la pobreza y, por tanto, se encuentra en el fichero P. Es la pregunta PT190.\n\nCapacidad del hogar para llegar a fin de mes. Cuyas opciones de respuesta son:\n\n\nCon mucha dificultad.\nCon dificultad.\nCon cierta dificultad.\nCon cierta facilidad.\nCon facilidad.\nCon mucha facilidad.\nEsta pregunta se refiere al hogar y se encuentra, específicamente, en el fichero H. Es la HS120.\n\nComo vemos, las preguntas no son idénticas, ni siquiera sus opciones de respuesta lo son. Pero sí resultan muy similares y podemos utizarlas como proxy para determinar si una persona ha mejorado o no su situación económica.\nVamos a superponer, pues, lo siguiente: una persona ha mejorado su situación económica, es decir, ha logrado subir en el ascensor social, si ha mejorado en el número de su respuesta de la pregunta 1 a la 2. Por ejemplo: una persona que responde Muy mala (opción 1) en la pregunta 1, y Con cierta dificultad (opción 3) en la segunda pregunta, aunque su situación econónomica siga sin calificarla de “buena”, sí que ha mejorado. Por el contrario si el número de la respuesta es el mismo o menor en la pregunta 2 que en la 1, dicha persona no habrá mejorado.\nDe esta manera, clasificaremos a los encuestados en dos categorías: 1) Personas que han mejorado - han subido con el ascensor social; 2) Personas que no han mejorado - no han subido con el ascensor social. Dicha sencilla categorización puede resultar simplista, pero nos permitirá llevar a cabo una regresión logística binaria, la más sencilla y, por ello, la más fácil de entender y realizar.\nEsta variable binaria será la variable dependiente de nuestra regresión logística.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "¿Cómo determinamos si una persona ha mejorado o no su situación económica?"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html",
    "href": "notebooks/Logistic Regression/laboral/laboral.html",
    "title": "Regresión Logística: laboral",
    "section": "",
    "text": "En este cuaderno se va a explicar los fundamentos de la Regresión Logística, como detectar cuando nos encontramos ante un problema que se debe abordar mediante este tipo de Regresión. Se verán los fundamentos teóricos que lo sustentan y las técnicas a llevar a cabo para analizar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado laboral.xlsx. Este contiene microdatos relativos a la Encuestas de estructura salarial. Resultados.. Concretamente, datos correspondientes al año 2018. Las variables de interés son las siguientes:\n\nEstudios: Nivel de estudios del encuestado. Valor 1 corresponde a individuos con muy bajo nivel académico (hasta primaria), y 0 a individuos con al menos nivel académico universitario.\nSalario: Sueldo bruto anual.\nEdad: Grupo de Edad del encuestado. Puesto que las clases tienen un orden intrínseco, la variable la vamos a tratar como cuantitativa.\n\n01 MENOS 19 AÑOS\n02 DE 20 A 29\n\n03 DE 30 A 39\n\n04 DE 40 A 49\n\n05 DE 50 A 59\n\n06 MÁS DE 59\n\nAntiguedad: Años de antigüedad.\nVacaciones: Días de vacaciones al año.\n\nEl objetivo es mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\n\n\n\nSe pretende hacer una regresión logística que clasifique la variable respuesta Estudios en función de varios predictores, todos ellos continuos: Edad, Vacaciones, Antiguedad y Salario.\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#dataset",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#dataset",
    "title": "Regresión Logística: laboral",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado laboral.xlsx. Este contiene microdatos relativos a la Encuestas de estructura salarial. Resultados.. Concretamente, datos correspondientes al año 2018. Las variables de interés son las siguientes:\n\nEstudios: Nivel de estudios del encuestado. Valor 1 corresponde a individuos con muy bajo nivel académico (hasta primaria), y 0 a individuos con al menos nivel académico universitario.\nSalario: Sueldo bruto anual.\nEdad: Grupo de Edad del encuestado. Puesto que las clases tienen un orden intrínseco, la variable la vamos a tratar como cuantitativa.\n\n01 MENOS 19 AÑOS\n02 DE 20 A 29\n\n03 DE 30 A 39\n\n04 DE 40 A 49\n\n05 DE 50 A 59\n\n06 MÁS DE 59\n\nAntiguedad: Años de antigüedad.\nVacaciones: Días de vacaciones al año.\n\nEl objetivo es mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Logística: laboral",
    "section": "",
    "text": "Se pretende hacer una regresión logística que clasifique la variable respuesta Estudios en función de varios predictores, todos ellos continuos: Edad, Vacaciones, Antiguedad y Salario.\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#cargar-librerías",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#cargar-librerías",
    "title": "Regresión Logística: laboral",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer los excels\nlibrary(caret) # para la confusion matrix\nlibrary(Epi) # para la ROC curve",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#lectura-de-datos",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#lectura-de-datos",
    "title": "Regresión Logística: laboral",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/laboral.xlsx\", sheet = \"Datos\")\n\n\n# Creamos un gráfico de quesito o de sectores para ver qué proporción de personas ha mejorado su situación económica:\n\n\ndatos_quesito &lt;- c(nrow(filter(datos, Estudios == \"0\")), nrow(filter(datos, Estudios == \"1\")))\netiquetas &lt;- c(\"Universitarios\", \"Hasta primaria\")\n\nporcentajes &lt;- paste0(round(datos_quesito / sum(datos_quesito) * 100, 1), \"%\")\ncolores &lt;- c(\"lightgreen\", \"lightcoral\")\n\npie(datos_quesito, labels = porcentajes, col = colores, radius = 1.05, cex = 1)\nlegend(\"topleft\", etiquetas, cex = 0.7, fill = colores)\ntitle(\"Nivel de estudios\")\n\n\n\n\n\n\n\n\nVemos que tenemos un porcentaje más grande de gente con estudios universitarios en la muestra que gente con estudios hasta primaria. De datos con gente con estudios entre primaria y universitarios no disponemos en esta muestra.\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Edad)) +\n  geom_density(\n    aes(\n      group = Estudios,\n      colour = Estudios,\n      fill = Estudios\n    ),\n    alpha = 0.2\n  )\n\n\n\n\n\n\n\n\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Vacaciones)) +\n  geom_density(\n    aes(\n      group = Estudios,\n      colour = Estudios,\n      fill = Estudios\n    ),\n    alpha = 0.2\n  )\n\n\n\n\n\n\n\n\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Antiguedad)) +\n  geom_density(\n    aes(\n      group = Estudios,\n      colour = Estudios,\n      fill = Estudios\n    ),\n    alpha = 0.2\n  )\n\n\n\n\n\n\n\n\n\ndatos &lt;- na.omit(datos)\ndatos %&gt;% ggplot(aes(x = Salario)) +\n  geom_density(\n    aes(\n      group = Estudios,\n      colour = Estudios,\n      fill = Estudios\n    ),\n    alpha = 0.2\n  )",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#introducción-1",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#introducción-1",
    "title": "Regresión Logística: laboral",
    "section": "Introducción",
    "text": "Introducción\nUn análisis de regresión logística es una técnica estadística multivariante que tiene como finalidad pronosticar o explicar los valores de una variable dependiente categórica a partir de una (regresión logística simple) o más (regresión logística múltiple) variables independientes categóricas o continuas. Dichas variables independientes reciben el nombre de covariables. Asimismo, a diferencia de lo que suele hacerse cuando tenemos una variable dependiente continua, cuando ésta es categórica, no interesa describir o pronosticar los valores concretos de dicha variable, sino la probabilidad de pertenecer a cada una de las categorías de la misma.\nAunque matemáticamente se pueda ajustar un modelo de regresión lineal clásico a la relación entre una variable dependiente categórica y una o varias covariables, cuando la variable dependiente es dicotómica (regresión logística binaria, caso más sencillo de regresión logística) no es apropiado utilizar un modelo de regresión lineal porque una variable dicotómica no se ajusta a una distribución normal, sino a una binomial. Ignorar esta cuestión podría llevar a obtener probabilidades imposibles: menores que cero o mayores que uno.\nPara evitar este problema, es preferible utilizar funciones que realicen predicciones comprendidas entre un máximo y un mínimo. Una de estas funciones - posiblemente la más empleada - es la curva logística o función sigmoide:\n[ =()= _0 + _1 X_1 + _2 X_2 + , p=P(Y=1) ]\nEs decir, estamos estimando con una regresión lineal el valor de \\(\\eta\\), que sí es una v.a. continua - a diferencia de Y que es binaria-.\nEsto es, \\(p=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\). De esta forma, para valores positivos muy grandes de \\(\\eta\\) llamado odds, \\(e^{-\\eta}\\) es aproximadamente cero, por lo que el valor de la función es 1; mientras que para valores negativos muy grandes de \\(\\eta\\), \\(e^{-\\eta}\\) tiende a infinito, haciendo que el valor de la función sea 0.\nA continuación, para simplificar un poco las cosas, consideremos el modelo de regresión logística más sencillo: regresión logística binaria simple (una sola covariable):\n[ P(Y=1)= ]\nLa interpretación de esta función es muy similar a la de una regresión lineal: el coeficiente \\(\\beta_0\\) representa la posición de la curva sobre el eje horizontal o de abscisas (más hacia la izquierda o más hacia la derecha); mientras que \\(\\beta_1\\) representa la pendiente de la curva, es decir, cuán inclinada está en su parte central (cuanto más inclinada, mayor capacidad de discriminar entre los dos valores de la variable dependiente).\nEjemplo sencillo Vamos a mostrar como una variable binaria no tiene sentido predecirla con una Regresión Lineal sino Logística.\n\n# Generación de datos para el ejemplo\nset.seed(123)\nn &lt;- 200\nAltura &lt;- rnorm(n, mean = 165, sd = 10)\n\n# Crear una variable binaria 'Sexo' en función de Altura\nSexo &lt;- as.factor(ifelse(Altura + rnorm(n) &gt; 165, 1, 0))\ndatos_ejemplo &lt;- data.frame(Altura, Sexo)\n\n# Regresión lineal\nmodelo_lineal &lt;- lm(Sexo ~ Altura, data = datos_ejemplo)\n\n# Regresión logística\nmodelo_logistico &lt;- glm(Sexo ~ Altura, data = datos_ejemplo, family = binomial)\n\n\n# Regresión Lineal\npar(mfrow = c(1, 2))\nplot(datos_ejemplo$Altura, datos_ejemplo$Sexo, col = \"lightblue\", main = \"Ajuste por Regresión Lineal\")\nabline(modelo_lineal, col = \"navy\")\n\n# Regresión Logística\n\nplot(datos_ejemplo$Altura, as.numeric(datos_ejemplo$Sexo) - 1, col = \"lightblue\", main = \"Regresión Logística\", xlab = \"Altura\", ylab = \"Sexo\")\ncurve(predict(modelo_logistico, data.frame(Altura = x), type = \"response\"), add = TRUE, col = \"navy\", lwd = 2)\n\n\n\n\n\n\n\n\nEn este ejemplo, se muestra cómo un ajuste por regresión lineal no se adapta bien a datos binarios, produciendo predicciones que pueden ser mayores que 1 o menores que 0. En cambio, la regresión logística produce una curva en forma de S que se adapta mejor a los datos, con predicciones que están siempre entre 0 y 1. Esto demuestra que para problemas de clasificación binaria, la regresión logística es una mejor opción que la regresión lineal.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#bondad-de-ajuste-e-interpretación-modelo",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#bondad-de-ajuste-e-interpretación-modelo",
    "title": "Regresión Logística: laboral",
    "section": "Bondad de Ajuste e Interpretación Modelo",
    "text": "Bondad de Ajuste e Interpretación Modelo\n\nInterpretación Modelo\nRecordar que el modelo tomaba la forma \\[\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\\], es decir, estamos estimando el log(odds). Esto nos lleva a las siguientes apreciaciones:\nAunque tanto \\(P(Y=1)\\), como \\(Odds(Y=1)\\), como \\(\\operatorname{logit}(Y=1)\\) expresan la misma idea, están en distinta escala:\n\nLa probabilidad toma valores comprendidos entre 0 y 1.\nLa odds tiene un valor mínimo de cero y no tiene máximo.\nLa logit o log(odds) no tiene ni mínimo ni máximo.\n\nPor ejemplo, a una probabilidad de 0,5, le corresponde una odds de 1 y un logit de 0. Ahora bien, es cierto que razonar en términos de cambios en los logaritmos resulta poco intuitivo. Por ello, es preferible interpretar el cambio en las odds o en la razón de ventajas (también llamada odds ratio, razón de probabilidades o razón de momios).\nLa interpretación más frecuente es interpretar los signos de los coeficientes del modelo, es decir, los signos de \\(\\beta_1, \\ldots , \\beta_k\\).\n\nSi \\(\\beta_i &gt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en un aumento de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) aumenta, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\nSi \\(\\beta_i &lt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en una disminución de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) disminuye, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\n\nUna pregunta importante en cualquier análisis de regresión es si el modelo propuesto se ajusta adecuadamente a los datos, lo que conduce naturalmente a la noción de una prueba formal para la falta de ajuste (o bondad de ajuste).\n\n\nMedidas Especifidad y Sensibilidad\nLa especificidad y la sensibilidad son medidas utilizadas para evaluar el rendimiento de un modelo predictivo, especialmente en problemas de clasificación binaria (donde solo hay dos clases). Las definimos como:\n\nSensibilidad (Sensitivity): Es la proporción de verdaderos positivos (casos positivos correctamente identificados) respecto al total de casos positivos reales. Es la capacidad del modelo para identificar correctamente los casos positivos.\nEspecificidad (Specificity): Es la proporción de verdaderos negativos (casos negativos correctamente identificados) respecto al total de casos negativos reales. Representa la capacidad del modelo para identificar correctamente los casos negativos.\n\nUn equilibrio entre ambas es deseable, pero depende del contexto específico del problema y de las consecuencias de los falsos positivos y falsos negativos. En el caso, por ejemplo, de detectar si un paciente tiene cáncer o no, parece más razonable centrarse en los Falsos Negativos, ya que un paciente que tiene cáncer no lo estamos detectando, lo que lleva un riesgo implícito muy alto.\n\n\n\n\n\n\n\n\n\n\nClasificado como Positivo\nClasificado como Negativo\nTotal\n\n\n\n\nRealmente Positivo\nVerdadero Positivo (VP)\nFalso Negativo (FN)\nVP + FN\n\n\nRealmente Negativo\nFalso Positivo (FP)\nVerdadero Negativo (VN)\nFP + VN\n\n\nTotal\nVP + FP\nFN + VN\n\n\n\n\nSensibilidad ( )\nEspecificidad: ( )\n\n\nCurva ROC\nLa curva ROC es una representación gráfica de la sensibilidad frente a la tasa de falsos positivos a varios umbrales de clasificación. Se utiliza comúnmente en análisis de clasificación para evaluar el rendimiento de un modelo.\nPara calcular el área bajo la curva ROC (AUC-ROC), se utiliza la tasa de falsos positivos y de falsos negativos Cuanto más cerca esté el AUC-ROC de 1, mejor será el rendimiento del modelo, ya que indica una mayor capacidad de distinguir entre clases.\nEs una medida de bondad porque evalúa qué tan bien puede discriminar un modelo entre las clases positivas y negativas. Cuanto más se acerque el AUC a 1, mejor será la capacidad del modelo para distinguir entre las clases. Se utiliza para comparar y seleccionar modelos, donde un AUC mayor indica un mejor rendimiento predictivo.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#formulación",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#formulación",
    "title": "Regresión Logística: laboral",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE: Convertir a factor las variables que tengan que ser tratadas como tal, de lo contrario R las tratará como numéricas. Además, la variable respuesta debe tener los niveles codificados como \\(0\\) y \\(1\\) para poder usar la función glm. El resto de variables convertirlas a numéricas en caso de que aplique.\n\ndatos$Edad &lt;- as.numeric(datos$Edad)\ndatos$Vacaciones &lt;- as.numeric(datos$Vacaciones)\ndatos$Antiguedad &lt;- as.numeric(datos$Antiguedad)\ndatos$Salario &lt;- as.numeric(datos$Salario)\n\n\n# Pasar factores a 0=Universitarios y 1=Hasta Primaria\ndatos$Estudios &lt;- as.factor(datos$Estudios)\n\n# Ver resumen de datos y ver si hay NA\nsummary(datos)\n\n Estudios       Edad         Vacaciones      Antiguedad      Salario      \n 0:63527   Min.   :1.000   Min.   : 0.00   Min.   : 0.0   Min.   :    63  \n 1:36255   1st Qu.:3.000   1st Qu.: 0.00   1st Qu.: 2.0   1st Qu.: 15200  \n           Median :4.000   Median :22.00   Median : 9.0   Median : 24959  \n           Mean   :3.955   Mean   :12.99   Mean   :10.5   Mean   : 30106  \n           3rd Qu.:5.000   3rd Qu.:23.00   3rd Qu.:16.0   3rd Qu.: 39228  \n           Max.   :6.000   Max.   :99.00   Max.   :52.0   Max.   :100000  \n\nsum(is.na(datos))\n\n[1] 0\n\n\nA continuación presentamos tres posibles modelos y posteriormente elegiremos uno de ellos.\n\nlmod1 : Queremos clasificar los Estudios en función de la edad de la persona (numérica).\nlmod2 : Queremos clasificar los Estudios en función de la edad de la persona (numérica) y las vacaciones (numérica).\nlmod3 : Queremos clasificar los Estudios en función de la edad de la persona (numérica), las vacaciones (numérica) y la Antigüedad (numérica).\nlmod4 : Queremos clasificar los Estudios en función de la edad de la persona (numérica), las vacaciones (numérica), la Antigüedad (numérica) y el Salario (numérica).\n\n\n# lmod1\nlmod1 &lt;- glm(formula = Estudios ~ Edad, family = binomial(link = logit), data = datos)\nsummary(lmod1)\n\n\nCall:\nglm(formula = Estudios ~ Edad, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.806239   0.026400  -68.42   &lt;2e-16 ***\nEdad         0.311042   0.006306   49.33   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 130778  on 99781  degrees of freedom\nResidual deviance: 128266  on 99780  degrees of freedom\nAIC: 128270\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod2\nlmod2 &lt;- glm(formula = Estudios ~ Edad + Vacaciones, family = binomial(link = logit), data = datos)\nsummary(lmod2)\n\n\nCall:\nglm(formula = Estudios ~ Edad + Vacaciones, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.3429591  0.0275630  -48.72   &lt;2e-16 ***\nEdad         0.3542012  0.0066062   53.62   &lt;2e-16 ***\nVacaciones  -0.0530736  0.0005864  -90.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 130778  on 99781  degrees of freedom\nResidual deviance: 119481  on 99779  degrees of freedom\nAIC: 119487\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod3\nlmod3 &lt;- glm(formula = Estudios ~ Edad + Vacaciones + Antiguedad, family = binomial(link = logit), data = datos)\nsummary(lmod3)\n\n\nCall:\nglm(formula = Estudios ~ Edad + Vacaciones + Antiguedad, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.7050721  0.0294638  -57.87   &lt;2e-16 ***\nEdad         0.5204287  0.0080291   64.82   &lt;2e-16 ***\nVacaciones  -0.0501357  0.0005933  -84.51   &lt;2e-16 ***\nAntiguedad  -0.0321310  0.0008546  -37.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 130778  on 99781  degrees of freedom\nResidual deviance: 118034  on 99778  degrees of freedom\nAIC: 118042\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod4\nlmod4 &lt;- glm(formula = Estudios ~ Edad + Vacaciones + Antiguedad + Salario, family = binomial(link = logit), data = datos)\nsummary(lmod3)\n\n\nCall:\nglm(formula = Estudios ~ Edad + Vacaciones + Antiguedad, family = binomial(link = logit), \n    data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.7050721  0.0294638  -57.87   &lt;2e-16 ***\nEdad         0.5204287  0.0080291   64.82   &lt;2e-16 ***\nVacaciones  -0.0501357  0.0005933  -84.51   &lt;2e-16 ***\nAntiguedad  -0.0321310  0.0008546  -37.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 130778  on 99781  degrees of freedom\nResidual deviance: 118034  on 99778  degrees of freedom\nAIC: 118042\n\nNumber of Fisher Scoring iterations: 4\n\n\nEn este caso, el Modelo 4 tiene el AIC más bajo, lo que sugiere que podría ser el mejor ajuste entre los tres modelos. Además es el que mayor bajada de AIC ha experimentado conforme al modelo anterior, incluyendo este la variable Salario. Es por ello que esta variable parece de vital importancia a la hora de clasificar el nivel de estudios. Sin embargo, es importante considerar otros aspectos y realizar pruebas adicionales si es necesario para validar el modelo seleccionado. Por otro lado, en términos de la Deviance podemos ver cosas parecidas.\nPara este modelo vamos a calcular la matriz de confusión y el área ROC. Hemos calculado la matriz de confusión utilizando un threshold de 0.51. Es decir, si hay mas de un 0.51 de probabilidad de que una observación pertenezca a la clase 1 (estudios hasta primaria), entonces lo clasificamos como tal.\nLuego veremos el valor óptimo para este threshold.\n\n# confusion matrices\npredicted2 &lt;- predict(lmod4, datos[, c(\"Edad\", \"Vacaciones\", \"Antiguedad\", \"Salario\")], type = \"response\")\nlibrary(caret)\nconfusionMatrix(data = as.factor(ifelse(predicted2 &gt; 0.5, 1, 0)), reference = datos$Estudios, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 53723 13431\n         1  9804 22824\n                                          \n               Accuracy : 0.7671          \n                 95% CI : (0.7645, 0.7698)\n    No Information Rate : 0.6367          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.4856          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.6295          \n            Specificity : 0.8457          \n         Pos Pred Value : 0.6995          \n         Neg Pred Value : 0.8000          \n             Prevalence : 0.3633          \n         Detection Rate : 0.2287          \n   Detection Prevalence : 0.3270          \n      Balanced Accuracy : 0.7376          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nRealicemos ahora la curva ROC con la función ROC del paquete Epi. Esa función nos va a devolver la ROC curve con la información pertinente para la regresión logística, encontrando el threshold óptimo para el que se obtienen mejores resultados en las métricas.\n\nROC(form = Estudios ~ Edad + Vacaciones + Antiguedad + Salario, data = datos, plot = \"ROC\", lwd = 3, cex = 1.5)\n\n\n\n\n\n\n\n\nObservamos una Especifidad del 72% y una Sensibilidad del 80%. Esto quiere decir que nuestro modelo es mejor evitando falsos negativos, que falsos positivos Es decir, que es mejor evitando clasificar a alguien como que tiene estudios cuando verdaderamente no los tiene, que al revés.\nDestacar que el elemento Ir.eta que aparece arriba, es el punto de corte óptimo (threshold óptimo) de la probabilidad. Es decir, si nuestra regresión logística predice que hay una probabilidad mayor de \\(0.348\\) de que una observación sea mujer, entonces la clasificaremos como tal.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#otras-consideraciones",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#otras-consideraciones",
    "title": "Regresión Logística: laboral",
    "section": "Otras consideraciones",
    "text": "Otras consideraciones\nPodemos usar el presente modelo para predecir la probabilidad de no tener estudios (máximo nivel primaria) en función de las variables predictoras de nuevas observaciones.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/laboral.html#interpretación-coeficientes",
    "href": "notebooks/Logistic Regression/laboral/laboral.html#interpretación-coeficientes",
    "title": "Regresión Logística: laboral",
    "section": "Interpretación coeficientes",
    "text": "Interpretación coeficientes\nVamos a volver a sacar el summary del modelo para proceder a explicar todo bien de nuevo.\n\nsummary(lmod4)\n\n\nCall:\nglm(formula = Estudios ~ Edad + Vacaciones + Antiguedad + Salario, \n    family = binomial(link = logit), data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.485e-01  3.340e-02  -13.43   &lt;2e-16 ***\nEdad         5.803e-01  9.029e-03   64.27   &lt;2e-16 ***\nVacaciones  -3.232e-02  6.752e-04  -47.86   &lt;2e-16 ***\nAntiguedad   2.324e-02  1.088e-03   21.36   &lt;2e-16 ***\nSalario     -8.936e-05  7.716e-07 -115.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 130778  on 99781  degrees of freedom\nResidual deviance:  94200  on 99777  degrees of freedom\nAIC: 94210\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nEdad: Por cada incremento unitario en la edad, el logaritmo de odds de éxito en Nivel de Estudios= Hasta primaria, aumenta aproximadamente en 0.58, manteniendo constante el resto de variables. Esto implica que a mayor edad es menor la incidencia de gente con estudios, lo cual se puede corroborar ya que conforme nos alejamos en el tiempo menos gente estudiaba.\nVacaciones: Por cada incremento unitario en la altura, el logaritmo de odds de éxito en Estudios disminuye aproximadamente en 0.023, manteniendo constante el resto de variables.\n\nAntiguedad: Por cada incremento unitario en el peso, el logaritmo de odds de éxito en Estudios aumenta aproximadamente en 0.023, manteniendo constante el resto de variables.\nsalario: Por cada incremento unitario en el peso, el logaritmo de odds de éxito en Estudios disminuye aproximadamente en 0.000089, manteniendo constante el resto de variables.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Regresión Logística: laboral"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html",
    "href": "notebooks/Logistic Regression/Partos/Partos.html",
    "title": "Regresión Logística: Partos",
    "section": "",
    "text": "En este cuaderno se va a explicar los fundamentos de la Regresión Logística, como detectar cuando nos encontramos ante un problema que se debe abordar mediante este tipo de Regresión. Se verán los fundamentos teóricos que lo sustentan y las técnicas a llevar a cabo para analizar los datos.\n\n\nEn este cuaderno vamos a analizar la presencia de una cesárea en un parto a partir de los microdatos de la Estadística de nacimientos. Movimiento natural de la población. Concretamente, se han tomado los datos relativos a partos, 2022 y los relativos a la Comunidad Autónoma de Navarra.. Dicho análisis nos sirve como excusa para tratar de mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\nVer fichero Dataset_cleaning para ver cómo se han tratado dichos microdatos hasta obtener un conjunto de datos adecuado para la regresión logística, y que tomaremos aquí como punto de partida. El fichero de datos procesado se puede encontrar en Partos.xlsx\nConcretamente tenemos las siguientes variables:\n\nbebes: Número de bebés nacidos en el parto. (categórica)\nsemanas: Número de semanas del embarazo.\nedad_madre: Edad de la madre en años cumplidos.\ncesárea: Si se ha llevado a cabo una cesárea en el parto (Categórica. 0=no, 1=si).\nparto_normal: Si el parto transcurrió con normalidad. (Categórica. 1=normal, 2=complicaciones)\n\n\n\n\n(Esto irá en la web de explica) Se pretende hacer una regresión logística que clasifique la variable respuesta cesárea - si en el parto se efectuó una cesárea - en función de varios predictores, tanto continuos (semanas, edad_madre) como categóricos (bebes).\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#dataset",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#dataset",
    "title": "Regresión Logística: Partos",
    "section": "",
    "text": "En este cuaderno vamos a analizar la presencia de una cesárea en un parto a partir de los microdatos de la Estadística de nacimientos. Movimiento natural de la población. Concretamente, se han tomado los datos relativos a partos, 2022 y los relativos a la Comunidad Autónoma de Navarra.. Dicho análisis nos sirve como excusa para tratar de mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\nVer fichero Dataset_cleaning para ver cómo se han tratado dichos microdatos hasta obtener un conjunto de datos adecuado para la regresión logística, y que tomaremos aquí como punto de partida. El fichero de datos procesado se puede encontrar en Partos.xlsx\nConcretamente tenemos las siguientes variables:\n\nbebes: Número de bebés nacidos en el parto. (categórica)\nsemanas: Número de semanas del embarazo.\nedad_madre: Edad de la madre en años cumplidos.\ncesárea: Si se ha llevado a cabo una cesárea en el parto (Categórica. 0=no, 1=si).\nparto_normal: Si el parto transcurrió con normalidad. (Categórica. 1=normal, 2=complicaciones)",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Logística: Partos",
    "section": "",
    "text": "(Esto irá en la web de explica) Se pretende hacer una regresión logística que clasifique la variable respuesta cesárea - si en el parto se efectuó una cesárea - en función de varios predictores, tanto continuos (semanas, edad_madre) como categóricos (bebes).\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#cargar-librerías",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#cargar-librerías",
    "title": "Regresión Logística: Partos",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer los excels\nlibrary(caret) # para la confusion matrix\nlibrary(Epi) # para la ROC curve",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#lectura-de-datos",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#lectura-de-datos",
    "title": "Regresión Logística: Partos",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos1 &lt;- read_excel(\"../../../files/partos.xlsx\", sheet = \"Datos\")",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#introducción-1",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#introducción-1",
    "title": "Regresión Logística: Partos",
    "section": "Introducción",
    "text": "Introducción\nUn análisis de regresión logística es una técnica estadística multivariante que tiene como finalidad pronosticar o explicar los valores de una variable dependiente categórica a partir de una (regresión logística simple) o más (regresión logística múltiple) variables independientes categóricas o continuas. Dichas variables independientes reciben el nombre de covariables. Asimismo, a diferencia de lo que suele hacerse cuando tenemos una variable dependiente continua, cuando ésta es categórica, no interesa describir o pronosticar los valores concretos de dicha variable, sino la probabilidad de pertenecer a cada una de las categorías de la misma.\nAunque matemáticamente se pueda ajustar un modelo de regresión lineal clásico a la relación entre una variable dependiente categórica y una o varias covariables, cuando la variable dependiente es dicotómica (regresión logística binaria, caso más sencillo de regresión logística) no es apropiado utilizar un modelo de regresión lineal porque una variable dicotómica no se ajusta a una distribución normal, sino a una binomial. Ignorar esta cuestión podría llevar a obtener probabilidades imposibles: menores que cero o mayores que uno.\nPara evitar este problema, es preferible utilizar funciones que realicen predicciones comprendidas entre un máximo y un mínimo. Una de estas funciones - posiblemente la más empleada - es la curva logística o función sigmoide:\n\\[\\begin{align}\n\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\n\\end{align}\\]\nEs decir, estamos estimando con una regresión lineal el valor de \\(\\eta\\), que sí es una v.a. continua - a diferencia de Y que es binaria-.\nEsto es, \\(p=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\). De esta forma, para valores positivos muy grandes de \\(\\eta\\) llamado odds, \\(e^{-\\eta}\\) es aproximadamente cero, por lo que el valor de la función es 1; mientras que para valores negativos muy grandes de \\(\\eta\\), \\(e^{-\\eta}\\) tiende a infinito, haciendo que el valor de la función sea 0.\nA continuación, para simplificar un poco las cosas, consideremos el modelo de regresión logística más sencillo: regresión logística binaria simple (una sola covariable):\n\\[\n\\begin{align}\nP(Y=1)=\\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ϵ)}}\n\\end{align}\n\\]\nLa interpretación de esta función es muy similar a la de una regresión lineal: el coeficiente \\(\\beta_0\\) representa la posición de la curva sobre el eje horizontal o de abscisas (más hacia la izquierda o más hacia la derecha); mientras que \\(\\beta_1\\) representa la pendiente de la curva, es decir, cuán inclinada está en su parte central (cuanto más inclinada, mayor capacidad de discriminar entre los dos valores de la variable dependiente).\nSi estuviésemos ante una regresión logística múltiple, cada variable independiente recibiría una ponderación proporcional a su capacidad para predecir Y.\nEjemplo sencillo Vamos a mostrar como una variable binaria no tiene sentido predecirla con una Regresión Lineal sino Logística.\n\n# Generación de datos para el ejemplo\nset.seed(123)\nn &lt;- 200\nAltura &lt;- rnorm(n, mean = 165, sd = 10)\n\n# Crear una variable binaria 'Sexo' en función de Altura\nSexo &lt;- as.factor(ifelse(Altura + rnorm(n) &gt; 165, 1, 0))\ndatos_ejemplo &lt;- data.frame(Altura, Sexo)\n\n# Regresión lineal\nmodelo_lineal &lt;- lm(Sexo ~ Altura, data = datos_ejemplo)\n\n# Regresión logística\nmodelo_logistico &lt;- glm(Sexo ~ Altura, data = datos_ejemplo, family = binomial)\n\n\n# Regresión Lineal\npar(mfrow = c(1, 2))\nplot(datos_ejemplo$Altura, datos_ejemplo$Sexo, col = \"lightblue\", main = \"Ajuste por Regresión Lineal\")\nabline(modelo_lineal, col = \"navy\")\n\n# Regresión Logística\n\nplot(datos_ejemplo$Altura, as.numeric(datos_ejemplo$Sexo) - 1, col = \"lightblue\", main = \"Regresión Logística\", xlab = \"Altura\", ylab = \"Sexo\")\ncurve(predict(modelo_logistico, data.frame(Altura = x), type = \"response\"), add = TRUE, col = \"navy\", lwd = 2)\n\n\n\n\n\n\n\n\nEn este ejemplo, se muestra cómo un ajuste por regresión lineal no se adapta bien a datos binarios, produciendo predicciones que pueden ser mayores que 1 o menores que 0. En cambio, la regresión logística produce una curva en forma de S que se adapta mejor a los datos, con predicciones que están siempre entre 0 y 1. Esto demuestra que para problemas de clasificación binaria, la regresión logística es una mejor opción que la regresión lineal.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#bondad-de-ajuste-e-interpretación-modelo",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#bondad-de-ajuste-e-interpretación-modelo",
    "title": "Regresión Logística: Partos",
    "section": "Bondad de Ajuste e Interpretación Modelo",
    "text": "Bondad de Ajuste e Interpretación Modelo\n\nInterpretación Modelo\nRecordar que el modelo tomaba la forma \\[\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\\], es decir, estamos estimando el log(odds). Esto nos lleva a las siguientes apreciaciones:\nAunque tanto \\(P(Y=1)\\), como \\(Odds(Y=1)\\), como \\(\\operatorname{logit}(Y=1)\\) expresan la misma idea, están en distinta escala:\n\nLa probabilidad toma valores comprendidos entre 0 y 1.\nLa odds tiene un valor mínimo de cero y no tiene máximo.\nLa logit o log(odds) no tiene ni mínimo ni máximo.\n\nPor ejemplo, a una probabilidad de 0,5, le corresponde una odds de 1 y un logit de 0. Ahora bien, es cierto que razonar en términos de cambios en los logaritmos resulta poco intuitivo. Por ello, es preferible interpretar el cambio en las odds o en la razón de ventajas (también llamada odds ratio, razón de probabilidades o razón de momios).\nLa interpretación más frecuente es interpretar los signos de los coeficientes del modelo, es decir, los signos de \\(\\beta_1, \\ldots , \\beta_k\\).\n\nSi \\(\\beta_i &gt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en un aumento de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) aumenta, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\nSi \\(\\beta_i &lt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en una disminución de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) disminuye, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\n\nUna pregunta importante en cualquier análisis de regresión es si el modelo propuesto se ajusta adecuadamente a los datos, lo que conduce naturalmente a la noción de una prueba formal para la falta de ajuste (o bondad de ajuste).\n\n\nMedidas Especifidad y Sensibilidad\nLa especificidad y la sensibilidad son medidas utilizadas para evaluar el rendimiento de un modelo predictivo, especialmente en problemas de clasificación binaria (donde solo hay dos clases). Las definimos como:\n\nSensibilidad (Sensitivity): Es la proporción de verdaderos positivos (casos positivos correctamente identificados) respecto al total de casos positivos reales. Es la capacidad del modelo para identificar correctamente los casos positivos.\nEspecificidad (Specificity): Es la proporción de verdaderos negativos (casos negativos correctamente identificados) respecto al total de casos negativos reales. Representa la capacidad del modelo para identificar correctamente los casos negativos.\n\nUn equilibrio entre ambas es deseable, pero depende del contexto específico del problema y de las consecuencias de los falsos positivos y falsos negativos. En el caso, por ejemplo, de detectar si un paciente tiene cáncer o no, parece más razonable centrarse en los Falsos Negativos, ya que un paciente que tiene cáncer no lo estamos detectando, lo que lleva un riesgo implícito muy alto.\n\n\n\n\n\n\n\n\n\n\nClasificado como Positivo\nClasificado como Negativo\nTotal\n\n\n\n\nRealmente Positivo\nVerdadero Positivo (VP)\nFalso Negativo (FN)\nVP + FN\n\n\nRealmente Negativo\nFalso Positivo (FP)\nVerdadero Negativo (VN)\nFP + VN\n\n\nTotal\nVP + FP\nFN + VN\n\n\n\n\nSensibilidad ( )\nEspecificidad: ( )\n\n\nCurva ROC\nLa curva ROC es una representación gráfica de la sensibilidad frente a la tasa de falsos positivos a varios umbrales de clasificación. Se utiliza comúnmente en análisis de clasificación para evaluar el rendimiento de un modelo.\nPara calcular el área bajo la curva ROC (AUC-ROC), se utiliza la tasa de falsos positivos y de falsos negativos. Cuanto más cerca esté el AUC-ROC de 1, mejor será el rendimiento del modelo, ya que indica una mayor capacidad de distinguir entre clases.\nEs una medida de bondad porque evalúa qué tan bien puede discriminar un modelo entre las clases positivas y negativas. Cuanto más se acerque el AUC a 1, mejor será la capacidad del modelo para distinguir entre las clases. Se utiliza para comparar y seleccionar modelos, donde un AUC mayor indica un mejor rendimiento predictivo.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#formulación",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#formulación",
    "title": "Regresión Logística: Partos",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE: Convertir a factor las variables que tengan que ser tratadas como tal, de lo contrario R las tratará como numéricas. Además, la variable respuesta debe tener los niveles codificados como \\(0\\) y \\(1\\) para poder usar la función glm.\n\ndatos1$cesarea &lt;- as.factor(datos1$cesarea)\ndatos1$bebes &lt;- as.factor(datos1$bebes)\ndatos1$parto_normal &lt;- as.factor(datos1$parto_normal)\n\nA continuación presentamos tres posibles modelos y posteriormente elegiremos uno de ellos.\n\nlmod1 : Queremos clasificar si hay cesárea en el parto en función de edad_madre, semanas de gestación.\nlmod2 : Queremos clasificar si hay cesárea en el parto en función de edad_madre, semanas de gestación, número de nacimientos en el parto.\nlmod3 : Queremos clasificar si hay cesárea en el parto en función de edad_madre, semanas de gestación, número de nacimientos en el parto y si hubo complicaciones en el parto.\n\n\n# lmod1\nlmod1 &lt;- glm(formula = cesarea ~ edad_madre + semanas, family = binomial(link = logit), data = datos1)\nsummary(lmod1)\n\n\nCall:\nglm(formula = cesarea ~ edad_madre + semanas, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.67592    1.15715   3.177  0.00149 ** \nedad_madre   0.05222    0.01162   4.493 7.01e-06 ***\nsemanas     -0.18442    0.02758  -6.688 2.27e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2421.6  on 2808  degrees of freedom\nResidual deviance: 2353.4  on 2806  degrees of freedom\nAIC: 2359.4\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod2\nlmod2 &lt;- glm(formula = cesarea ~ edad_madre + semanas + bebes, family = binomial(link = logit), data = datos1)\nsummary(lmod2)\n\n\nCall:\nglm(formula = cesarea ~ edad_madre + semanas + bebes, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.71472    1.19260   2.276 0.022828 *  \nedad_madre   0.04917    0.01167   4.213 2.52e-05 ***\nsemanas     -0.15782    0.02860  -5.518 3.44e-08 ***\nbebes2       1.11969    0.29308   3.820 0.000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2421.6  on 2808  degrees of freedom\nResidual deviance: 2339.5  on 2805  degrees of freedom\nAIC: 2347.5\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod3\nlmod3 &lt;- glm(formula = cesarea ~ edad_madre + semanas + bebes + parto_normal, family = binomial(link = logit), data = datos1)\nsummary(lmod3)\n\n\nCall:\nglm(formula = cesarea ~ edad_madre + semanas + bebes + parto_normal, \n    family = binomial(link = logit), data = datos1)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    2.65290    1.30581   2.032  0.04219 *  \nedad_madre     0.03975    0.01246   3.189  0.00143 ** \nsemanas       -0.16412    0.03164  -5.188 2.13e-07 ***\nbebes2         0.76749    0.33415   2.297  0.02163 *  \nparto_normal2  2.13934    0.11780  18.160  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2421.6  on 2808  degrees of freedom\nResidual deviance: 2010.4  on 2804  degrees of freedom\nAIC: 2020.4\n\nNumber of Fisher Scoring iterations: 5\n\n\nEl tercer modelo tiene el valor más bajo de la deviance residual, lo que indica un mejor ajuste del modelo a los datos en comparación con los dos modelos anteriores. Esto sugiere que la inclusión de “parto_normal” mejora la capacidad del modelo para explicar la variabilidad observada en la variable de respuesta. Además, también es el modelo con el valor más bajo para AIC luego parece razonable quedarnos con el.\nPara este modelo vamos a calcular la matriz de confusión y el área ROC. Como punto de corte para clasificar la observación como cesárea o no, tomamos p=0.227 (obtenido más abajo al ver la ROC). Es decir, si bajo el modelo una observación presenta una predice una probabilidad de cesárea mayor de 0.227, entonces lo clasificaremos como tal.\n\n# confusion matrices\npredicted3 &lt;- predict(lmod3, datos1[, c(\"edad_madre\", \"semanas\", \"bebes\", \"parto_normal\")], type = \"response\")\n\nconfusionMatrix(data = as.factor(ifelse(predicted3 &gt; 0.227, 1, 0)), reference = datos1$cesarea, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2069  191\n         1  305  244\n                                          \n               Accuracy : 0.8234          \n                 95% CI : (0.8088, 0.8374)\n    No Information Rate : 0.8451          \n    P-Value [Acc &gt; NIR] : 0.9992          \n                                          \n                  Kappa : 0.3906          \n                                          \n Mcnemar's Test P-Value : 3.898e-07       \n                                          \n            Sensitivity : 0.56092         \n            Specificity : 0.87152         \n         Pos Pred Value : 0.44444         \n         Neg Pred Value : 0.91549         \n             Prevalence : 0.15486         \n         Detection Rate : 0.08686         \n   Detection Prevalence : 0.19544         \n      Balanced Accuracy : 0.71622         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n# The ROC function\n\n# lmod3\nROC(form = cesarea ~ +semanas + bebes + parto_normal, data = datos1, plot = \"ROC\", lwd = 3, cex = 1.5)\n\n\n\n\n\n\n\n\nObservamos una Especifidad del 62% y una Sensibilidad del 50%. Esto quiere decir que nuestro modelo es mejor evitando falsos positivos, que falsos negativos. Es decir, que es mejor evitando que digamos que la calidad de vida de una persona ha mejorado cuando realmente no lo ha hecho, que al revés. Lo cual es deseable.\nDestacar que el elemento Ir.eta que aparece arriba, es el punto de corte de la probabilidad. Es decir, si nuestra regresión logística predice que hay una probabilidad mayor de \\(0.179\\) de que haya mejorado la calidad de vida, nosotros lo clasificamos como que efectivamente ha mejorado y si es menor, lo clasificamos como que no.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#otras-consideraciones",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#otras-consideraciones",
    "title": "Regresión Logística: Partos",
    "section": "Otras consideraciones",
    "text": "Otras consideraciones\nPodemos usar el presente modelo para predecir la probabilidad de cesárea en función de las variables predictoras de nuevas observaciones.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Partos.html#interpretación-coeficientes",
    "href": "notebooks/Logistic Regression/Partos/Partos.html#interpretación-coeficientes",
    "title": "Regresión Logística: Partos",
    "section": "Interpretación coeficientes",
    "text": "Interpretación coeficientes\nVamos a volver a sacar el summary del modelo para proceder a explicar todo bien de nuevo.\n\nsummary(lmod2)\n\n\nCall:\nglm(formula = cesarea ~ edad_madre + semanas + bebes, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.71472    1.19260   2.276 0.022828 *  \nedad_madre   0.04917    0.01167   4.213 2.52e-05 ***\nsemanas     -0.15782    0.02860  -5.518 3.44e-08 ***\nbebes2       1.11969    0.29308   3.820 0.000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2421.6  on 2808  degrees of freedom\nResidual deviance: 2339.5  on 2805  degrees of freedom\nAIC: 2347.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nEdad_madre: Por cada incremento unitario en la edad de la madre, el logaritmo de odds de éxito aumenta aproximadamente en 0.04, manteniendo constante el resto de variables.\nsemanas: Por cada incremento unitario en las semanas, el logaritmo de odds de éxito disminuye aproximadamente en 0.15, manteniendo constante el resto de variables.\nbebes2: Cuando la variable bebes2 cambia de 0 a 1 , el logaritmo de odds de éxito aumenta aproximadamente en 1.19, manteniendo constante el resto de variables.\n\nEs decir, la variable que mas aumenta la probabilidad de cesárea en comparación con el resto, es tener más de un bebé en el parto.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Regresión Logística: Partos"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html",
    "title": "Análisis Discriminante: salud",
    "section": "",
    "text": "En este notebook se va a explicar el método de Análisis de Discriminante Lineal y Cuadrático con el objetivo de clasificar una variable binaria. Mediante un ejemplo práctico se mostrará como llevar a cabo dichas técnicas.\n\n\nEn este cuaderno vamos a analizar el dataset llamado salud.xlsx. Este contiene microdatos relativos a la Encuesta Nacional de Salud. Concretamente, datos correspondientes al año 2017. Las variables de interés son las siguientes:\n\nEDAD: Identificación del adulto seleccionado: Edad.\nSEXO: Identificación del adulto seleccionado: Sexo.\nAltura: Altura en cm.\nPeso: Peso en kg.\n\nEl objetivo de este estudio será aplicar un Análisis Discriminante para intentar clasificar el sexo de la población a partir del resto de variables.\n\n\n\nSe pretende hacer un Análisis de Discriminante empleando los procedimientos de LDA (Análisis de Discriminante Lineal) y QDA (Análisis de Discriminante Cuadrático).\n\nHacer un análisis exploratorio. Ver si las variables separan bien las clases.\nConvertir a factor la variable respuesta.\nRealizar una partición de datos Train/Test.\nFijar semilla cuando se usen números aleatorios (para la reproductividad).\nEjecutar algoritmos lda/qda y evaluar su bondad.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#dataset",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#dataset",
    "title": "Análisis Discriminante: salud",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado salud.xlsx. Este contiene microdatos relativos a la Encuesta Nacional de Salud. Concretamente, datos correspondientes al año 2017. Las variables de interés son las siguientes:\n\nEDAD: Identificación del adulto seleccionado: Edad.\nSEXO: Identificación del adulto seleccionado: Sexo.\nAltura: Altura en cm.\nPeso: Peso en kg.\n\nEl objetivo de este estudio será aplicar un Análisis Discriminante para intentar clasificar el sexo de la población a partir del resto de variables.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#descripción-del-trabajo-a-realizar",
    "title": "Análisis Discriminante: salud",
    "section": "",
    "text": "Se pretende hacer un Análisis de Discriminante empleando los procedimientos de LDA (Análisis de Discriminante Lineal) y QDA (Análisis de Discriminante Cuadrático).\n\nHacer un análisis exploratorio. Ver si las variables separan bien las clases.\nConvertir a factor la variable respuesta.\nRealizar una partición de datos Train/Test.\nFijar semilla cuando se usen números aleatorios (para la reproductividad).\nEjecutar algoritmos lda/qda y evaluar su bondad.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#cargar-librerías",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#cargar-librerías",
    "title": "Análisis Discriminante: salud",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(caret) # For data paratition\nlibrary(MASS) # funciones lda() y qda()\nlibrary(klaR) # partimat",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#lectura-datos",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#lectura-datos",
    "title": "Análisis Discriminante: salud",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../../files/salud.xlsx\", sheet = \"Datos\")\n\nVeamos un resumen de los datos y si hay algún NA para quitarlo/imputarlo.\n\ndatos$Edad &lt;- as.numeric(datos$Edad)\ndatos$Altura &lt;- as.numeric(datos$Altura)\ndatos$Peso &lt;- as.numeric(datos$Peso)\nsummary(datos)\n\n      Edad            Sexo               Altura           Peso       \n Min.   : 15.00   Length:23089       Min.   :120.0   Min.   : 26.00  \n 1st Qu.: 39.00   Class :character   1st Qu.:160.0   1st Qu.: 62.00  \n Median : 52.00   Mode  :character   Median :166.0   Median : 71.00  \n Mean   : 52.84                      Mean   :166.7   Mean   : 72.66  \n 3rd Qu.: 67.00                      3rd Qu.:173.0   3rd Qu.: 81.00  \n Max.   :103.00                      Max.   :204.0   Max.   :180.00  \n NA's   :1070                        NA's   :1070    NA's   :1070    \n\ncat(\"Hay un número de NA's igual a: \",sum(is.na(datos)),\"\\n\")\n\nHay un número de NA's igual a:  4280",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#introducción-1",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#introducción-1",
    "title": "Análisis Discriminante: salud",
    "section": "Introducción",
    "text": "Introducción\nEl análisis discriminante es una técnica estadística utilizada para clasificar observaciones en grupos o categorías predefinidas en función de un conjunto de variables predictoras.\nEl objetivo principal es identificar las características o variables que mejor distinguen entre diferentes grupos conocidos. Es una técnica de aprendizaje supervisado, lo que significa que requiere un conjunto de datos etiquetado con información sobre las categorías o grupos a los que pertenecen las observaciones.\nEl análisis discriminante se puede dividir en dos tipos principales:\n\nAnálisis Discriminante Lineal (LDA): LDA asume que las variables predictoras tienen una distribución normal y que las matrices de covarianza de las variables predictoras son iguales para todos los grupos. LDA busca un hiperplano (o límite de decisión) que maximice la distancia entre las medias de los grupos y minimice la varianza dentro de cada grupo. Debido a la asunción de igual covarianzas, los hiperplanos que separan las clases son lineales.\nAnálisis Discriminante Cuadrático (QDA): QDA relaja la suposición de igualdad de matrices de covarianza y permite que cada grupo tenga su propia matriz de covarianza. Esto hace que QDA sea más flexible pero también requiere más datos para estimar las matrices de covarianza para cada grupo. Los hiperplanos que separan las clases son cuadráticos.\n\nAhora grafiquemos los histogramas de las variables distinguiendo entre variable respuesta a ver si alguno de ellos parece separar bien las variables y por tanto es susceptible de que funcione luego bien en LDA/QDA.\nVamos a dibujar las densidades de las variables distinguiendo entre las clases de la variable respuesta para ver si alguna de las variables permite una clara separación entre variables.\n\nlibrary(ggplot2)\nggplot(datos, aes(Edad)) +\n  geom_density(aes(group = Sexo, colour = Sexo, fill = Sexo), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(datos, aes(Altura)) +\n  geom_density(aes(group = Sexo, colour = Sexo, fill = Sexo), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(datos, aes(Peso)) +\n  geom_density(aes(group = Sexo, colour = Sexo, fill = Sexo), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\nComo cabía esperar, la variable edad no discrimina bien el sexo de una persona debido a que no hay una relación directa. Sin embargo, tanto en las variables peso como altura se observa como la clase 1 (hombres) muestran valores más altos que las mujeres en ambas variables. Esto puede ser un buen indicador a la hora de discriminar.\nVeamos ahora en 2D las clasificaciones y observamos que hay buenas discriminaciones, luego todo nos hace pensar que los métodos de después funcionarán bastante bien.\n\ndatos$Sexo &lt;- as.factor(datos$Sexo)\nLabel &lt;- datos$Sexo\ncolo &lt;- c(\"pink\", \"#1874CD\")[Label]\nX &lt;- datos[, c(1, 3:4)]\npairs(X, main = \"Diabetes data set\", pch = 20, col = colo, lower.panel = NULL, cex = 0.2, oma = c(1, 3, 3, 15))\n\n\n\n\n\n\n\n\nMUY IMPORTANTE CONVERTIR LA VARIABLE RESPUESTA A FACTOR ya que representa dos clases, es decir, es una variable cualitativa.\n\ndatos$Sexo &lt;- as.factor(datos$Sexo)",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#partición-de-datos",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#partición-de-datos",
    "title": "Análisis Discriminante: salud",
    "section": "Partición de datos",
    "text": "Partición de datos\nPara evitar el overffiting debido a que el modelo se ha entrenado demasiado con los datos proporcionados, vamos a realizar una partición de datos y una vez entrenado el modelo, lo evaluaremos con la partición segunda.\nPara ello usamos la función caret::createDataPartition() que nos permite mantener la proporción de la variable binaria respuesta. En caso de no usar esta función podríamos estar dejando en la partición de entreno observaciones con la misma variable respuesta y dejar la clase minoritaria infrarepresentada, lo que podría incurrir en un mal ajuste del modelo para dicha clase.\n\nset.seed(785248) # For Reproducibility\nspl &lt;- createDataPartition(datos$Sexo, p = 0.75, list = FALSE) # 75% for training\nsaludTrain &lt;- datos[spl, ]\nsaludTest &lt;- datos[-spl, ]",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#análisis-de-discriminante-lineal-lda",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#análisis-de-discriminante-lineal-lda",
    "title": "Análisis Discriminante: salud",
    "section": "Análisis de Discriminante Lineal (LDA)",
    "text": "Análisis de Discriminante Lineal (LDA)\nComo se ha comentado previamente:\n\nSe supone que la distribución de los datos es una normal multivariante.\nSupone que todas las matrices de covarianzas son iguales ( y por tanto el clasificador es lineal).\n\n\nset.seed(785248) # For Reproducibility\n\nlda.class.salud &lt;- lda(Sexo ~ Altura + Peso + Edad, data = saludTrain)\n# qda.class.diabetes\n\nplot &lt;- partimat(Sexo ~ Altura + Peso + Edad, data = saludTrain, method = \"lda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(785248)\n\n# predict with the lda model and test data\npred.lda &lt;- predict(lda.class.salud, saludTest)$class\n\n# confusion matrix\nconfusionMatrix(pred.lda, saludTest$Sexo)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    1    2\n         1 2072  411\n         2  507 2514\n                                         \n               Accuracy : 0.8332         \n                 95% CI : (0.8231, 0.843)\n    No Information Rate : 0.5314         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.6644         \n                                         \n Mcnemar's Test P-Value : 0.001716       \n                                         \n            Sensitivity : 0.8034         \n            Specificity : 0.8595         \n         Pos Pred Value : 0.8345         \n         Neg Pred Value : 0.8322         \n             Prevalence : 0.4686         \n         Detection Rate : 0.3765         \n   Detection Prevalence : 0.4511         \n      Balanced Accuracy : 0.8314         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\nExactitud (Accuracy): La exactitud del modelo es del 83%, lo que significa que el modelo clasificó correctamente aproximadamente el 83% de las instancias en el conjunto de prueba.\nSensibilidad (Sensitivity o Recall): La sensibilidad del modelo para la clase 1 es del 80%, lo que indica que el modelo identificó correctamente alrededor del 80% de las instancias de la clase 1 en el conjunto de prueba.\nEspecificidad (Specificity): La especificidad del modelo para la clase 0 es del 85%, lo que significa que el modelo identificó correctamente alrededor del 85% de las instancias de la clase 0 en el conjunto de prueba.\n\nEn general, estos resultados muestran que el modelo LDA tiene un rendimiento bueno en la clasificación de las instancias en el conjunto de prueba, con una exactitud y un kappa significativos.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#análisis-de-discriminante-cuadrático",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/salud.html#análisis-de-discriminante-cuadrático",
    "title": "Análisis Discriminante: salud",
    "section": "Análisis de Discriminante Cuadrático",
    "text": "Análisis de Discriminante Cuadrático\nComo se ha comentado previamente:\n\nSe supone que la distribución de los datos es una normal multivariante.\nNO supone que todas las matrices de covarianzas son iguales ( y por tanto el clasificador es cuadrático y no lineal).\nMétodo muy inestable a menos que tengamos muestras muy grandes y de dimensiones bajas.\n\nEs más inestable debido a que al permitir que cada grupo tenga su propia matriz de covarianza, aunque lo que lo hace más flexible, requiere muchos más datos para estimar las matrices de covarianza para cada grupo.\n\nset.seed(785248) # For Reproducibility\n\nqda.class.salud &lt;- qda(Sexo ~ Altura + Peso + Edad, data = saludTrain)\n# qda.class.diabetes\n\nplot &lt;- partimat(Sexo ~ Altura + Peso + Edad, data = saludTrain, method = \"qda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(785248)\n\n# predict with the lda model and test data\npred.qda &lt;- predict(qda.class.salud, saludTest)$class\n\n# confusion matrix\nconfusionMatrix(pred.qda, saludTest$Sexo)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    1    2\n         1 2068  412\n         2  511 2513\n                                          \n               Accuracy : 0.8323          \n                 95% CI : (0.8222, 0.8421)\n    No Information Rate : 0.5314          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6625          \n                                          \n Mcnemar's Test P-Value : 0.001257        \n                                          \n            Sensitivity : 0.8019          \n            Specificity : 0.8591          \n         Pos Pred Value : 0.8339          \n         Neg Pred Value : 0.8310          \n             Prevalence : 0.4686          \n         Detection Rate : 0.3757          \n   Detection Prevalence : 0.4506          \n      Balanced Accuracy : 0.8305          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nExactitud (Accuracy): La exactitud del modelo es del 83%, lo que significa que el modelo clasificó correctamente aproximadamente el 83% de las instancias en el conjunto de prueba.\nSensibilidad (Sensitivity o Recall): La sensibilidad del modelo para la clase 1 es del 80%, lo que indica que el modelo identificó correctamente alrededor del 80% de las instancias de la clase 1 en el conjunto de prueba.\nEspecificidad (Specificity): La especificidad del modelo para la clase 0 es del 85%, lo que significa que el modelo identificó correctamente alrededor del 85% de las instancias de la clase 0 en el conjunto de prueba.\n\nComparando estos resultados con los del modelo LDA, observamos ambos modelos tienen una exactitud, sensibilidad y valor predictivo negativo parecidos.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Análisis Discriminante: salud"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html",
    "title": "Análisis Discriminante: laboral",
    "section": "",
    "text": "En este notebook se va a explicar el método de Análisis de Discriminante Lineal y Cuadrático con el objetivo de clasificar una variable binaria. Mediante un ejemplo práctico se mostrará como llevar a cabo dichas técnicas.\n\n\nEn este cuaderno vamos a analizar el dataset llamado laboral.xlsx. Este contiene microdatos relativos a la Encuestas de estructura salarial. Concretamente, datos correspondientes al año 2018. Las variables de interés son las siguientes:\n\nEstudios: Nivel de estudios del encuestado. Valor 1 corresponde a individuos con muy bajo nivel académico (hasta primaria), y 0 a individuos con al menos nivel académico universitario. Notar que en el conjunto de datos no hay encuestados con nivel de estudios medio, sólo muy bajo o muy alto.\nSalario: Sueldo bruto anual.\nEdad: Grupo de Edad del encuestado. Puesto que las clases tienen un orden intrínseco, la variable la vamos a tratar como cuantitativa.\n\n01 MENOS 19 AÑOS\n02 DE 20 A 29\n\n03 DE 30 A 39\n\n04 DE 40 A 49\n\n05 DE 50 A 59\n\n06 MÁS DE 59\n\nAntiguedad: Años de antigüedad.\nVacaciones: Días de vacaciones al año.\n\nEl objetivo de este estudio será aplicar un Análisis Discriminante para clasificar a la población entre alto y bajo nivel de estudios a partir de estas variables. Además se verá si el salario en si mismo constituye un buen factor de discriminación.\n\n\n\nSe pretende hacer un Análisis de Discriminante empleando los procedimientos de LDA (Análisis de Discriminante Lineal) y QDA (Análisis de Discriminante Cuadrático).\n\nHacer un análisis exploratorio. Ver si las variables separan bien las clases.\nConvertir a factor la variable respuesta.\nRealizar una partición de datos Train/Test.\nFijar semilla cuando se usen números aleatorios (para la reproductividad).\nEjecutar algoritmos lda/qda y evaluar su bondad.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#dataset",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#dataset",
    "title": "Análisis Discriminante: laboral",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado laboral.xlsx. Este contiene microdatos relativos a la Encuestas de estructura salarial. Concretamente, datos correspondientes al año 2018. Las variables de interés son las siguientes:\n\nEstudios: Nivel de estudios del encuestado. Valor 1 corresponde a individuos con muy bajo nivel académico (hasta primaria), y 0 a individuos con al menos nivel académico universitario. Notar que en el conjunto de datos no hay encuestados con nivel de estudios medio, sólo muy bajo o muy alto.\nSalario: Sueldo bruto anual.\nEdad: Grupo de Edad del encuestado. Puesto que las clases tienen un orden intrínseco, la variable la vamos a tratar como cuantitativa.\n\n01 MENOS 19 AÑOS\n02 DE 20 A 29\n\n03 DE 30 A 39\n\n04 DE 40 A 49\n\n05 DE 50 A 59\n\n06 MÁS DE 59\n\nAntiguedad: Años de antigüedad.\nVacaciones: Días de vacaciones al año.\n\nEl objetivo de este estudio será aplicar un Análisis Discriminante para clasificar a la población entre alto y bajo nivel de estudios a partir de estas variables. Además se verá si el salario en si mismo constituye un buen factor de discriminación.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#descripción-del-trabajo-a-realizar",
    "title": "Análisis Discriminante: laboral",
    "section": "",
    "text": "Se pretende hacer un Análisis de Discriminante empleando los procedimientos de LDA (Análisis de Discriminante Lineal) y QDA (Análisis de Discriminante Cuadrático).\n\nHacer un análisis exploratorio. Ver si las variables separan bien las clases.\nConvertir a factor la variable respuesta.\nRealizar una partición de datos Train/Test.\nFijar semilla cuando se usen números aleatorios (para la reproductividad).\nEjecutar algoritmos lda/qda y evaluar su bondad.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#cargar-librerías",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#cargar-librerías",
    "title": "Análisis Discriminante: laboral",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(caret) # For data paratition\nlibrary(MASS) # funciones lda() y qda()\nlibrary(klaR) # partimat",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#lectura-datos",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#lectura-datos",
    "title": "Análisis Discriminante: laboral",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../../files/laboral.xlsx\", sheet = \"Datos\")\n\nVeamos un resumen de los datos y si hay algún NA para quitarlo/imputarlo.\n\nsummary(datos)\n\n   Estudios              Edad         Vacaciones      Antiguedad  \n Length:99782       Min.   :1.000   Min.   : 0.00   Min.   : 0.0  \n Class :character   1st Qu.:3.000   1st Qu.: 0.00   1st Qu.: 2.0  \n Mode  :character   Median :4.000   Median :22.00   Median : 9.0  \n                    Mean   :3.955   Mean   :12.99   Mean   :10.5  \n                    3rd Qu.:5.000   3rd Qu.:23.00   3rd Qu.:16.0  \n                    Max.   :6.000   Max.   :99.00   Max.   :52.0  \n    Salario      \n Min.   :    63  \n 1st Qu.: 15200  \n Median : 24959  \n Mean   : 30106  \n 3rd Qu.: 39228  \n Max.   :100000  \n\nsum(is.na(datos))\n\n[1] 0",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#introducción-1",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#introducción-1",
    "title": "Análisis Discriminante: laboral",
    "section": "Introducción",
    "text": "Introducción\nEl análisis discriminante es una técnica estadística utilizada para clasificar observaciones en grupos o categorías predefinidas en función de un conjunto de variables predictoras.\nEl objetivo principal es identificar las características o variables que mejor distinguen entre diferentes grupos conocidos. Es una técnica de aprendizaje supervisado, lo que significa que requiere un conjunto de datos etiquetado con información sobre las categorías o grupos a los que pertenecen las observaciones.\nEl análisis discriminante se puede dividir en dos tipos principales:\n\nAnálisis Discriminante Lineal (LDA): LDA asume que las variables predictoras tienen una distribución normal y que las matrices de covarianza de las variables predictoras son iguales para todos los grupos. LDA busca un hiperplano (o límite de decisión) que maximice la distancia entre las medias de los grupos y minimice la varianza dentro de cada grupo. Debido a la asunción de igual covarianzas, los hiperplanos que separan las clases son lineales.\nAnálisis Discriminante Cuadrático (QDA): QDA relaja la suposición de igualdad de matrices de covarianza y permite que cada grupo tenga su propia matriz de covarianza. Esto hace que QDA sea más flexible pero también requiere más datos para estimar las matrices de covarianza para cada grupo. Los hiperplanos que separan las clases son cuadráticos.\n\nAhora grafiquemos los histogramas de las variables distinguiendo entre variable respuesta a ver si alguno de ellos parece separar bien las variables y por tanto es susceptible de que funcione luego bien en LDA/QDA.\nVamos a dibujar las densidades de las variables distinguiendo entre las clases de la variable respuesta para ver si alguna de las variables permite una clara separación entre variables.\n\nlibrary(ggplot2)\nggplot(datos, aes(Salario)) +\n  geom_density(aes(group = Estudios, colour = Estudios, fill = Estudios), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(datos, aes(Vacaciones)) +\n  geom_density(aes(group = Estudios, colour = Estudios, fill = Estudios), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(datos, aes(Antiguedad)) +\n  geom_density(aes(group = Estudios, colour = Estudios, fill = Estudios), alpha = 0.1) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\nVemos que la única que parece discriminar algo la variable respuesta, es la variable salarios. Se observa que los que tienen variable respuesta 0, en la mayoría de los casos presentan un salario anual bruto mayor. Luego esto puede ser un indicador de que el presente método funcionará.\nVeamos ahora en 2D las clasificaciones y observamos que la variable Salario parece discriminar muy bien las clases cuando la juntamos con cualquiera de las demás, ya que se ven los colores de ambas clases separados en la última columna.\n\ndatos$Estudios &lt;- as.factor(datos$Estudios)\nLabel &lt;- datos$Estudios\ncolo &lt;- c(\"pink\", \"#1874CD\")[Label]\nX &lt;- datos[, c(2:5)]\npairs(X, main = \"Diabetes data set\", pch = 20, col = colo, lower.panel = NULL, cex = 0.2, oma = c(1, 3, 3, 15))\n\n\n\n\n\n\n\n\nPor último, visualizamos las correlaciones entre variables, que no son muy altas. Aunque las correlaciones no son influyentes para el uso de este tipo de algoritmos, es aconsejable visualizarlas por si se encuentra algún dato llamativo.\n\nlibrary(corrplot)\ncorrplot(cor((datos[, 2:5])),\n  type = \"upper\",\n  order = \"original\",\n  method = \"shade\",\n  tl.col = \"black\",\n  tl.srt = 45,\n  addCoef.col = \"black\",\n  diag = FALSE\n)\n\n\n\n\n\n\n\n\nMUY IMPORTANTE CONVERTIR LA VARIABLE RESPUESTA A FACTOR ya que representa dos clases, es decir, es una variable cualitativa.\n\ndatos$Estudios &lt;- as.factor(datos$Estudios)",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#partición-de-datos",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#partición-de-datos",
    "title": "Análisis Discriminante: laboral",
    "section": "Partición de datos",
    "text": "Partición de datos\nPara evitar el overffiting debido a que el modelo se ha entrenado demasiado con los datos proporcionados, vamos a realizar una partición de datos y una vez entrenado el modelo, lo evaluaremos con la partición segunda.\nPara ello usamos la función caret::createDataPartition() que nos permite mantener la proporción de la variable binaria respuesta. En caso de no usar esta función podríamos estar dejando en la partición de entreno observaciones con la misma variable respuesta y dejar la clase minoritaria infrarepresentada, lo que podría incurrir en un mal ajuste del modelo para dicha clase.\n\nset.seed(785248) # For Reproducibility\nspl &lt;- createDataPartition(datos$Estudios, p = 0.75, list = FALSE) # 75% for training\nlaboralTrain &lt;- datos[spl, ]\nlaboralTest &lt;- datos[-spl, ]",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#análisis-de-discriminante-lineal-lda",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#análisis-de-discriminante-lineal-lda",
    "title": "Análisis Discriminante: laboral",
    "section": "Análisis de Discriminante Lineal (LDA)",
    "text": "Análisis de Discriminante Lineal (LDA)\nComo se ha comentado previamente:\n\nSe supone que la distribución de los datos es una normal multivariante.\nSupone que todas las matrices de covarianzas son iguales ( y por tanto el clasificador es lineal).\n\n\nset.seed(785248) # For Reproducibility\n\nlda.class.laboral &lt;- lda(Estudios ~ Salario + Edad + Vacaciones + Antiguedad, data = laboralTrain)\n# qda.class.diabetes\n\n\nplot &lt;- partimat(Estudios ~ Salario + Edad + Vacaciones + Antiguedad, data = laboralTrain, method = \"lda\")\n\n\n\n\n\n\n\n\n\nset.seed(785248)\n\n# predict with the lda model and test data\npred.lda &lt;- predict(lda.class.laboral, laboralTest)$class\n\n# confusion matrix\nconfusionMatrix(pred.lda, laboralTest$Estudios)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 13492  3633\n         1  2389  5430\n                                          \n               Accuracy : 0.7586          \n                 95% CI : (0.7532, 0.7639)\n    No Information Rate : 0.6367          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.4623          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.8496          \n            Specificity : 0.5991          \n         Pos Pred Value : 0.7879          \n         Neg Pred Value : 0.6945          \n             Prevalence : 0.6367          \n         Detection Rate : 0.5409          \n   Detection Prevalence : 0.6865          \n      Balanced Accuracy : 0.7244          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nExactitud (Accuracy): La exactitud del modelo es del 75.86%, lo que significa que el modelo clasificó correctamente aproximadamente el 75.86% de las instancias en el conjunto de prueba.\nSensibilidad (Sensitivity o Recall): La sensibilidad del modelo para la clase 0 es del 84.96%, lo que indica que el modelo identificó correctamente alrededor del 84.96% de las instancias de la clase 0 en el conjunto de prueba.\nEspecificidad (Specificity): La especificidad del modelo para la clase 1 es del 59.91%, lo que significa que el modelo identificó correctamente alrededor del 59.91% de las instancias de la clase 1 en el conjunto de prueba.\n\nEn general, estos resultados muestran que el modelo LDA tiene un rendimiento decente en la clasificación de las instancias en el conjunto de prueba, con una exactitud y un kappa significativos.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#análisis-de-discriminante-cuadrático",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/laboral.html#análisis-de-discriminante-cuadrático",
    "title": "Análisis Discriminante: laboral",
    "section": "Análisis de Discriminante Cuadrático",
    "text": "Análisis de Discriminante Cuadrático\nComo se ha comentado previamente:\n\nSe supone que la distribución de los datos es una normal multivariante.\nNo supone que todas las matrices de covarianzas son iguales ( y por tanto el clasificador es cuadrático y no lineal).\nMétodo muy inestable a menos que tengamos muestras muy grandes y de dimensiones bajas.\n\nEs más inestable debido a que al permitir que cada grupo tenga su propia matriz de covarianza, aunque lo que lo hace más flexible, requiere muchos más datos para estimar las matrices de covarianza para cada grupo.\n\nset.seed(785248) # For Reproducibility\nqda.class.laboral &lt;- qda(Estudios ~ Salario + Edad + Vacaciones + Antiguedad, data = laboralTrain)\n\n\nplot &lt;- partimat(Estudios ~ Salario + Edad + Vacaciones + Antiguedad, data = laboralTrain, method = \"qda\")\n\n\n\n\n\n\n\n\n\nset.seed(785248)\n# predict with the qda model and test data\npred.qda &lt;- predict(qda.class.laboral, laboralTest)$class\n\n# confusion matrix\nconfusionMatrix(pred.qda, laboralTest$Estudios)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 12453  2383\n         1  3428  6680\n                                          \n               Accuracy : 0.767           \n                 95% CI : (0.7617, 0.7723)\n    No Information Rate : 0.6367          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5086          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.7841          \n            Specificity : 0.7371          \n         Pos Pred Value : 0.8394          \n         Neg Pred Value : 0.6609          \n             Prevalence : 0.6367          \n         Detection Rate : 0.4992          \n   Detection Prevalence : 0.5948          \n      Balanced Accuracy : 0.7606          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nExactitud (Accuracy): La exactitud del modelo es del 76.7%, lo que significa que el modelo clasificó correctamente aproximadamente el 76.7% de las instancias en el conjunto de prueba.\nSensibilidad (Sensitivity o Recall): La sensibilidad del modelo para la clase 0 es del 78.41%, lo que indica que el modelo identificó correctamente alrededor del 78.41% de las instancias de la clase 0 en el conjunto de prueba.\nEspecificidad (Specificity): La especificidad del modelo para la clase 1 es del 73.71%, lo que significa que el modelo identificó correctamente alrededor del 73.71% de las instancias de la clase 1 en el conjunto de prueba.\n\nComparando estos resultados con los del modelo LDA, observamos que el modelo QDA tiene una mayor exactitud, sensibilidad y valor predictivo negativo. Sin embargo, el modelo LDA tiene una mayor especificidad y valor predictivo positivo. La elección entre LDA y QDA depende del contexto específico del problema y de las preferencias en términos de los errores de clasificación que se desean minimizar. Si se valora más la identificación correcta de la clase minoritaria (clase 1), el modelo QDA podría ser más adecuado debido a su mayor sensibilidad. Si se desea minimizar los falsos positivos, el modelo LDA podría ser preferible debido a su mayor especificidad.",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Análisis Discriminante: laboral"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html",
    "title": "Cluster K-Means: VarCovid",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo K-Means a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado VarCovid. Este contiene datos relativos a las Tasas de Variación de fallecidos en el año 2020 (Año Covid) respecto al año anterior. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Concretamente en este dataset tenemos las siguientes variables:\n\nccaa: Comunidades Autónomas\n2020SM20: Tasa de variación del acumulado hasta la semana 20 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n2020SM53: Tasa de variación del acumulado hasta la semana 53 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n3Ola: Tasa de variación entre el acumulado entre la semana 51 de 2020 y la semana 10 de 2021, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la tercera ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster K-Means\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster K-Means de las ccaa en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#dataset",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#dataset",
    "title": "Cluster K-Means: VarCovid",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado VarCovid. Este contiene datos relativos a las Tasas de Variación de fallecidos en el año 2020 (Año Covid) respecto al año anterior. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Concretamente en este dataset tenemos las siguientes variables:\n\nccaa: Comunidades Autónomas\n2020SM20: Tasa de variación del acumulado hasta la semana 20 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n2020SM53: Tasa de variación del acumulado hasta la semana 53 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n3Ola: Tasa de variación entre el acumulado entre la semana 51 de 2020 y la semana 10 de 2021, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la tercera ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster K-Means",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster K-Means: VarCovid",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster K-Means de las ccaa en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#cargar-librerías",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#cargar-librerías",
    "title": "Cluster K-Means: VarCovid",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(factoextra) # fviz_cluster function",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#lectura-de-datos",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#lectura-de-datos",
    "title": "Cluster K-Means: VarCovid",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../files/VarCovid.xlsx\", sheet = \"Datos\")",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#introducción-1",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#introducción-1",
    "title": "Cluster K-Means: VarCovid",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de clúster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clústeres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nEjemplo ilustrativo: Imaginar que tenemos un conjunto de datos de alumnos de un colegio de educación primaria y para cada uno de ellos disponemos de varias variables como edad, sexo, altura, promedio de notas.. Imaginar que queremos hacer grupos de estudiantes para aplicar programas de estudios. Intuitivamente asociaremos a cada edad un curso, y de manera adicional podemos decir que si tienen una media de notas muy alta se les podría asociar a un curso superior y si es muy baja a uno inferior. Es decir, a partir de determinadas reglas de decisión hemos conseguido clasificar las observaciones en diferentes grupos de datos.\nConcretamente, el Cluster K-Means define clusters de modo que se minimice la variación total dentro del grupo de acuerdo con el algoritmo Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias al cuadrado de las distancias euclidianas entre elementos y el centroide correspondiente. Se describe a continuación.\nLos pasos generales de este algoritmo son:\n\nEspecificar el número de clusters (K) que se se desean obtener.\nSeleccionar aleatoriamente k objetos del conjunto de datos como centros del grupo (centroides). Asigna cada observación a su centroide más cercano, según la distancia entre el objeto y el centroide(es necesario elegir una función de distancia).\nPara cada uno de los k grupos, actualizar el centroide del grupo calculando los nuevos valores medios de todos los puntos de datos del grupo. El centroide de un grupo K-ésimo es un vector de longitud p que contiene las medias de todas las variables para las observaciones en el grupo K-ésimo; p es el número de variables.\nCalcular la distancia entre las observaciones y los nuevos centroides, asignado las observaciones al cluster del centroide más cercano.\nRepetir pasos 3-4 sucesivamente hasta que los centroides no cambien. En ese caso se supone que se ha alcanzado la convergencia y ya se han encontrado unos clusters estables. De forma predeterminada, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones, con el fin de evitar que entre en una secuencia infinita de iteraciones en el caso de no converger nunca.\n\nVéase funciones de R stats::kmeans(x, centers, iter.max, nstart) que realizan los pasos 2-5 automáticamente.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#formulación",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#formulación",
    "title": "Cluster K-Means: VarCovid",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nVer que no hay ningún NA en el dataset.\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\n\nifelse(sum(is.na(data)) == 0, print(\"There is no NA in the dataset.\"), print(\"There is some NA in the dataset.\"))\n\n[1] \"There is no NA in the dataset.\"\n\n\n[1] \"There is no NA in the dataset.\"\n\n\nSi queremos que el código sea reproducible, es necesario fijar semilla (función set.seed(n)) ya que el algoritmo k-means elige los centroides iniciales aleatoriamente.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"1Ola\", \"2Ola\")]\n\nresultado &lt;- scale(resultado) # scaling/standardizing\nrownames(resultado) &lt;- datos$ccaa # Para que nos salgan luego los nombres\ncomunidades &lt;- datos$ccaa\n\n\n# K-MEANS algortihm\nset.seed(785248) # reproducibilidad\nk1 &lt;- kmeans(resultado, centers = 3, nstart = 25)\nk1\n\nK-means clustering with 3 clusters of sizes 5, 12, 3\n\nCluster means:\n         1Ola       2Ola\n1 -0.06792789  1.3528076\n2 -0.45864725 -0.5113092\n3  1.94780216 -0.2094426\n\nClustering vector:\n             Total nacional                   Andalucia \n                          2                           2 \n                     Aragon     Asturias, Principado de \n                          1                           2 \n             Balears, Illes                    Canarias \n                          2                           2 \n                  Cantabria             Castilla y Leon \n                          2                           1 \n       Castilla - La Mancha                    Cataluna \n                          3                           3 \n       Comunitat Valenciana                 Extremadura \n                          2                           2 \n                    Galicia        Madrid, Comunidad de \n                          2                           3 \n          Murcia, Region de Navarra, Comunidad Foral de \n                          2                           2 \n                 Pais Vasco                   Rioja, La \n                          2                           1 \n                      Ceuta                     Melilla \n                          1                           1 \n\nWithin cluster sum of squares by cluster:\n[1] 4.039852 5.699660 1.912043\n (between_SS / total_SS =  69.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nfviz_cluster(k1, data = resultado) # plot\n\n\n\n\n\n\n\n\nPodemos observar que la agrupación en 3 clusters que ha hecho el algoritmo K-MEANS es bastante similar a la que obtuvimos con el cluster jerárquico. Por un lado tenemos un cluster de los valores que se encuentran más a la derecha, luego otro con los que están más arriba y otros con los más cercanos al origen. En cierto modo:\n\nEl cluster azul representa las CCAA donde el exceso de mortalidad respecto al año anterior fue mucho mayor en la primera que en la segunda ola, es decir, hubo más muertes en la primera que en la segunda ola. Notar que en este cluster encontramos las dos comunidades con más población y flujo de visitantes del país, Comunidad de Madrid y Cataluña, luego tiene sentido que fueran las pioneras en tener una tasa alta de muertes. De hecho durante las primeras semanas de virus en España, fueron Castilla-La Mancha y Madrid las que presentaban peores números.\nEl cluster rojo representa las CCAA donde el exceso de mortalidad respecto al año anterior fue mucho mayor en la segunda que en la primera ola, es decir, hubo más muertes en la segunda que en la primera ola. Notar, que a excepción de Ceuta y Melilla, las comunidades que aparecen aquí, son comunidades con gran población residente en núcleos rurales y por ello la propagación del virus tardó en extenderse. Debido a que no tienen grandes ciudades esta propagación inicial fue más lenta y por ello la segunda ola causo más exceso de mortalidad que la primera.\nPor último, el cluster verde presenta comunidades que tuvieron una incidencia parecida en la primera y segunda ola.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#método-elbow",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#método-elbow",
    "title": "Cluster K-Means: VarCovid",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"wss\", print.summary = TRUE)\n\n\n\n\n\n\n\n\nEl número óptimo de k parece ser 3 que es donde más se reduce la pendiente y la variabilidad explicada no parece disminuir de forma tan rápida. De todos modos, también podría parecer razonable tomar el 4 o 5. Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#método-silhouette",
    "href": "notebooks/Cluster K-Means/VarCovid/VarCovid.html#método-silhouette",
    "title": "Cluster K-Means: VarCovid",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 5 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 5 e intentar analizar de nuevo los resultados.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "VarCovid",
      "Cluster K-Means: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo K-Means a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado salario_kmeans.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual en las comunidades autónomas en España junto a Ceuta y Melilla, relativos al año 2018. Los datos corresponden a las operaciones Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios y Cifras Oficiales de Población de los Municipios Españoles (IOE 30245). Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nNombre: Comunidades autónomas.\nPoblacion: Número de habitantes en la respectiva comunidad autónoma\nSalario: Salario medio anual.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Poblacion y Salario. Concretamente usaremos la técnica K-Means.\n\n\n\nEn este notebook se expondrá como realizar un Análisis Cluster empleando la técnica K-Means en función de las variables Poblacion y Salario para agrupar las comunidades autónomas.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan clúster\nEstandarizar datos y probar k-means con k=4.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clústeres y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#dataset",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#dataset",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado salario_kmeans.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual en las comunidades autónomas en España junto a Ceuta y Melilla, relativos al año 2018. Los datos corresponden a las operaciones Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios y Cifras Oficiales de Población de los Municipios Españoles (IOE 30245). Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nNombre: Comunidades autónomas.\nPoblacion: Número de habitantes en la respectiva comunidad autónoma\nSalario: Salario medio anual.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Poblacion y Salario. Concretamente usaremos la técnica K-Means.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "",
    "text": "En este notebook se expondrá como realizar un Análisis Cluster empleando la técnica K-Means en función de las variables Poblacion y Salario para agrupar las comunidades autónomas.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan clúster\nEstandarizar datos y probar k-means con k=4.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clústeres y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#cargar-librerías",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#cargar-librerías",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\nlibrary(factoextra) # fviz_cluster function\nlibrary(gridExtra) # Para el layout de los gráficos",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#lectura-de-datos",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#lectura-de-datos",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/salario_kmeans.xlsx\", sheet = \"Datos\")\n\n\n# Histogram dim1\nhistogram &lt;- ggplot(datos, aes(x = Salario)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Salario\", x = \"Salario\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim1\nboxplot &lt;- ggplot(datos, aes(x = \"d\", y = Salario)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Salario\", x = \"\", y = \"Salario\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n# Histogram dim2\nhistogram2 &lt;- ggplot(datos, aes(x = Poblacion)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Poblacion\", x = \"Poblacion\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim2\nboxplot2 &lt;- ggplot(datos, aes(x = \"d\", y = Poblacion)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Poblacion\", x = \"\", y = \"Poblacion\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\ngrid.arrange(histogram, histogram2, boxplot, boxplot2, nrow = 2, ncol = 2, widths = c(0.3, 0.3))\n\n\n\n\n\n\n\n\n\nDistribución Salarial:\n\nEl rango intercuartílico se extiende aproximadamente de 21.000 a 24.000, lo que indica que la mayoría de las comunidades tienen salarios en este rango.\nNo hay valores atípicos significativos, y los salarios más altos y más bajos están bien dentro de un rango razonable.\n\nDistribución Población:\n\nLa mayoría de las comunidades tienen poblaciones menores a 5.000.000.\nEl rango intercuartílico se extiende aproximadamente de 1.000.000 a 3.000.000.\nExiste un grupo pequeño pero significativo de comunidades con poblaciones muy altas (más de 7.500.000), lo cual podría estar influenciado por comunidades como Madrid o Cataluña.\n\n\nEstos gráficos pueden ayudar a identificar patrones y a entender mejor la estructura demográfica y económica de las comunidades autónomas, facilitando la toma de decisiones políticas y económicas.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#introducción-1",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#introducción-1",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Clúster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clústeres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nEjemplo ilustrativo: Imaginar que tenemos un conjunto de datos de alumnos de un colegio de educación primaria y para cada uno de ellos disponemos de varias variables como edad, sexo, altura, promedio de notas.. Imaginar que queremos hacer grupos de estudiantes para aplicar programas de estudios. Intuitivamente asociaremos a cada edad un curso, y de manera adicional podemos decir que si tienen una media de notas muy alta se les podría asociar a un curso superior y si es muy baja a uno inferior. Es decir, a partir de determinadas reglas de decisión hemos conseguido clasificar las observaciones en diferentes grupos de datos.\nConcretamente, la técnica K-Means define clústeres de modo que se minimice la variación total dentro del grupo de acuerdo con el algoritmo Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias al cuadrado de las distancias euclidianas entre elementos y el centroide correspondiente. Se describe a continuación.\nLos pasos generales de este algoritmo son:\n\nEspecificar el número de clústeres (K) que se se desean obtener.\nSeleccionar aleatoriamente k objetos del conjunto de datos como centros del grupo (centroides). Asigna cada observación a su centroide más cercano, según la distancia entre el objeto y el centroide (es necesario elegir una función de distancia).\nPara cada uno de los k grupos, actualizar el centroide del grupo calculando los nuevos valores medios de todos los puntos de datos del grupo. El centroide de un grupo K-ésimo es un vector de longitud p que contiene las medias de todas las variables para las observaciones en el grupo K-ésimo; p es el número de variables.\nCalcular la distancia entre las observaciones y los nuevos centroides, asignado las observaciones al clúster del centroide más cercano.\nRepetir pasos 3-4 sucesivamente hasta que los centroides no cambien. En ese caso se supone que se ha alcanzado la convergencia y ya se han encontrado unos clústeres estables. De forma predeterminada, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones, con el fin de evitar que entre en una secuencia infinita de iteraciones en el caso de no converger nunca.\n\nVéase funciones de R stats::kmeans(x, centers, iter.max, nstart) que realizan los pasos 2-5 automáticamente.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#método-elbow",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#método-elbow",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clústeres ya no proporciona un beneficio significativo en la varianza explicada (distancia promedio de los elementos al centroide del clúster) o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clústeres, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clústeres, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clústeres, indicando un equilibrio entre una mayor partición (más clústeres) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"wss\", print.summary = TRUE)\n\n\n\n\n\n\n\n\nEste gráfico ayuda a determinar el número óptimo de clústeres para el algoritmo K-means. El método del codo (Elbow Method) consiste en observar el punto donde la reducción en la suma de los cuadrados dentro del clúster (WSS) empieza a disminuir de manera considerable.\n\nEl gráfico muestra un fuerte descenso en WSS hasta 3 ó 4 clústeres, luego se reduce más lentamente.\nEl “codo” más pronunciado parece estar en 3 ó 4 clústeres, luego se podría tomar alguno de estos como óptimo.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#método-silhouette",
    "href": "notebooks/Cluster K-Means/salario_kmeans/salario_kmeans.html#método-silhouette",
    "title": "Cluster K-Means: salario_kmeans",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-clúster (distancia al resto de puntos en su mismo grupo) y la distancia media al clúster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el clúster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clústeres. El número óptimo de clústeres es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-clúster y separación inter-clúster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 2 puesto que es el caso cuyos clústeres maximiza el valor de Silhouette, representando una mejor cohesión intra-clúster y separación inter-clúster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clústeres igual a 2 ó 3 e intentar analizar de nuevo los resultados. Destacar que, en última instancia, el número de clústeres depende del interés del usuario que deberá fijarlo en función de sus objetivos o analizando que número es el óptimo.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Salario Kmeans",
      "Cluster K-Means: salario_kmeans"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo Jerárquico a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado VarCovid. Este contiene datos relativos a las Tasas de Variación de fallecidos en el año 2020 (Año Covid) respecto al año anterior. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Concretamente en este dataset tenemos las siguientes variables:\n\nccaa: Comunidades Autónomas\n2020SM20: Tasa de variación del acumulado hasta la semana 20 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n2020SM53: Tasa de variación del acumulado hasta la semana 53 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n3Ola: Tasa de variación entre el acumulado entre la semana 51 de 2020 y la semana 10 de 2021, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la tercera ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster jerárquico.\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las ccaa en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#dataset",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#dataset",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado VarCovid. Este contiene datos relativos a las Tasas de Variación de fallecidos en el año 2020 (Año Covid) respecto al año anterior. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Concretamente en este dataset tenemos las siguientes variables:\n\nccaa: Comunidades Autónomas\n2020SM20: Tasa de variación del acumulado hasta la semana 20 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n2020SM53: Tasa de variación del acumulado hasta la semana 53 incluida del año 2020 respecto al año anterior en ese mismo periodo.\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n3Ola: Tasa de variación entre el acumulado entre la semana 51 de 2020 y la semana 10 de 2021, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la tercera ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster jerárquico.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las ccaa en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#cargar-librerías",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#cargar-librerías",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dendextend) # Para dendogramas\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\n\nlibrary(factoextra) # fviz_cluster function",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#lectura-de-datos",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#lectura-de-datos",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../files/VarCovid.xlsx\", sheet = \"Datos\")\n\n\nVer que no hay ningún NA en el dataset.\n\n\nifelse(sum(is.na(datos)) == 0, print(\"There is no NA in the dataset.\"), print(\"There is some NA in the dataset.\"))\n\n[1] \"There is no NA in the dataset.\"\n\n\n[1] \"There is no NA in the dataset.\"",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#introducción-1",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#introducción-1",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Cluster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clusteres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nConcretamente, el Cluster Jerárquico realiza estos grupos -o clusters- de manera jerárquica y ascendente, es decir que sucesivamente van fusionando grupos desde el elemento individual (mayor nivel de grupos, uno por individuo) hacia arriba.\nLa representación de la jerarquía de cluster se representa por medio de un dendograma, en el que las sucesivas fusiones de las ramas a los distintos niveles nos informan de las sucesivas fusiones de los grupos en grupos de superior nivel (mayor tamaño, menor homogeneidad) sucesivamente:\nLos pasos concretos del Cluster Jerárquico son:\n\nMatriz de distancia o similitud: Se calcula una matriz que mide la distancia o similitud entre cada par de observaciones. Algunas de las medidas comunes son:\n\nEuclidiana: Mide la distancia más corta entre dos puntos en un espacio euclidiano. Es útil cuando las dimensiones tienen una escala similar y se desea tener en cuenta la magnitud absoluta de las diferencias.\nManhattan (o Cityblock): Calcula la suma de las diferencias absolutas entre las coordenadas de dos puntos. Es útil cuando las dimensiones no están en la misma escala y se quiere una medida robusta a los valores atípicos.\nGower: métrica de distancia utilizada específicamente para conjuntos de datos mixtos que contienen variables numéricas y categóricas. Esta distancia tiene en cuenta diferentes tipos de variables al calcular la similitud entre dos observaciones. Se define como una combinación ponderada de las distancias entre variables.\n\nFusión de clusteres: En el enfoque aglomerativo, se fusionan gradualmente los clusteres más cercanos según la medida de distancia o similitud elegida. Esto nos lleva a la pregunta, ¿Cómo se calcula la distancia entre Clusters calcular la distancia o similitud entre clusteres en el proceso de agrupamiento jerárquico?. Existen varios métodos de enlace, destacando:\n\nEnlace Simple (Single Linkage): Calcula la distancia entre clusteres como la distancia más corta entre cualquier punto de un cluster y cualquier punto del otro cluster. Es sensible a la presencia de valores atípicos y al fenómeno del encadenamiento.\nEnlace Completo (Complete Linkage): Mide la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\nEnlace Promedio (Average Linkage): Calcula la distancia entre clusteres como la media de todas las distancias entre pares de puntos, uno de cada cluster. Más robusto frente a valores atípicos que el enlace simple y menos propenso al encadenamiento que el enlace completo.\nEnlace de Ward: Minimiza la varianza dentro de los clusteres al fusionarlos. Intenta minimizar la suma de cuadrados dentro de cada cluster después de la fusión.\n\nRepresentación jerárquica: Esto resulta en un dendrograma que muestra la jerarquía de agrupamiento, donde la altura en el dendrograma indica la distancia o disimilitud en la que se unen los clusteres.\n\nEl clustering jerárquico permite explorar diferentes niveles de granularidad en los datos, pero puede ser computacionalmente costoso para grandes conjuntos de datos. Es crucial elegir la medida de similitud adecuada y el método de enlace (criterio para unir clusteres, single linkage, complete linkage, average linkage,…) para obtener resultados significativos.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#formulación",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#formulación",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\nNotar que la distancia más apropiada para usar es la Euclidea ya que ambas variables 1Ola y 2Ola son del mismo tipo y corresponden a meses consecutivos, es decir, representan el mismo fenómeno demográfico y en la misma escala (una vez hayamos escalado). Además como estamos interesados en la diferencia de estas variables a la hora de hacer cluster, esta es la distancia más adecuada.\nEn cuanto al método para hacer los clusters, vamos a dejar el que viene por defecto, el complete. Este se basa en medir la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"1Ola\", \"2Ola\")]\n\nresultado &lt;- scale(resultado) # scaling/standardizing\nrownames(resultado) &lt;- datos$ccaa # Para que nos salgan luego los nombres\ncomunidades &lt;- datos$ccaa\n\n# Matriz de distancias\nd &lt;- dist(resultado, method = \"euclidean\")\n\n# Hierarchical clustering using Complete Linkage\nhc1 &lt;- hclust(d, method = \"complete\")\n\n# Plot the obtained dendrogram\nplot(hc1, cex = 0.6, hang = -1)\nabline(h = 100, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nEn el dendrograma mostrado arriba, cada hoja corresponde a una observación. A medida que avanzamos en el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una altura mayor.\nLa altura de la fusión, representada en el eje vertical, indica la (des) similitud entre dos observaciones. Cuanto mayor sea la altura de la fusión, menos similares son las observaciones. Es importante destacar que las conclusiones sobre la proximidad de dos observaciones solo se pueden inferir en función de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.\nLa altura del corte en el dendrograma controla el número de clusters obtenidos. Cumple el mismo papel que ‘k’ en la agrupación k-means. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función cutree. Suponer que queremos 3 clusteres:\n\n# Cut tree into 4 groups\nsub1 &lt;- cutree(hc1, k = 3)\n\n# Number of members in each cluster\ntable(sub1)\n\nsub1\n 1  2  3 \n16  2  2 \n\n\nPodemos mostrar los grupos junto al dataframe con la función mutate del paquete dplyr()\n\n# Mostrar Clusters\ndatos %&gt;%\n  mutate(cluster = sub1) %&gt;%\n  head()\n\n# A tibble: 6 × 7\n  ccaa                    `2020SM20` `2020SM53` `1Ola` `2Ola` `3Ola` cluster\n  &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt;\n1 Total nacional               26.0       20.0   77.0   18.6   25.3        1\n2 Andalucia                     4.49      12.8   16.7   21.9   27.8        1\n3 Aragon                       22.3       24.0   57.5   41.5   21.9        2\n4 Asturias, Principado de       9.84      14.7   25.6   22.1   20.9        1\n5 Balears, Illes                1.88       9.06  16.6   15.2   23.1        1\n6 Canarias                      2.22       5.83   8.44   9.69   6.24       1\n\ncolnames(datos) &lt;- c(\"ccaa\", \"2020SM20\", \"2020SM53\", \"PrimOla\", \"SegOla\", \"TercOla\")\n\n\n\n# Gráfico de puntos\ng1 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = ccaa)) +\n  geom_point(aes(colour = factor(sub1))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub1))) +\n  labs(colour = \"Clusters\")\n\ng1\n\n\n\n\n\n\n\n\nDel anterior gráfico y de los clusters podemos concluir: hay grandes diferencias entre los tres clusters.\n\nEl Cluster Azul corresponde a las comunidades donde la primera ola de COVID generó un gran exceso de mortalidad respecto al año anterior, son los casos de la Comunidad de Madrid y Castilla-La Mancha.\n\nEn cambio, el Cluster Verde corresponde a las comunidades donde se produjo un mayor exceso de fallecidos en la segunda ola, como son los casos de Melilla y Aragón.\nEl Cluster Rojo corresponde a las provincias donde hubo un exceso de fallecidos más parecido entre ambas olas, aunque podemos observar algunas diferencias entre algunas de ellas, por ejemplo, en Cataluña hubo un mayor exceso en la primera y en Ceuta hubo un mayor exceso en la segunda.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#con-otros-métodos-de-enlace",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#con-otros-métodos-de-enlace",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Con otros métodos de Enlace",
    "text": "Con otros métodos de Enlace\nVamos a probar a usar otros métodos de Enlace descritos previamente a ver si seguimos obteniendo los mismos clusters y poder llegar a una conclusión sólida.\n\n# For Complete\nplot(hc1, cex = 0.6, sub = \"\",main=\"complete\")\nrect.hclust(hc1, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Ward.D method\nhc2 &lt;- hclust(d, method = \"ward.D\")\nsub2 &lt;- cutree(hc2, k = 3)\n\nplot(hc2, cex = 0.6, sub = \"\", main = \"ward.D\")\nrect.hclust(hc2, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Average method\nhc3 &lt;- hclust(d, method = \"average\")\nsub3 &lt;- cutree(hc3, k = 3)\n\nplot(hc3, cex = 0.6, sub = \"\", main = \"average\")\nrect.hclust(hc3, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Single method\nhc4 &lt;- hclust(d, method = \"single\")\nsub4 &lt;- cutree(hc4, k = 3)\n\nplot(hc4, cex = 0.6, sub = \"\", main = \"single\")\nrect.hclust(hc4, k = 3, border = 2:5)\n\n\n\n\n\n\n\n\n\ng2 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = ccaa)) +\n  geom_point(aes(colour = factor(sub2))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub2))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng3 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = ccaa)) +\n  geom_point(aes(colour = factor(sub3))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub3))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng4 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = ccaa)) +\n  geom_point(aes(colour = factor(sub4))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub4))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\n\nlibrary(ggpubr)\nggarrange(g1, g2, g3, g4 + rremove(\"x.text\"),\n  labels = c(\"Complete\", \"ward.D\", \"Average\", \"Simple\"),\n  ncol = 2, nrow = 2, vjust = 0.9,\n  font.label = list(size = 8, color = \"black\")\n)\n\n\n\n\n\n\n\n\nDe 4 tipos diferentes que hemos probado, nos salen 3 que se han clusterizado de igual manera, luego parece razonable la interpretación anterior. Para todos casos menos el de ward, el Cluster Azul corresponde a las comunidades donde la primera ola de COVID generó un gran exceso de mortalidad respecto al año anterior, son los casos de la Comunidad de Madrid y Castilla-La Mancha. En cambio, el Cluster Verde corresponde a las comunidades donde se produjo un mayor exceso de fallecidos en la segunda ola, como son los casos de Melilla y Aragón. El Cluster Rojo corresponde a las provincias donde hubo un exceso de fallecidos más parecido entre ambas olas, aunque podemos observar algunas diferencias entre algunas de ellas, por ejemplo, en Cataluña hubo un mayor exceso en la primera y en Ceuta hubo un mayor exceso en la segunda. Resumiendo:\n\nEl cluster azul representa las CCAA donde el exceso de mortalidad respecto al año anterior fue mucho mayor en la primera que en la segunda ola, es decir, hubo más muertes en la primera que en la segunda ola. Notar que en este cluster encontramos la comunidad con más población y flujo de visitantes del país, Comunidad de Madrid , luego tiene sentido que fuera la pionera en tener una tasa alta de muertes. Además, durante las primeras semanas de virus en España, fueron Castilla-La Mancha y Madrid las que presentaban peores números.\nEl cluster verde representa las CCAA donde el exceso de mortalidad respecto al año anterior fue mucho mayor en la segunda que en la primera ola, es decir, hubo más muertes en la segunda que en la primera ola. Notar, que a excepción de Melilla, Aragón es una comunidad con gran población residente en núcleos rurales y por ello la propagación del virus tardó en extenderse. Debido a que no tienen grandes ciudades esta propagación inicial fue más lenta y por ello la segunda ola causo más exceso de mortalidad que la primera.\nPor último, el cluster rojo presenta comunidades que tuvieron una incidencia parecida en la primera y segunda ola.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#método-elbow",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#método-elbow",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"wss\")\n\n\n\n\n\n\n\n\nEl número óptimo de k parece ser 3 que es donde más se reduce la pendiente y la variabilidad explicada no parece disminuir de forma tan rápida. De todos modos, también podría parecer razonable tomar el 4. Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#método-silhouette",
    "href": "notebooks/Cluster Jerarquico/VarCovid/VarCovid.html#método-silhouette",
    "title": "Cluster Jerárquico: VarCovid",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 3 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 5 e intentar analizar de nuevo los resultados.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid",
      "Cluster Jerárquico: VarCovid"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo Jerárquico a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado ecv_cluster.xlsx. Este contiene datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021. Las variables de interés son las siguientes:\n\nCCAA: Comunidades Autónomas\ntaspobex: Tasa de riesgo de pobreza o exclusión social (indicador AROPE).\ntaspob: Tasa en riesgo de pobreza (renta año anterior a la entrevista).\ntascar: Tasa personas con carencia material severa.\ntasvivtrab: Tasa de hogares viviendo con baja intensidad en el trabajo (de 0 a 59 años).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables definidas arriba. Concretamente usaremos un cluster jerárquico.\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables taspobex y tascar .\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (taspobex, tascar).\nEstandarizar datos y probar cluster jerárquico con k=3.\nElegir Función Distancia y Método de Enlace (o comparar varias).\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#dataset",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#dataset",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado ecv_cluster.xlsx. Este contiene datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021. Las variables de interés son las siguientes:\n\nCCAA: Comunidades Autónomas\ntaspobex: Tasa de riesgo de pobreza o exclusión social (indicador AROPE).\ntaspob: Tasa en riesgo de pobreza (renta año anterior a la entrevista).\ntascar: Tasa personas con carencia material severa.\ntasvivtrab: Tasa de hogares viviendo con baja intensidad en el trabajo (de 0 a 59 años).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables definidas arriba. Concretamente usaremos un cluster jerárquico.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables taspobex y tascar .\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (taspobex, tascar).\nEstandarizar datos y probar cluster jerárquico con k=3.\nElegir Función Distancia y Método de Enlace (o comparar varias).\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#cargar-librerías",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#cargar-librerías",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dendextend) # Para dendogramas\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\nlibrary(factoextra) # fviz_cluster function\nlibrary(gridExtra) # layout de los gráficos",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#lectura-de-datos",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#lectura-de-datos",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/ecv_cluster.xlsx\", sheet = \"Datos\")",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#introducción-1",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#introducción-1",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Cluster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clusteres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nConcretamente, el Cluster Jerárquico realiza estos grupos -o clusters- de manera jerárquica y ascendente, es decir que sucesivamente van fusionando grupos desde el elemento individual (mayor nivel de grupos, uno por individuo) hacia arriba.\nLa representación de la jerarquía de cluster se representa por medio de un dendograma, en el que las sucesivas fusiones de las ramas a los distintos niveles nos informan de las sucesivas fusiones de los grupos en grupos de superior nivel (mayor tamaño, menor homogeneidad) sucesivamente:\nLos pasos concretos del Cluster Jerárquico son:\n\nMatriz de distancia o similitud: Se calcula una matriz que mide la distancia o similitud entre cada par de observaciones. Algunas de las medidas comunes son:\n\nEuclidiana: Mide la distancia más corta entre dos puntos en un espacio euclidiano. Es útil cuando las dimensiones tienen una escala similar y se desea tener en cuenta la magnitud absoluta de las diferencias.\nManhattan (o Cityblock): Calcula la suma de las diferencias absolutas entre las coordenadas de dos puntos. Es útil cuando las dimensiones no están en la misma escala y se quiere una medida robusta a los valores atípicos.\nGower: métrica de distancia utilizada específicamente para conjuntos de datos mixtos que contienen variables numéricas y categóricas. Esta distancia tiene en cuenta diferentes tipos de variables al calcular la similitud entre dos observaciones. Se define como una combinación ponderada de las distancias entre variables.\n\nFusión de clusteres: En el enfoque aglomerativo, se fusionan gradualmente los clusteres más cercanos según la medida de distancia o similitud elegida. Esto nos lleva a la pregunta, ¿Cómo se calcula la distancia entre Clusters calcular la distancia o similitud entre clusteres en el proceso de agrupamiento jerárquico?. Existen varios métodos de enlace, destacando:\n\nEnlace Simple (Single Linkage): Calcula la distancia entre clusteres como la distancia más corta entre cualquier punto de un cluster y cualquier punto del otro cluster. Es sensible a la presencia de valores atípicos y al fenómeno del encadenamiento.\nEnlace Completo (Complete Linkage): Mide la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\nEnlace Promedio (Average Linkage): Calcula la distancia entre clusteres como la media de todas las distancias entre pares de puntos, uno de cada cluster. Más robusto frente a valores atípicos que el enlace simple y menos propenso al encadenamiento que el enlace completo.\nEnlace de Ward: Minimiza la varianza dentro de los clusteres al fusionarlos. Intenta minimizar la suma de cuadrados dentro de cada cluster después de la fusión.\n\nRepresentación jerárquica: Esto resulta en un dendrograma que muestra la jerarquía de agrupamiento, donde la altura en el dendrograma indica la distancia o disimilitud en la que se unen los clusteres.\n\nEl clustering jerárquico permite explorar diferentes niveles de granularidad en los datos, pero puede ser computacionalmente costoso para grandes conjuntos de datos. Es crucial elegir la medida de similitud adecuada y el método de enlace (criterio para unir clusteres, single linkage, complete linkage, average linkage,…) para obtener resultados significativos.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#formulación",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#formulación",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\nNotar que la distancia más apropiada para usar es la Euclidea ya que ambas variables son del mismo tipo, tasas, y representan el mismo fenómeno demográfico y en la misma escala (una vez hayamos escalado). En cuanto al método para hacer los clusters, vamos a dejar el que viene por defecto, el complete. Este se basa en medir la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"taspobex\", \"tascar\")]\n\nresultado &lt;- scale(resultado) # scaling/standardizing\nrownames(resultado) &lt;- datos$CCAA # Para que nos salgan luego los nombres\n\n# Matriz de distancias\nd &lt;- dist(resultado, method = \"euclidean\")\n\n# Hierarchical clustering using Complete Linkage\nhc1 &lt;- hclust(d, method = \"complete\")\n\n# Plot the obtained dendrogram\nplot(hc1, cex = 0.6, hang = -1)\nabline(h = 2, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nEn el dendrograma mostrado arriba, cada hoja corresponde a una observación. A medida que avanzamos en el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una altura mayor.\nLa altura de la fusión, representada en el eje vertical, indica la (des) similitud entre dos observaciones. Cuanto mayor sea la altura de la fusión, menos similares son las observaciones. Es importante destacar que las conclusiones sobre la proximidad de dos observaciones solo se pueden inferir en función de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.\nLa altura del corte en el dendrograma controla el número de clusters obtenidos. Cumple el mismo papel que ‘k’ en la agrupación k-means. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función cutree. Suponer que queremos 3 clusteres:\n\n# Cut tree into 3 groups\nsub1 &lt;- cutree(hc1, k = 3)\n\n# Number of members in each cluster\ntable(sub1)\n\nsub1\n 1  2  3 \n 3 13  3 \n\n\nPodemos añadir los clusters junto al dataframe con la función dplyr::mutate. El gráfico de puntos dibujando los clusters por colores vamos a dibujarlo para los valores originales sin escalar (ya que el escalado sólo lo hemos hecho a la hora de definir los clusters).\n\n# Mostrar Clusters\ndatos %&gt;%\n  mutate(cluster = sub1) %&gt;%\n  select(\"CCAA\", \"taspobex\", \"tascar\", \"cluster\")\n\n# A tibble: 19 × 4\n   CCAA                        taspobex tascar cluster\n   &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt;\n 1 Andalucía                       38.4   10.2       1\n 2 Aragón                          20.3    5.6       2\n 3 Asturias, Principado de         26.6    5.5       2\n 4 Balears, Illes                  24.5    8.5       2\n 5 Canarias                        38.3   13.5       3\n 6 Cantabria                       21.6    5.7       2\n 7 Castilla y León                 22.4    3.8       2\n 8 Castilla - La Mancha            31.4    5.1       2\n 9 Cataluña                        22.1    7.3       2\n10 Comunitat Valenciana            30.3    7.1       2\n11 Extremadura                     39.1    6.9       1\n12 Galicia                         24.5    3.8       2\n13 Madrid, Comunidad de            21.1    6         2\n14 Murcia, Región de               34.7    9.1       1\n15 Navarra, Comunidad Foral de     16.6    5.5       2\n16 País Vasco                      15.9    5.2       2\n17 Rioja, La                       20.1    3.8       2\n18 Ceuta                           42.4   21.4       3\n19 Melilla                         38.1   17.2       3\n\n\n\n# Graficamos datos diferenciando clusters\ng1 &lt;- ggplot(datos, aes(taspobex, tascar, label = CCAA)) +\n  geom_point(aes(colour = factor(sub1))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub1))) +\n  labs(colour = \"Clusters\")\n\ng1\n\n\n\n\n\n\n\n\nSe puede destacar lo siguiente:\n\nPor un lado, en azul las CCAA que presentan una tasa de riesgo de pobreza y una tasa de personas con carencia material severa muy alta. Estas son Canarias, Ceuta y Melilla, lo cual cabría pensar que tiene sentido ya que ambas tres regiones contienen fuertes corrientes migratorias debido a que se encuentran en puntos fronterizos en el que hay mucha inmigración ilegal procedente de países como Marruecos. Esta población es más propensa a prior a sufrir pobreza y carencia material debido a su situación precaria e irregular.\nEn rojo se encuentran las CCAA que presentan una tasa de riesgo de pobreza similares a las anteriores pero la de personas con carencia material alta no es tan grande como en las citadas anteriormente. Estas son Andalucía, Murcia y Extremadura, que efectivamente es común que aparezcan en la prensa anualmente como regiones con más pobreza dentro de España (a excepción de Murica) y sin embargo presentan una tasa de personas con carencia material no tan alta como las anteriores puesto que no hay tanta población en situación irregular que pueda derivar en una carencia material sustancial.\nPor último, en verde encontramos el resto de CCAA que tienen valores más razonables para ambos indicadores.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#método-elbow",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#método-elbow",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"wss\")\n\n\n\n\n\n\n\n\nEn el gráfico proporcionado, buscamos el “codo” del gráfico, que es el punto donde la suma total de cuadrados dentro de los clusters (total within-cluster sum of squares) deja de decrecer significativamente con la adición de más clusters.\nObservando el gráfico, el punto de inflexión más claro se encuentra en k = 2. Aquí es donde la disminución en la suma de cuadrados empieza a ser menos pronunciada en comparación con la disminución entre 1 y 2 clusters. De todos modos, también podría parecer razonable tomar el 3. Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#método-silhouette",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#método-silhouette",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 2 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster. No obstante, k=3 también presenta valores muy altos luego también podría ser una buena elección.\nNOTA: Ahora vamos repetir el estudio anterior con el número de clusters igual a 2 e intentar analizar de nuevo los resultados.\nVamos a utilizar en todos casos la distancia euclídea como ya se ha razonado antes y vamos a usar diferentes métodos de Fusión de Clusters para ver si obtenemos resultados significativos.\n\n# For Complete method\n\nhc1 &lt;- hclust(d, method = \"complete\")\nsub1 &lt;- cutree(hc1, k = 2)\n\n\n# Ward.D method\nhc2 &lt;- hclust(d, method = \"ward.D\")\nsub2 &lt;- cutree(hc2, k = 2)\n\n\n\n\n# Average method\nhc3 &lt;- hclust(d, method = \"average\")\nsub3 &lt;- cutree(hc3, k = 2)\n\n\n# Single method\nhc4 &lt;- hclust(d, method = \"single\")\nsub4 &lt;- cutree(hc4, k = 2)\n\n\n\n\n\n\n\n\n\n\nNotar que en todos casos hemos obtenido (a excepción de con simple) distribuciones similares en los clusters, que viene siendo lo mismo que en el caso de k=3 clusters pero agrupando los dos primeros clusters que habíamos visto. Es por ello que los resultados parecen coherentes.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#footnotes",
    "href": "notebooks/Cluster Jerarquico/ecv_cluster/ecv_cluster.html#footnotes",
    "title": "Cluster Jerárquico: ecv_cluster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEDA viene del Inglés Exploratory Data Analysis y son los pasos relativos en los que se exploran las variables para tener una idea de que forma toma el dataset.↩︎",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Ecv Cluster",
      "Cluster Jerárquico: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo Jerárquico a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado desigualdad_CCAA.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18. Los datos (relativos a las variables salario medio mujeres, hombres que nos interesan) han sido extraídos de la Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios. Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nCCAA: Comunidades Autónomas.\nSalmedmuj: Salario medio anual (mujeres).\nSalmedhom: Salario medio anual (hombres).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Salmedmuj y Salmedhom. Concretamente usaremos un cluster jerárquico.\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables Salmedmuj y Salmedhom para agrupar las comunidades por las diferencias de salarios entre sexos.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (Salmedmuj, Salmedhom).\nEstandarizar datos y probar cluster jerárquico con k=4.\nElegir Función Distancia y Método de Enlace (o comparar varias).\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#dataset",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#dataset",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado desigualdad_CCAA.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18. Los datos (relativos a las variables salario medio mujeres, hombres que nos interesan) han sido extraídos de la Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios. Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nCCAA: Comunidades Autónomas.\nSalmedmuj: Salario medio anual (mujeres).\nSalmedhom: Salario medio anual (hombres).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Salmedmuj y Salmedhom. Concretamente usaremos un cluster jerárquico.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables Salmedmuj y Salmedhom para agrupar las comunidades por las diferencias de salarios entre sexos.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (Salmedmuj, Salmedhom).\nEstandarizar datos y probar cluster jerárquico con k=4.\nElegir Función Distancia y Método de Enlace (o comparar varias).\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#cargar-librerías",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#cargar-librerías",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dendextend) # Para dendogramas\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\nlibrary(factoextra) # fviz_cluster function\nlibrary(gridExtra) # layout de los gráficos",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#lectura-de-datos",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#lectura-de-datos",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/desigualdad_ccaa.xlsx\", sheet = \"Datos\")\n\n\n# Histogram dim1\nhistogram &lt;- ggplot(datos, aes(x = Salmedmuj)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Salmedmuj\", x = \"Salmedmuj\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim1\nboxplot &lt;- ggplot(datos, aes(x = \"d\", y = Salmedmuj)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Salmedmuj\", x = \"\", y = \"Salmedmuj\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n# Histogram dim2\nhistogram2 &lt;- ggplot(datos, aes(x = Salmedhom)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Salmedhom\", x = \"Salmedhom\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim2\nboxplot2 &lt;- ggplot(datos, aes(x = \"d\", y = Salmedhom)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Salmedhom\", x = \"\", y = \"Salmedhom\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\ngrid.arrange(histogram, histogram2, boxplot, boxplot2, nrow = 2, ncol = 2, widths = c(0.3, 0.3))\n\n\n\n\n\n\n\n\n\nDistribución Salarial:\n\nLos salarios medios de las mujeres muestran una mayor dispersión y variabilidad en comparación con los de los hombres. Esto se evidencia por la presencia de un valor atípico en los salarios de las mujeres.\nLos salarios medios de los hombres están más concentrados y muestran menos variabilidad, sin valores atípicos significativos.\n\nMedianas Salariales:\n\nLa mediana del salario medio de los hombres (26,000) es más alta que la de las mujeres (20,000), lo que podría indicar una brecha salarial entre hombres y mujeres en la muestra de datos.\n\n\nEstos gráficos proporcionan una clara representación visual de las diferencias en la distribución de los salarios medios entre hombres y mujeres, lo que puede ser útil para análisis posteriores y toma de decisiones.\n\nVer que no hay ningún NA en el dataset.\n\n\nifelse(sum(is.na(data)) == 0, print(\"There is no NA in the dataset.\"), print(\"There is some NA in the dataset.\"))\n\n[1] \"There is no NA in the dataset.\"\n\n\n[1] \"There is no NA in the dataset.\"",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#introducción-1",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#introducción-1",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Cluster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clusteres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nConcretamente, el Cluster Jerárquico realiza estos grupos -o clusters- de manera jerárquica y ascendente, es decir que sucesivamente van fusionando grupos desde el elemento individual (mayor nivel de grupos, uno por individuo) hacia arriba.\nLa representación de la jerarquía de cluster se representa por medio de un dendograma, en el que las sucesivas fusiones de las ramas a los distintos niveles nos informan de las sucesivas fusiones de los grupos en grupos de superior nivel (mayor tamaño, menor homogeneidad) sucesivamente:\nLos pasos concretos del Cluster Jerárquico son:\n\nMatriz de distancia o similitud: Se calcula una matriz que mide la distancia o similitud entre cada par de observaciones. Algunas de las medidas comunes son:\n\nEuclidiana: Mide la distancia más corta entre dos puntos en un espacio euclidiano. Es útil cuando las dimensiones tienen una escala similar y se desea tener en cuenta la magnitud absoluta de las diferencias.\nManhattan (o Cityblock): Calcula la suma de las diferencias absolutas entre las coordenadas de dos puntos. Es útil cuando las dimensiones no están en la misma escala y se quiere una medida robusta a los valores atípicos.\nGower: métrica de distancia utilizada específicamente para conjuntos de datos mixtos que contienen variables numéricas y categóricas. Esta distancia tiene en cuenta diferentes tipos de variables al calcular la similitud entre dos observaciones. Se define como una combinación ponderada de las distancias entre variables.\n\nFusión de clusteres: En el enfoque aglomerativo, se fusionan gradualmente los clusteres más cercanos según la medida de distancia o similitud elegida. Esto nos lleva a la pregunta, ¿Cómo se calcula la distancia entre Clusters calcular la distancia o similitud entre clusteres en el proceso de agrupamiento jerárquico?. Existen varios métodos de enlace, destacando:\n\nEnlace Simple (Single Linkage): Calcula la distancia entre clusteres como la distancia más corta entre cualquier punto de un cluster y cualquier punto del otro cluster. Es sensible a la presencia de valores atípicos y al fenómeno del encadenamiento.\nEnlace Completo (Complete Linkage): Mide la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\nEnlace Promedio (Average Linkage): Calcula la distancia entre clusteres como la media de todas las distancias entre pares de puntos, uno de cada cluster. Más robusto frente a valores atípicos que el enlace simple y menos propenso al encadenamiento que el enlace completo.\nEnlace de Ward: Minimiza la varianza dentro de los clusteres al fusionarlos. Intenta minimizar la suma de cuadrados dentro de cada cluster después de la fusión.\n\nRepresentación jerárquica: Esto resulta en un dendrograma que muestra la jerarquía de agrupamiento, donde la altura en el dendrograma indica la distancia o disimilitud en la que se unen los clusteres.\n\nEl clustering jerárquico permite explorar diferentes niveles de granularidad en los datos, pero puede ser computacionalmente costoso para grandes conjuntos de datos. Es crucial elegir la medida de similitud adecuada y el método de enlace (criterio para unir clusteres, single linkage, complete linkage, average linkage,…) para obtener resultados significativos.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#formulación",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#formulación",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\nNotar que la distancia más apropiada para usar es la Euclidea ya que ambas variables saldehom y saldemuj son del mismo tipo, es decir, representan el mismo fenómeno y en la misma escala (una vez hayamos escalado). Además como estamos interesados en la diferencia de estas variables a la hora de hacer cluster, esta es la distancia más a\nEn cuanto al método para hacer los clusters, vamos a dejar el que viene por defecto, el complete. Este se basa en medir la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"Salmedmuj\", \"Salmedhom\")]\n\nrownames(resultado) &lt;- datos$CCAA # Para que nos salgan luego los nombres\n\n# Matriz de distancias euclidea\nd &lt;- dist(resultado, method = \"euclidean\")\n\n# Hierarchical clustering using Complete Linkage\nhc1 &lt;- hclust(d, method = \"complete\")\n\n# Plot the obtained dendrogram\nplot(hc1, cex = 0.6, hang = -1)\nabline(h = 1500, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nEn el dendrograma mostrado arriba, cada hoja corresponde a una observación. A medida que avanzamos en el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una altura mayor.\nLa altura de la fusión, representada en el eje vertical, indica la (des)similitud entre dos observaciones. Cuanto mayor sea la altura de la fusión, menos similares son las observaciones. Es importante destacar que las conclusiones sobre la proximidad de dos observaciones solo se pueden inferir en función de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.\nLa altura del corte en el dendrograma controla el número de clusters obtenidos. Cumple el mismo papel que ‘k’ en la agrupación k-means. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función cutree. Suponer que queremos 4 clusteres:\n\n# Cut tree into 4 groups\nsub1 &lt;- cutree(hc1, k = 4)\n\n# Number of members in each cluster\ntable(sub1)\n\nsub1\n1 2 3 4 \n9 2 2 4 \n\n\nPodemos mostrar los grupos junto al dataframe con la función mutate del paquete dplyr.\n\n# Mostrar Clusters\ndatos &lt;- datos %&gt;%\n  mutate(cluster = sub1) %&gt;%\n  select(\"CCAA\", \"Salmedmuj\", \"Salmedhom\")\n\n\n\n# Gráfico de puntos\ng1 &lt;- ggplot(datos, aes(Salmedmuj, Salmedhom, label = CCAA)) +\n  geom_point(aes(colour = factor(sub1))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub1))) +\n  labs(colour = \"Clusters\")\n\ng1\n\n\n\n\n\n\n\n\nLos clusters se forman de manera clara, con diferencias notables en los salarios medios de hombres y mujeres entre ellos.\n\nCluster 1 (Rojo): Representa comunidades autónomas con salarios medios anuales bajos tanto para hombres como para mujeres. Este grupo incluye a comunidades como Andalucía, Murcia y Castilla y León. Estas tienen salarios medios bajos en comparación con otros clusters, pero hay una menor disparidad entre los salarios de hombres y mujeres en comparación con otros clusters.\nCluster 2 (Verde): Incluye comunidades con salarios medios algo superiores, tanto para hombres como para mujeres. Ejemplos incluyen Aragón y Asturias. Estas presentan salarios medios ligeramente más altos que las del cluster 1, pero aún no alcanzan los niveles de los clusters 3 y 4. Hay una mayor equidad salarial entre hombres y mujeres.\nCluster 3 (Azul): Comunidades con los salarios medios más bajos, especialmente destacable en Extremadura y Canarias. La brecha salarial en estas regiones también parece ser más amplia.\nCluster 4 (Morado): Agrupa a las comunidades con los salarios medios más altos para ambos géneros. Este cluster incluye a comunidades como Madrid, País Vasco y Navarra. Se observa una mayor disparidad salarial entre hombres y mujeres en estas regiones, siendo el País Vasco la comunidad con los salarios más altos tanto para hombres como para mujeres.\n\nImplicaciones\n\nEquidad Salarial: El análisis sugiere que las regiones con salarios más altos también tienden a tener una mayor disparidad salarial entre hombres y mujeres, siendo generalmente las mujeres las que presentan en media salarios más bajos. Las políticas de equidad salarial podrían enfocarse en estas regiones y hacer un estudio más profundo para saber si dicha diferencia es estructural o puede ser debida a causas externas.\nDesarrollo Regional: Las regiones en los clusters 1 y 3 podrían beneficiarse de políticas de desarrollo económico y de incremento salarial, para mejorar el bienestar económico de sus habitantes.\nFocalización de Políticas: Las estrategias y políticas pueden ser diseñadas específicamente para cada cluster, atendiendo a sus características y necesidades particulares en términos de salario medio y equidad salarial.\n\nEste análisis proporciona una visión clara de cómo varían los salarios medios de hombres y mujeres entre las diferentes comunidades autónomas y permite identificar áreas específicas que requieren atención para mejorar la equidad y el desarrollo económico.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#con-otros-métodos-de-enlace",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#con-otros-métodos-de-enlace",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Con otros métodos de Enlace",
    "text": "Con otros métodos de Enlace\nVamos a probar a usar otros métodos de Enlace descritos previamente a ver si seguimos obteniendo los mismos clusters y poder llegar a una conclusión sólida.\n\n# For Complete\nplot(hc1, cex = 0.6, sub = \"\", main = \"complete\")\nrect.hclust(hc1, k = 4, border = 2:5)\n\n\n\n\n\n\n\n# Ward.D method\nhc2 &lt;- hclust(d, method = \"ward.D\")\nsub2 &lt;- cutree(hc2, k = 4)\n\nplot(hc2, cex = 0.6, sub = \"\", main = \"ward.D\")\nrect.hclust(hc2, k = 4, border = 2:5)\n\n\n\n\n\n\n\n# Average method\nhc3 &lt;- hclust(d, method = \"average\")\nsub3 &lt;- cutree(hc3, k = 4)\n\nplot(hc3, cex = 0.6, sub = \"\", main = \"average\")\nrect.hclust(hc3, k = 4, border = 2:5)\n\n\n\n\n\n\n\n# Single method\nhc4 &lt;- hclust(d, method = \"single\")\nsub4 &lt;- cutree(hc4, k = 4)\n\nplot(hc4, cex = 0.6, sub = \"\", main = \"single\")\nrect.hclust(hc4, k = 4, border = 2:5)\n\n\n\n\n\n\n\n\n\ng2 &lt;- ggplot(datos, aes(Salmedhom, Salmedmuj, label = CCAA)) +\n  geom_point(aes(colour = factor(sub2))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub2))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng3 &lt;- ggplot(datos, aes(Salmedhom, Salmedmuj, label = CCAA)) +\n  geom_point(aes(colour = factor(sub3))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub3))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng4 &lt;- ggplot(datos, aes(Salmedhom, Salmedmuj, label = CCAA)) +\n  geom_point(aes(colour = factor(sub4))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub4))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\n\nlibrary(ggpubr)\nggarrange(g1, g2, g3, g4 + rremove(\"x.text\"),\n  labels = c(\"Complete\", \"ward.D\", \"Average\", \"Single\"),\n  ncol = 2, nrow = 2, vjust = 0.9,\n  font.label = list(size = 8, color = \"black\")\n)\n\n\n\n\n\n\n\n\nEl gráfico muestra cuatro agrupaciones de clusters diferentes generadas utilizando distintos métodos de distancia: Complete, Ward.D, Average, y Simple. A continuación, se comparan y analizan los resultados para evaluar su coherencia.\n\nCoherencia General:\n\nEn general, los resultados muestran una tendencia coherente en la agrupación de comunidades con salarios similares, especialmente destacando las comunidades con salarios más altos (País Vasco, Madrid, Navarra) que consistentemente forman un cluster separado.\nLas comunidades con salarios más bajos también tienden a agruparse de manera coherente, aunque la inclusión de algunas comunidades en clusters específicos varía según el método.\n\nDiferencias Específicas:\n\nComplete y Average: Tienden a formar clusters más compactos y visualmente más claros.\nWard.D: Ofrece la separación más clara y significativa, con una estructura de clusters bien definida.\nSingle: Produce clusters menos compactos y puede mezclar comunidades que no parecen intuitivamente similares en términos de salario.\n\n\nLos resultados son en general coherentes, pero el método de Ward.D parece ofrecer la estructura de clusters más clara y significativa, con una mejor separación de las comunidades autónomas según sus salarios medios. Complete y Average también proporcionan agrupaciones razonables, aunque no tan claras como Ward.D. Single, aunque útil en ciertos contextos, no parece tan adecuado para este análisis debido a su tendencia a crear clusters menos intuitivos.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#método-elbow",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#método-elbow",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada (distancia promedio de los elementos al centroide del cluster) o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"wss\")\n\n\n\n\n\n\n\n\nEn el gráfico proporcionado, buscamos el “codo” del gráfico, que es el punto donde la suma total de cuadrados dentro de los clusters (total within-cluster sum of squares) deja de decrecer significativamente con la adición de más clusters.\nObservando el gráfico, el punto de inflexión más claro se encuentra en k = 2. Aquí es donde la disminución en la suma de cuadrados empieza a ser menos pronunciada en comparación con la disminución entre 1 y 2 clusters. De todos modos, también podría parecer razonable tomar el 3. Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#método-silhouette",
    "href": "notebooks/Cluster Jerarquico/desigualdad_ccaa/desigualdad_ccaa.html#método-silhouette",
    "title": "Cluster Jerárquico: desigualdad_CCAA",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos afirma que el número óptimo es 2 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 2 e intentar analizar de nuevo los resultados.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "Desigualdad Ccaa",
      "Cluster Jerárquico: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado VarCovid_provincias.xlsx, elaborado a partir de los datos publicados por el INE. Este dataset está compuesto por el número de fallecidos durante las semanas que duraron las respectivas olas, así como un promedio entre los años 2017, 2018 y 2019 del número de fallecidos en esas mismas semanas y la tasa de variación entre ambas para ver el exceso de muertes que se ha producido. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Esta última variable será la que nosotros utilicemos en nuestro estudio.\n\nprovincia: provincia\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de provincias en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster jerárquico.\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las provincias en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#dataset",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#dataset",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado VarCovid_provincias.xlsx, elaborado a partir de los datos publicados por el INE. Este dataset está compuesto por el número de fallecidos durante las semanas que duraron las respectivas olas, así como un promedio entre los años 2017, 2018 y 2019 del número de fallecidos en esas mismas semanas y la tasa de variación entre ambas para ver el exceso de muertes que se ha producido. Los datos han sido extraídos de la Operación 30324 Estimación de Defunciones Semanales (EDeS), que se encuentra dentro de la temática Salud (Sociedad). Esta última variable será la que nosotros utilicemos en nuestro estudio.\n\nprovincia: provincia\n1Ola: Tasa de variación entre el acumulado entre la semana 11 de 2020 y la semana 18, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la primera ola\n2Ola: Tasa de variación entre el acumulado entre la semana 32 de 2020 y la semana 49, ambas incluidas, respecto a las mismas semanas del año anterior. Tiempo correspondiente a la segunda ola.\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de provincias en función de las variables 1Ola y 2Ola. Concretamente usaremos un cluster jerárquico.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las provincias en función a las variables 1Ola y 2Ola.\n\nHacer un análisis exploratorio.\nVer si hay NA’s y si es necesario escalar los datos.\nPlantear variables sobre las que se van a hacer los cluster.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#cargar-librerías",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#cargar-librerías",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dendextend) # Para dendogramas\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\n\nlibrary(factoextra) # fviz_cluster function",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#lectura-de-datos",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#lectura-de-datos",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../files/VarCovid_provincias.xlsx\", sheet = \"Datos\")\n\n\nVer que no hay ningún NA en el dataset.\n\n\nifelse(sum(is.na(datos)) == 0, print(\"There is no NA in the dataset.\"), print(\"There is some NA in the dataset.\"))\n\n[1] \"There is no NA in the dataset.\"\n\n\n[1] \"There is no NA in the dataset.\"",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#introducción-1",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#introducción-1",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Cluster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clusteres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nConcretamente, el Cluster Jerárquico realiza estos grupos -o clusters- de manera jerárquica y ascendente, es decir que sucesivamente van fusionando grupos desde el elemento individual (mayor nivel de grupos, uno por individuo) hacia arriba.\nLa representación de la jerarquía de cluster se representa por medio de un dendograma, en el que las sucesivas fusiones de las ramas a los distintos niveles nos informan de las sucesivas fusiones de los grupos en grupos de superior nivel (mayor tamaño, menor homogeneidad) sucesivamente:\nLos pasos concretos del Cluster Jerárquico son:\n\nMatriz de distancia o similitud: Se calcula una matriz que mide la distancia o similitud entre cada par de observaciones. Algunas de las medidas comunes son:\n\nEuclidiana: Mide la distancia más corta entre dos puntos en un espacio euclidiano. Es útil cuando las dimensiones tienen una escala similar y se desea tener en cuenta la magnitud absoluta de las diferencias.\nManhattan (o Cityblock): Calcula la suma de las diferencias absolutas entre las coordenadas de dos puntos. Es útil cuando las dimensiones no están en la misma escala y se quiere una medida robusta a los valores atípicos.\nGower: métrica de distancia utilizada específicamente para conjuntos de datos mixtos que contienen variables numéricas y categóricas. Esta distancia tiene en cuenta diferentes tipos de variables al calcular la similitud entre dos observaciones. Se define como una combinación ponderada de las distancias entre variables.\n\nFusión de clusteres: En el enfoque aglomerativo, se fusionan gradualmente los clusteres más cercanos según la medida de distancia o similitud elegida. Esto nos lleva a la pregunta, ¿Cómo se calcula la distancia entre Clusters calcular la distancia o similitud entre clusteres en el proceso de agrupamiento jerárquico?. Existen varios métodos de enlace, destacando:\n\nEnlace Simple (Single Linkage): Calcula la distancia entre clusteres como la distancia más corta entre cualquier punto de un cluster y cualquier punto del otro cluster. Es sensible a la presencia de valores atípicos y al fenómeno del encadenamiento.\nEnlace Completo (Complete Linkage): Mide la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\nEnlace Promedio (Average Linkage): Calcula la distancia entre clusteres como la media de todas las distancias entre pares de puntos, uno de cada cluster. Más robusto frente a valores atípicos que el enlace simple y menos propenso al encadenamiento que el enlace completo.\nEnlace de Ward: Minimiza la varianza dentro de los clusteres al fusionarlos. Intenta minimizar la suma de cuadrados dentro de cada cluster después de la fusión.\n\nRepresentación jerárquica: Esto resulta en un dendrograma que muestra la jerarquía de agrupamiento, donde la altura en el dendrograma indica la distancia o disimilitud en la que se unen los clusteres.\n\nEl clustering jerárquico permite explorar diferentes niveles de granularidad en los datos, pero puede ser computacionalmente costoso para grandes conjuntos de datos. Es crucial elegir la medida de similitud adecuada y el método de enlace (criterio para unir clusteres, single linkage, complete linkage, average linkage,…) para obtener resultados significativos.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#formulación",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#formulación",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\nNotar que la distancia más apropiada para usar es la Euclidea ya que ambas variables 1Ola y 2Ola son del mismo tipo y corresponden a meses consecutivos, es decir, representan el mismo fenómeno demográfico y en la misma escala (una vez hayamos escalado). Además como estamos interesados en la diferencia de estas variables a la hora de hacer cluster, esta es la distancia más adecuada.\nEn cuanto al método para hacer los clusters, vamos a dejar el que viene por defecto, el complete. Este se basa en medir la distancia entre clusteres como la distancia más larga entre cualquier punto de un cluster y cualquier punto del otro cluster. Menos sensible a valores atípicos, pero puede generar clusteres de tamaño desigual.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"1Ola\", \"2Ola\")]\n\nresultado &lt;- scale(resultado) # scaling/standardizing\nrownames(resultado) &lt;- datos$provincia # Para que nos salgan luego los nombres\nprovincias &lt;- datos$provincia\n\n# Matriz de distancias\nd &lt;- dist(resultado, method = \"euclidean\")\n\n# Hierarchical clustering using Complete Linkage\nhc1 &lt;- hclust(d, method = \"complete\")\n\n# Plot the obtained dendrogram\nplot(hc1, cex = 0.6, hang = -1)\nabline(h = 3, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nEn el dendrograma mostrado arriba, cada hoja corresponde a una observación. A medida que avanzamos en el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una altura mayor.\nLa altura de la fusión, representada en el eje vertical, indica la (des) similitud entre dos observaciones. Cuanto mayor sea la altura de la fusión, menos similares son las observaciones. Es importante destacar que las conclusiones sobre la proximidad de dos observaciones solo se pueden inferir en función de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.\nLa altura del corte en el dendrograma controla el número de clusters obtenidos. Cumple el mismo papel que ‘k’ en la agrupación k-means. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función cutree. Suponer que queremos 3 clusteres:\n\n# Cut tree into 3 groups\nsub1 &lt;- cutree(hc1, k = 3)\n\n# Number of members in each cluster\ntable(sub1)\n\nsub1\n 1  2  3 \n31 11 11 \n\n\nPodemos mostrar los grupos junto al dataframe con la función mutate del paquete dplyr.\n\n# Mostrar Clusters\ndatos %&gt;%\n  mutate(cluster = sub1) %&gt;%\n  head()\n\n# A tibble: 6 × 17\n  provincia  falle_1Ola falle1_med17_19 `1Ola` falle_2Ola falle2_med17_19 `2Ola`\n  &lt;chr&gt;           &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Total nac…     113148          63962.  76.9      157968         133845.  18.0 \n2 Albacete         1600            553  189.         1287           1173.   9.75\n3 Alicante/…       3011           2404.  25.3        5852           5082.  15.1 \n4 Almería           851            801    6.24       2003           1702   17.7 \n5 Araba/Ala…        841            410. 105.         1022            935.   9.34\n6 Asturias         2445           1962.  24.6        5157           4207.  22.6 \n# ℹ 10 more variables: falle_3Ola &lt;dbl&gt;, falle3_med17_19 &lt;dbl&gt;, `3Ola` &lt;dbl&gt;,\n#   falle_4Ola &lt;dbl&gt;, falle4_med17_19 &lt;dbl&gt;, `4Ola` &lt;dbl&gt;, falle_5Ola &lt;dbl&gt;,\n#   falle5_med17_19 &lt;dbl&gt;, `5Ola` &lt;dbl&gt;, cluster &lt;int&gt;\n\ndatos &lt;- datos[, c(\"provincia\", \"1Ola\", \"2Ola\")]\ncolnames(datos) &lt;- c(\"provincia\", \"PrimOla\", \"SegOla\")\nresultado &lt;- as.data.frame(resultado)\nresultado$provincia &lt;- datos$provincia\ncolnames(resultado) &lt;- c(\"PrimOla\", \"SegOla\", \"provincia\")\n\n\n\n# Gráfico de puntos\ng1 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = provincia)) +\n  geom_point(aes(colour = factor(sub1))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub1))) +\n  labs(colour = \"Clusters\")\n\ng1\n\n\n\n\n\n\n\n\nDel anterior gráfico y de los clusters podemos concluir: hay grandes diferencias entre los tres clusters.\n\nEl Cluster Verde corresponde a las provincias donde la primera ola de COVID generó un gran exceso de mortalidad respecto al año anterior, son los casos de las provincias relativas a la comunidad de Madrid y Castilla-La Mancha.\nEn cambio, el Cluster Azul corresponde a las provincias donde se produjo un mayor exceso de fallecidos en la segunda ola, como son los casos de las provincias relativas a la comunidad de Aragón y Melilla en general.\nEl Cluster Rojo corresponde a las provincias donde hubo un exceso de fallecidos más parecido entre ambas olas, aunque podemos observar algunas diferencias entre algunas de ellas, por ejemplo, en Cataluña hubo un mayor exceso en la primera y en Ceuta hubo un mayor exceso en la segunda.\n\nNotar que estos resultados son en gran medida coherentes con los presentados en el mismo estudio por provincias Autónomas en vez de por provincias.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#con-otros-métodos-de-enlace",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#con-otros-métodos-de-enlace",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Con otros métodos de Enlace",
    "text": "Con otros métodos de Enlace\nVamos a probar a usar otros métodos de Enlace descritos previamente a ver si seguimos obteniendo los mismos clusters y poder llegar a una conclusión sólida.\n\n# For Complete\nplot(hc1, cex = 0.6, sub = \"\", main = \"complete\")\nrect.hclust(hc1, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Ward.D method\nhc2 &lt;- hclust(d, method = \"ward.D\")\nsub2 &lt;- cutree(hc2, k = 3)\n\nplot(hc2, cex = 0.6, sub = \"\", main = \"ward.D\")\nrect.hclust(hc2, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Average method\nhc3 &lt;- hclust(d, method = \"average\")\nsub3 &lt;- cutree(hc3, k = 3)\n\nplot(hc3, cex = 0.6, sub = \"\", main = \"average\")\nrect.hclust(hc3, k = 3, border = 2:5)\n\n\n\n\n\n\n\n# Single method\nhc4 &lt;- hclust(d, method = \"single\")\nsub4 &lt;- cutree(hc4, k = 3)\n\nplot(hc4, cex = 0.6, sub = \"\", main = \"single\")\nrect.hclust(hc4, k = 3, border = 2:5)\n\n\n\n\n\n\n\n\n\ng2 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = provincia)) +\n  geom_point(aes(colour = factor(sub2))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub2))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng3 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = provincia)) +\n  geom_point(aes(colour = factor(sub3))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub3))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\ng4 &lt;- ggplot(datos, aes(PrimOla, SegOla, label = provincia)) +\n  geom_point(aes(colour = factor(sub4))) +\n  geom_text(hjust = 0, vjust = 0, size = 2, aes(colour = factor(sub4))) +\n  labs(colour = \"Clusters\") +\n  theme(\n    axis.text = element_text(size = 6),\n    axis.title = element_text(size = 6, face = \"bold\"), legend.title = element_text(size = 6)\n  )\n\n\nlibrary(ggpubr)\nggarrange(g1, g2, g3, g4 + rremove(\"x.text\"),\n  labels = c(\"Complete\", \"ward.D\", \"Average\", \"Simple\"),\n  ncol = 2, nrow = 2, vjust = 0.9,\n  font.label = list(size = 8, color = \"black\")\n)\n\n\n\n\n\n\n\n\nDe 4 tipos diferentes que hemos probado, nos salen 3 que se han clusterizado de igual manera, luego parece razonable la interpretación anterior. Para todos casos menos el de ward,\n\nel Cluster Verde corresponde a las provincias donde la primera ola de COVID generó un gran exceso de mortalidad respecto al año anterior, son los casos de las provincias relativas a la comunidad de Madrid y Castilla la Mancha.\nel Cluster Azul corresponde a las provincias donde se produjo un mayor exceso de fallecidos en la segunda ola, como son los casos de Melilla y Aragón.\nel Cluster Rojo corresponde a las provincias donde hubo un exceso de fallecidos más parecido entre ambas olas, aunque podemos observar algunas diferencias entre algunas de ellas, por ejemplo, en Cataluña hubo un mayor exceso en la primera y en Ceuta hubo un mayor exceso en la segunda.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#método-elbow",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#método-elbow",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"wss\")\n\n\n\n\n\n\n\n\nEl número óptimo de k parece ser 4 que es donde más se reduce la pendiente y la variabilidad explicada no parece disminuir de forma tan rápida. De todos modos, también podría parecer razonable tomar el 3 Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#método-silhouette",
    "href": "notebooks/Cluster Jerarquico/VarCovid_provincias/VarCovid_provincias.html#método-silhouette",
    "title": "Cluster Jerárquico: VarCovid_provincias",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, hcut, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 2 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 2 e intentar analizar de nuevo los resultados.",
    "crumbs": [
      "Notebooks",
      "Cluster Jerarquico",
      "VarCovid Provincias",
      "Cluster Jerárquico: VarCovid_provincias"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo K-Means a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado desigualdad_CCAA.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18. Los datos (relativos a las variables salario medio mujeres, hombres que nos interesan) han sido extraídos de la Operación Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios. Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nCCAA: Comunidades Autónomas.\nSalmedmuj: Salario medio anual (mujeres).\nSalmedhom: Salario medio anual (hombres).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Salmedmuj y Salmedhom. Concretamente usaremos un cluster K Means.\n\n\n\nEn este notebook se expondrá como realizar un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables Salmedmuj y Salmedhom para agrupar las comunidades por las diferencias de salarios entre sexos.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (Salmedmuj, Salmedhom).\nEstandarizar datos y probar cluster k-means con k=4.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#dataset",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#dataset",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado desigualdad_CCAA.xlsx. Este dataset presenta un conjunto de datos sobre el salario medio anual de hombres y mujeres en España, relativos a años 2017/18. Los datos (relativos a las variables salario medio mujeres, hombres que nos interesan) han sido extraídos de la Operación Encuesta Anual de Estructura Salarial (IOE 30189), que se encuentra dentro de la temática Mercado laboral y salarios. Concretamente en este dataset tenemos las siguientes variables (que nos interesan para este análisis):\n\nCCAA: Comunidades Autónomas.\nSalmedmuj: Salario medio anual (mujeres).\nSalmedhom: Salario medio anual (hombres).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables Salmedmuj y Salmedhom. Concretamente usaremos un cluster K Means.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "",
    "text": "En este notebook se expondrá como realizar un Análisis Cluster empleando el procedimiento Cluster Jerárquico de las CCAA en función a las variables Salmedmuj y Salmedhom para agrupar las comunidades por las diferencias de salarios entre sexos.\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (Salmedmuj, Salmedhom).\nEstandarizar datos y probar cluster k-means con k=4.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#cargar-librerías",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#cargar-librerías",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\nlibrary(factoextra) # fviz_cluster function\nlibrary(gridExtra) # Para el layout de los gráficos",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#lectura-de-datos",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#lectura-de-datos",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos &lt;- read_excel(\"../../../files/desigualdad_ccaa.xlsx\", sheet = \"Datos\")\n\n\n# Histogram dim1\nhistogram &lt;- ggplot(datos, aes(x = Salmedmuj)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Salmedmuj\", x = \"Salmedmuj\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim1\nboxplot &lt;- ggplot(datos, aes(x = \"d\", y = Salmedmuj)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Salmedmuj\", x = \"\", y = \"Salmedmuj\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n# Histogram dim2\nhistogram2 &lt;- ggplot(datos, aes(x = Salmedhom)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Salmedhom\", x = \"Salmedhom\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim2\nboxplot2 &lt;- ggplot(datos, aes(x = \"d\", y = Salmedhom)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Salmedhom\", x = \"\", y = \"Salmedhom\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\ngrid.arrange(histogram, histogram2, boxplot, boxplot2, nrow = 2, ncol = 2, widths = c(0.3, 0.3))\n\n\n\n\n\n\n\n\n\nDistribución Salarial:\n\nLos salarios medios de las mujeres muestran una mayor dispersión y variabilidad en comparación con los de los hombres. Esto se evidencia por la presencia de un valor atípico en los salarios de las mujeres.\nLos salarios medios de los hombres están más concentrados y muestran menos variabilidad, sin valores atípicos significativos.\n\nMedianas Salariales:\n\nLa mediana del salario medio de los hombres (26,000) es más alta que la de las mujeres (20,000), lo que podría indicar una brecha salarial entre hombres y mujeres en la muestra de datos.\n\n\nEstos gráficos proporcionan una clara representación visual de las diferencias en la distribución de los salarios medios entre hombres y mujeres, lo que puede ser útil para análisis posteriores y toma de decisiones.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#introducción-1",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#introducción-1",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis Cluster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clústeres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nEjemplo ilustrativo: Imaginar que tenemos un conjunto de datos de alumnos de un colegio de educación primaria y para cada uno de ellos disponemos de varias variables como edad, sexo, altura, promedio de notas.. Imaginar que queremos hacer grupos de estudiantes para aplicar programas de estudios. Intuitivamente asociaremos a cada edad un curso, y de manera adicional podemos decir que si tienen una media de notas muy alta se les podría asociar a un curso superior y si es muy baja a uno inferior. Es decir, a partir de determinadas reglas de decisión hemos conseguido clasificar las observaciones en diferentes grupos de datos.\nConcretamente, el Cluster K-Means define clusters de modo que se minimice la variación total dentro del grupo de acuerdo con el algoritmo Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias al cuadrado de las distancias euclidianas entre elementos y el centroide correspondiente. Se describe a continuación.\nLos pasos generales de este algoritmo son:\n\nEspecificar el número de clusters (K) que se se desean obtener.\nSeleccionar aleatoriamente k objetos del conjunto de datos como centros del grupo (centroides). Asigna cada observación a su centroide más cercano, según la distancia entre el objeto y el centroide(es necesario elegir una función de distancia).\nPara cada uno de los k grupos, actualizar el centroide del grupo calculando los nuevos valores medios de todos los puntos de datos del grupo. El centroide de un grupo K-ésimo es un vector de longitud p que contiene las medias de todas las variables para las observaciones en el grupo K-ésimo; p es el número de variables.\nCalcular la distancia entre las observaciones y los nuevos centroides, asignado las observaciones al cluster del centroide más cercano.\nRepetir pasos 3-4 sucesivamente hasta que los centroides no cambien. En ese caso se supone que se ha alcanzado la convergencia y ya se han encontrado unos clusters estables. De forma predeterminada, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones, con el fin de evitar que entre en una secuencia infinita de iteraciones en el caso de no converger nunca.\n\nVéase funciones de R stats::kmeans(x, centers, iter.max, nstart) que realizan los pasos 2-5 automáticamente.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#método-elbow",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#método-elbow",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada (distancia promedio de los elementos al centroide del cluster) o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"wss\", print.summary = TRUE)\n\n\n\n\n\n\n\n\nEste gráfico ayuda a determinar el número óptimo de clusters para el algoritmo K-means. El método del codo (Elbow Method) consiste en observar el punto donde la reducción en la suma de los cuadrados dentro del cluster (WSS) empieza a disminuir de manera considerable.\n\nEl gráfico muestra un fuerte descenso en WSS hasta 2 clusters, luego se reduce más lentamente.\nEl “codo” más pronunciado parece estar en 2 clusters, pero dado que estamos utilizando 4 clusters en el primer gráfico, es razonable asumir que el análisis también considera la relevancia de esos clusters adicionales para capturar variaciones en los datos.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#método-silhouette",
    "href": "notebooks/Cluster K-Means/desigualdad_ccaa/desigualdad_ccaa.html#método-silhouette",
    "title": "Cluster K-Means: desigualdad_CCAA",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 2 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 2 e intentar analizar de nuevo los resultados. Destacar que, en última instancia, el número de clusters depende del interés del usuario que deberá fijarlo en función de sus objetivos o analizando que número es el óptimo.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Desigualdad Ccaa",
      "Cluster K-Means: desigualdad_CCAA"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo un Análisis Cluster de tipo K-Means a partir de un conjunto de datos, explicando los fundamentos teóricos en el que se basa para agrupar los datos.\n\n\nEn este cuaderno vamos a analizar el dataset llamado ecv_cluster.xlsx. Este contiene datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021. Las variables de interés son las siguientes:\n\nccaa: Comunidades Autónomas\ntaspobex: Tasa de riesgo de pobreza o exclusión social (indicador AROPE).\ntaspob: Tasa en riesgo de pobreza (renta año anterior a la entrevista).\ntascar: Tasa con carencia material severa.\ntasvivtrab: Tasa de hogares viviendo con baja intensidad en el trabajo (de 0 a 59 años).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables definidas arriba. Concretamente usaremos un cluster K-Means\n\n\n\nSe pretende hacer un Análisis Cluster empleando el procedimiento Cluster K-Means de las CCAA en función a las variables taspobex y tascar .\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (taspobex, tascar).\nEstandarizar datos y probar cluster k-means con k=3.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#dataset",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#dataset",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado ecv_cluster.xlsx. Este contiene datos por Comunidades Autónomas sobre la tasa de riesgo de pobreza, la carencia material o la situación laboral que encontramos dentro de la Encuesta de Condiciones de Vida (ECV). Datos correspondientes al año 2021. Las variables de interés son las siguientes:\n\nccaa: Comunidades Autónomas\ntaspobex: Tasa de riesgo de pobreza o exclusión social (indicador AROPE).\ntaspob: Tasa en riesgo de pobreza (renta año anterior a la entrevista).\ntascar: Tasa con carencia material severa.\ntasvivtrab: Tasa de hogares viviendo con baja intensidad en el trabajo (de 0 a 59 años).\n\nEl objetivo de este estudio será aplicar un Análisis Cluster para hacer grupos de comunidades autónomas en función de las variables definidas arriba. Concretamente usaremos un cluster K-Means",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#descripción-del-trabajo-a-realizar",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "",
    "text": "Se pretende hacer un Análisis Cluster empleando el procedimiento Cluster K-Means de las CCAA en función a las variables taspobex y tascar .\n\nHacer un análisis exploratorio.Ver si hay NA’s y si es necesario escalar los datos.\nVariables sobre las que se buscan cluster (taspobex, tascar).\nEstandarizar datos y probar cluster k-means con k=3.\nInterpretar resultados.\nVer métodos Elbow y Silhouette si hay otro número óptimo de clusters y en ese caso repetir el estudio.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#cargar-librerías",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#cargar-librerías",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(ggplot2) # Nice plots\nlibrary(stats) # hclust package\nlibrary(factoextra) # fviz_cluster function\nlibrary(gridExtra) # Para el layout de los gráficos\nlibrary(dplyr)",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#lectura-de-datos",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#lectura-de-datos",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../files/ecv_cluster.xlsx\", sheet = \"Datos\")\n\n\n# Histogram dim1\nhistogram &lt;- ggplot(datos, aes(x = taspobex)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of taspobex\", x = \"taspobex\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim1\nboxplot &lt;- ggplot(datos, aes(x = \"d\", y = taspobex)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of taspobex\", x = \"\", y = \"taspobex\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n# Histogram dim2\nhistogram2 &lt;- ggplot(datos, aes(x = tascar)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of tascar\", x = \"tascar\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot dim2\nboxplot2 &lt;- ggplot(datos, aes(x = \"d\", y = tascar)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of tascar\", x = \"\", y = \"tascar\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\ngrid.arrange(histogram, histogram2, boxplot, boxplot2, nrow = 2, ncol = 2, widths = c(0.3, 0.3))\n\n\n\n\n\n\n\n\nLa tasa de riesgo de pobreza o exclusión social (taspobex) no solo es generalmente más alta, sino que también presenta una mayor variabilidad en comparación con la tasa de carencia material severa (tascar). La asimetría positiva en ambas variables sugiere la presencia de algunas Comunidades Autónomas con tasas significativamente más altas que la mediana, especialmente en taspobex.\nEsta comparación es crucial para entender cómo diferentes aspectos de la pobreza y la carencia material afectan a las Comunidades Autónomas y puede guiar políticas más específicas y efectivas para abordar estas cuestiones.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#introducción-1",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#introducción-1",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de clúster es una técnica de aprendizaje no supervisado que agrupa datos similares en conjuntos, llamados clústeres. El objetivo es dividir un conjunto de datos en grupos homogéneos, donde los miembros de cada grupo son más similares entre sí que con los miembros de otros grupos, según algún criterio de similitud predefinido.\nEjemplo ilustrativo: Imaginar que tenemos un conjunto de datos de alumnos de un colegio de educación primaria y para cada uno de ellos disponemos de varias variables como edad, sexo, altura, promedio de notas.. Imaginar que queremos hacer grupos de estudiantes para aplicar programas de estudios. Intuitivamente asociaremos a cada edad un curso, y de manera adicional podemos decir que si tienen una media de notas muy alta se les podría asociar a un curso superior y si es muy baja a uno inferior. Es decir, a partir de determinadas reglas de decisión hemos conseguido clasificar las observaciones en diferentes grupos de datos.\nConcretamente, el Cluster K-Means define clusters de modo que se minimice la variación total dentro del grupo de acuerdo con el algoritmo Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias al cuadrado de las distancias euclidianas entre elementos y el centroide correspondiente. Se describe a continuación.\nLos pasos generales de este algoritmo son:\n\nEspecificar el número de clusters (K) que se se desean obtener.\nSeleccionar aleatoriamente k objetos del conjunto de datos como centros del grupo (centroides). Asigna cada observación a su centroide más cercano, según la distancia entre el objeto y el centroide(es necesario elegir una función de distancia).\nPara cada uno de los k grupos, actualizar el centroide del grupo calculando los nuevos valores medios de todos los puntos de datos del grupo. El centroide de un grupo K-ésimo es un vector de longitud p que contiene las medias de todas las variables para las observaciones en el grupo K-ésimo; p es el número de variables.\nCalcular la distancia entre las observaciones y los nuevos centroides, asignado las observaciones al cluster del centroide más cercano.\nRepetir pasos 3-4 sucesivamente hasta que los centroides no cambien. En ese caso se supone que se ha alcanzado la convergencia y ya se han encontrado unos clusters estables. De forma predeterminada, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones, con el fin de evitar que entre en una secuencia infinita de iteraciones en el caso de no converger nunca.\n\nVéase funciones de R stats::kmeans(x, centers, iter.max, nstart) que realizan los pasos 2-5 automáticamente.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#formulación",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#formulación",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE:\n\nVer que no hay ningún NA en el dataset.\nEl escalado es un paso esencial en la fase de preprocesamiento de datos para los algoritmos de agrupación. Garantiza que cada característica contribuya por igual al proceso de decisión del algoritmo, lo que lleva a resultados de agrupación más precisos e interpretables.\n\n\nifelse(sum(is.na(data)) == 0, print(\"There is no NA in the dataset.\"), print(\"There is some NA in the dataset.\"))\n\n[1] \"There is no NA in the dataset.\"\n\n\n[1] \"There is no NA in the dataset.\"\n\n\nSi queremos que el código sea reproducible, es necesario fijar semilla (función set.seed(n)) ya que el algoritmo k-means elige los centroides iniciales aleatoriamente.\n\n# Preparación de los datos\nresultado &lt;- datos[, c(\"taspobex\", \"tascar\")]\n\nresultado &lt;- scale(resultado) # scaling/standardizing\nrownames(resultado) &lt;- datos$CCAA # Para que nos salgan luego los nombres\ncomunidades &lt;- datos$CCAA\n\n\n# K-MEANS algortihm\nset.seed(785248) # reproducibilidad\nk1 &lt;- kmeans(resultado, centers = 3, nstart = 25)\nk1\n\nK-means clustering with 3 clusters of sizes 11, 3, 5\n\nCluster means:\n    taspobex      tascar\n1 -0.7548531 -0.51757600\n2  1.3941669  1.99603588\n3  0.8241767 -0.05895433\n\nClustering vector:\n                  Andalucía                      Aragón \n                          3                           1 \n    Asturias, Principado de              Balears, Illes \n                          1                           1 \n                   Canarias                   Cantabria \n                          2                           1 \n            Castilla y León        Castilla - La Mancha \n                          1                           3 \n                   Cataluña        Comunitat Valenciana \n                          1                           3 \n                Extremadura                     Galicia \n                          3                           1 \n       Madrid, Comunidad de           Murcia, Región de \n                          1                           3 \nNavarra, Comunidad Foral de                  País Vasco \n                          1                           1 \n                  Rioja, La                       Ceuta \n                          1                           2 \n                    Melilla \n                          2 \n\nWithin cluster sum of squares by cluster:\n[1] 2.413697 1.571024 1.603415\n (between_SS / total_SS =  84.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nfviz_cluster(k1, data = resultado) # plot\n\n\n\n\n\n\n\n\nPodemos observar que la agrupación en 3 clusters que ha hecho el algoritmo K-MEANS es bastante similar a la que obtuvimos con el cluster jerárquico. Por un lado tenemos un cluster de los valores que se encuentran más a la derecha, luego otro con los que están más arriba y otros con los más cercanos al origen. En cierto modo:\n\nEl cluster verde representa las CCAA que presentan una tasa de riesgo de pobreza y una tasa de personas con carencia material severa muy alta. Estas son Canarias, Ceuta y Melilla, lo cual cabría pensar que tiene sentido ya que ambas tres regiones contienen fuertes corrientes migratorias debido a que se encuentran en puntos fronterizos en el que hay mucha inmigración ilegal procedente de países como Marruecos.\nEn el cluster azul se encuentran las CCAA que presentan una tasa de riesgo de pobreza similares a las anteriores pero la de personas con carencia material alta no es tan grande como en las citadas anteriormente. Estas son Andalucía, Murica y Extremadura, que efectivamente es común que aparezcan en la prensa anualmente como regiones con más pobreza dentro de España (a excepción de Murica y Valencia) y sin embargo presentan una tasa de personas con carencia material no tan alta como las anteriores puesto que no hay tanta población en situación irregular que pueda derivar en una carencia material sustancial.\nPor último, el cluster rojo presenta comunidades que tuvieron una incidencia parecida en la primera y segunda ola.\n\nNotar que podemos mostrar los datos originales junto al cluster al que pertenecen.\n\n# Mostrar Clusters\ndatos %&gt;%\n  mutate(cluster = k1$cluster) %&gt;%\n  select(\"CCAA\", \"taspobex\", \"tascar\", \"cluster\")\n\n# A tibble: 19 × 4\n   CCAA                        taspobex tascar cluster\n   &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt;\n 1 Andalucía                       38.4   10.2       3\n 2 Aragón                          20.3    5.6       1\n 3 Asturias, Principado de         26.6    5.5       1\n 4 Balears, Illes                  24.5    8.5       1\n 5 Canarias                        38.3   13.5       2\n 6 Cantabria                       21.6    5.7       1\n 7 Castilla y León                 22.4    3.8       1\n 8 Castilla - La Mancha            31.4    5.1       3\n 9 Cataluña                        22.1    7.3       1\n10 Comunitat Valenciana            30.3    7.1       3\n11 Extremadura                     39.1    6.9       3\n12 Galicia                         24.5    3.8       1\n13 Madrid, Comunidad de            21.1    6         1\n14 Murcia, Región de               34.7    9.1       3\n15 Navarra, Comunidad Foral de     16.6    5.5       1\n16 País Vasco                      15.9    5.2       1\n17 Rioja, La                       20.1    3.8       1\n18 Ceuta                           42.4   21.4       2\n19 Melilla                         38.1   17.2       2",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#método-elbow",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#método-elbow",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Método Elbow",
    "text": "Método Elbow\nUna de las formas comunes de determinar este número es a través del método del codo o elbow en inglés. Este método busca identificar el punto donde la adición de más clusters ya no proporciona un beneficio significativo en la varianza explicada o la cohesión dentro de los grupos.\nAl representar la variación explicada en función del número de clusters, observamos un gráfico que se asemeja a la forma de un codo. A medida que aumentamos el número de clusters, la varianza explicada tiende a disminuir. El punto en el que esta disminución se estabiliza o se aplana marca el número óptimo de clusters, indicando un equilibrio entre una mayor partición (más clusters) y una adecuada interpretabilidad de los grupos.\n\n#  Método Elbow\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"wss\", print.summary = TRUE)\n\n\n\n\n\n\n\n\nEl número óptimo de k parece ser 3 que es donde más se reduce la pendiente y la variabilidad explicada no parece disminuir de forma tan rápida. De todos modos, también podría parecer razonable tomar el 2. Es por ello que vamos a usar algún método adicional.",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#método-silhouette",
    "href": "notebooks/Cluster K-Means/ecv_cluster/ecv_cluster.html#método-silhouette",
    "title": "Cluster K-Means: ecv_cluster",
    "section": "Método Silhouette",
    "text": "Método Silhouette\nEl método Silhouette es una técnica utilizada para determinar la calidad de la agrupación en un conjunto de datos. Consiste en calcular el valor de la silueta para cada punto de datos, que mide qué tan similar es un punto a su propio grupo (cohesión) en comparación con otros grupos vecinos (separación).\nEl proceso implica:\n\nCálculo de la silueta individual: Para cada punto de datos, se calcula la silueta, que es la diferencia entre la distancia media intra-cluster (distancia al resto de puntos en su mismo grupo) y la distancia media al cluster más cercano (distancia a los puntos del grupo más próximo, excluyendo el propio grupo).\nValor de la silueta global: Se obtiene el promedio de las siluetas individuales de todos los puntos de datos en el conjunto. Contra más cercano a 1, mejor formado estará el cluster.\n\nLa siguiente función generará un gráfico que muestra los valores de Silhouette en función del número de clusters. El número óptimo de clusters es típicamente aquel que maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\n\n#  Método Silhouette\nset.seed(785248)\nfactoextra::fviz_nbclust(resultado, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nEste método nos reafirma que el número óptimo es 2 puesto que es el caso cuyos clusters maximiza el valor de Silhouette, representando una mejor cohesión intra-cluster y separación inter-cluster.\nNOTA: Ahora podríamos repetir el estudio anterior con el número de clusters igual a 2 e intentar analizar de nuevo los resultados.\n\n# K-MEANS algortihm\nset.seed(785248) # reproducibilidad\nk1 &lt;- kmeans(resultado, centers = 2, nstart = 25)\nk1\n\nK-means clustering with 2 clusters of sizes 13, 6\n\nCluster means:\n    taspobex     tascar\n1 -0.5834244 -0.4985867\n2  1.2640861  1.0802711\n\nClustering vector:\n                  Andalucía                      Aragón \n                          2                           1 \n    Asturias, Principado de              Balears, Illes \n                          1                           1 \n                   Canarias                   Cantabria \n                          2                           1 \n            Castilla y León        Castilla - La Mancha \n                          1                           1 \n                   Cataluña        Comunitat Valenciana \n                          1                           1 \n                Extremadura                     Galicia \n                          2                           1 \n       Madrid, Comunidad de           Murcia, Región de \n                          1                           2 \nNavarra, Comunidad Foral de                  País Vasco \n                          1                           1 \n                  Rioja, La                       Ceuta \n                          1                           2 \n                    Melilla \n                          2 \n\nWithin cluster sum of squares by cluster:\n[1] 4.639180 7.114779\n (between_SS / total_SS =  67.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nfviz_cluster(k1, data = resultado) # plot",
    "crumbs": [
      "Notebooks",
      "Cluster K-Means",
      "Ecv Cluster",
      "Cluster K-Means: ecv_cluster"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/Dataset_cleaning.html",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/laboral/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente analizar el nivel de estudios de una población (con gente con muchos estudios y gente con muy pocos) a partir de los microdatos de la Encuestas de estructura salarial. Resultados Concretamente, se han tomado los datos relativos a 2018.\nLos microdatos de la Encuestas de estructura salarial. Resultados pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177025&menu=resultados&secc=1254736061996&idp=1254735976596#!tabs-1254736195110. De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de un anaálisis de discriminante y el nombre que tomará en el dataset exportado.\n\nESTU: Nivel de estudios del encuestado.\nRETRINOIN: Sueldo bruto anual (sin incluir subsidios por incapacidad).\nRETRIIN: Sueldo bruto anual por incapacidad.\nANOS2: Edad en años cumplidos del encuestado.\nANOANTI: Años de antigüedad.\nVAL: Días de vacaciones al año.\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Microdatos\nlibrary(haven)\ndatos &lt;- read_sav(\"~/Downloads/datos_2018/SPSS/EES_2018.sav\")\ndatos2 &lt;- datos[, c(\"ESTU\", \"RETRINOIN\", \"RETRIIN\", \"ANOS2\", \"VAL\", \"ANOANTI\")]\n\n# El salario es la suma del salario de no incapacidad y del de incapacidad\ndatos2$salario &lt;- as.numeric(datos2$RETRINOIN) + as.numeric(datos2$RETRIIN)\n\n# Los que tiene salarios muy grandes los dejamos en 100k (marca de clase)\ndatos2$salario &lt;- ifelse(datos2$salario &gt; 100000, 100000, datos2$salario)\ndatos2$Estudios &lt;- as.character(datos2$ESTU)\ndatos &lt;- na.omit(datos)\n\n# Filtramos los estudios seleccionando con \"1\" y \"2\" la gente que tiene estudios hasta primaria\n# y con \"6\" y \"7\" los que tienen al menos esudiuos universitarios\ndatos2 &lt;- dplyr::filter(datos2, ESTU == \"1\" | ESTU == \"2\" | ESTU == \"6\" | ESTU == \"7\")\n\n\n# Nuevos nombres de variables\ndatos2$Estudios &lt;- factor(ifelse(datos2$ESTU == \"2\", 1, ifelse(datos2$ESTU == \"1\", 1, 0)))\ndatos2$Antiguedad &lt;- as.numeric(datos2$ANOANTI)\ndatos2$Salario &lt;- as.numeric(datos2$salario)\ndatos2$Edad &lt;- as.numeric(as.character(datos2$ANOS2))\ndatos2$Vacaciones &lt;- as.numeric(datos2$VAL)\n# Solo guardamos las que queremos\ndatos2 &lt;- datos2[, c(\"Estudios\", \"Antiguedad\", \"Salario\", \"Edad\", \"Vacaciones\")]\n\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos2, \"../../../../files/laboral.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Laboral",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/Dataset_cleaning.html",
    "href": "notebooks/Analisis Discriminante/Lineal_y_Cuadratico/salud/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos en bruto para generar un fichero accesible con el fin de discriminar el sexo de una persona a partir de los microdatos de la Encuesta Nacional de Salud. Resultados Concretamente, se han tomado los datos relativos a 2017.\nLos microdatos de la Encuesta 2017 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176783&menu=resultados&secc=1254736195295&idp=1254735573175#!tabs-1254736195295. Tomamos el fichero relativo a Adultos (15 años y más). De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de un anaálisis de discriminante y el nombre que tomará en el dataset exportado.\n\nEDADa: Identificación del adulto seleccionado: Edad.\nSEXOa: Identificación del adulto seleccionado: Sexo.\nS109: Altura en cm.\nS110: Peso en kg.\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\nDestacar que las observaciones correspondientes a personas con más de 220kg y/o altura más de 220cm las hemos eliminado pues parecen errores.\n# Microdatos\nlibrary(readxl)\ndatos &lt;- read_excel(\"/Users/davpero/Downloads/BECA/datos_ensalud17_xlsx/MICRODAT.CA.xlsx\")\ndatos &lt;- datos[, c(\"EDADa\", \"SEXOa\", \"S109\", \"S110\")]\nlibrary(dplyr)\n\n# Quitamos las observaciones que carecen de valor\ndatos &lt;- na.omit(datos)\n\n\n\n\n# Conevrtimos a factor variable sexo y la renombramos\ndatos$SEXO &lt;- as.factor(ifelse(datos$SEXOa == 1, 1, 0))\n\n# Numéricas las demas y las renombramos\ndatos$EDAD &lt;- as.numeric(datos$EDADa)\ndatos$Altura &lt;- as.numeric(datos$S109)\ndatos$Peso &lt;- as.numeric(datos$S110)\n\n# Los valores atípicos los quitamos, los que tienen un peso y una altura que no tiene sentido\ndatos$Peso[datos$Peso &gt; 220] &lt;- NA\ndatos$Altura[datos$Altura &gt; 220] &lt;- NA\ndatos &lt;- na.omit(datos)\n\n\n# Nuevo conjunto de datos con las variables deseadas\ndatos &lt;- datos[, c(\"EDAD\", \"SEXO\", \"Altura\", \"Peso\")]\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos, \"../../../../files/salud.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis salud.xlsx.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Analisis Discriminante",
      "Lineal Y Cuadratico",
      "Salud",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/Partos/Dataset_cleaning.html",
    "href": "notebooks/Logistic Regression/Partos/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente analizar la presencia de una cesárea en un parto a partir de los microdatos de la Estadística de nacimientos. Movimiento natural de la población. Concretamente, se han tomado los datos relativos a partos, 2022 y los relativos a la Comunidad Autónoma de Navarra.\nLos microdatos de la Estadística de nacimientos, Movimiento natural de la población, 2022 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177007&menu=resultados&secc=1254736195443&idp=1254735573002#!tabs-1254736195443. De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de una regresión logística y el nombre que tomará en el dataset exportado.\n\nMULTIPLI: Número de bebés nacidos en el parto. (categórica)\nSEMANAS: Número de semanas del embarazo.\nEDADM: Edad de la madre en años cumplidos.\nCESAREA: Si se ha llevado a cabo una césarea en el parto (Categórica. 1=si, 2=no).\nNORMA: Si el parto transcurrió con normalidad. (Categórica. 1=normal, 2=Complicaciones)\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Librería tratamiento dataframes\nlibrary(dplyr)\n\n\n# Microdatos\nload(\"~/Downloads/datos_partos22/R/MNPpartos_2022.RData\")\ndatos1 &lt;- Microdatos[, c(\"MULTIPLI\", \"SEMANAS\", \"EDADM\", \"CESAREA\", \"PROMA\", \"NORMA\")]\n\n# Quitamos observaciones con NA\ndatos1 &lt;- na.omit(datos1)\n\n# Seleccionamos CCAA de Navarra, código 31\ndatos1 &lt;- datos1 %&gt;% filter(PROMA == 31) # 31\n\n# Cambiamos niveles de cesarea\ndatos1$CESAREA &lt;- as.factor(ifelse(datos1$CESAREA == 1, 1, 0))\n\ndatos1 &lt;- datos1 %&gt;% rename(\n  bebes = MULTIPLI,\n  semanas = SEMANAS,\n  edad_madre = EDADM,\n  cesarea = CESAREA,\n  parto_normal = NORMA\n)\nCon esto nos queda el siguiente dataset.\n\nbebes: Número de bebés nacidos en el parto. (categórica)\nsemanas: Número de semanas del embarazo.\nedad_madre: Edad de la madre en años cumplidos.\ncesarea: Si se ha llevado a cabo una césarea en el parto (Categórica. 0=no, 1=si).\nparto_normal: Si el parto transcurrió con normalidad. (Categórica. 1=normal, 2=complicaciones)\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos1, \"../../../files/Partos.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis.El fichero de datos procesado se puede encontrar en Partos.xlsx\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Partos",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/laboral/Dataset_cleaning.html",
    "href": "notebooks/Logistic Regression/laboral/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente analizar el nivel de estudios de una población(con gente con muchos esudios y gente con muy pocos) a partir de los microdatos de la Encuestas de estructura salarial. Resultados Concretamente, se han tomado los datos relativos a 2018.\nLos microdatos de la Encuestas de estructura salarial. Resultados pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177025&menu=resultados&secc=1254736061996&idp=1254735976596#!tabs-1254736195110. De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de un anaálisis de discriminante y el nombre que tomará en el dataset exportado.\n\nESTU: Nivel de estudios del encuestado.\nRETRINOIN: Sueldo bruto anual (sin incluir subsidios por incapacidad).\nRETRIIN: Sueldo bruto anual por incapacidad.\nANOS2: Edad en años cumplidos del encuestado.\nANOANTI: Años de antigüedad.\nVAL: Días de vacaciones al año.\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Microdatos\nlibrary(haven)\ndatos &lt;- read_sav(\"~/Downloads/datos_2018/SPSS/EES_2018.sav\")\ndatos2 &lt;- datos[, c(\"ESTU\", \"RETRINOIN\", \"RETRIIN\", \"ANOS2\", \"VAL\", \"ANOANTI\")]\n\n# El salario es la suma del salario de no incapacidad y del de incapacidad\ndatos2$salario &lt;- as.numeric(datos2$RETRINOIN) + as.numeric(datos2$RETRIIN)\n\n# Los que tiene salarios muy grandes los dejamos en 100k (marca de clase)\ndatos2$salario &lt;- ifelse(datos2$salario &gt; 100000, 100000, datos2$salario)\ndatos2$Estudios &lt;- as.character(datos2$ESTU)\ndatos &lt;- na.omit(datos)\n\n# Filtramos los estudios seleccionando con \"1\" y \"2\" la gente que tiene estudios hasta primaria\n# y con \"6\" y \"7\" los que tienen al menos esudiuos universitarios\ndatos2 &lt;- dplyr::filter(datos2, ESTU == \"1\" | ESTU == \"2\" | ESTU == \"6\" | ESTU == \"7\")\n\n\n# Nuevos nombres de variables\ndatos2$Estudios &lt;- factor(ifelse(datos2$ESTU == \"2\", 1, ifelse(datos2$ESTU == \"1\", 1, 0)))\ndatos2$Antiguedad &lt;- as.numeric(datos2$ANOANTI)\ndatos2$Salario &lt;- as.numeric(datos2$salario)\ndatos2$Edad &lt;- as.numeric(as.character(datos2$ANOS2))\ndatos2$Vacaciones &lt;- as.numeric(datos2$VAL)\n# Solo guardamos las que queremos\ndatos2 &lt;- datos2[, c(\"Estudios\", \"Antiguedad\", \"Salario\", \"Edad\", \"Vacaciones\")]\n\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos2, \"../../../files/laboral.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. laboral.xlsx.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Laboral",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html",
    "href": "notebooks/Logistic Regression/ECV/ECV.html",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "",
    "text": "En este cuaderno se va a explicar los fundamentos de la Regresión Logística, como detectar cuando nos encontramos ante un problema que se debe abordar mediante este tipo de Regresión. Se verán los fundamentos teóricos que lo sustentan y las técnicas a llevar a cabo para analizar los datos.\n\n\nEn este cuaderno vamos a analizar la existencia del llamado “ascensor social” en España a partir de los microdatos de la Encuesta de Condiciones de Vida (ECV) del año 2019. Dicho análisis nos sirve como excusa para tratar de mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\nEn primer lugar, debemos definir qué es eso del “ascensor social” y cómo vamos a tratar de analizarlo nosotros. Éste se define como la posibilidad de ascender o descender de clase social. Aunque podemos considerar que la pertenencia a una determinada clase social - si es que éstas existen de forma estanca y perfectamente distinguible - se explica por una combinación de aspectos: nivel económico, cultural, de estudios, etc, nosotros nos vamos a centrar simplemente en el nivel económico.\nVer fichero Dataset_cleaning.rmd para ver cómo se han tratado dichos microdatos hasta obtener un conjunto de datos adecuado para la regresión logística, y que tomaremos aquí como punto de partida. En este cuaderno vamos a analizar el dataset llamado ECV.xlsx.\nConcretamente tenemos las siguientes variables:\n\nMSE (Mejora Situación Económica): Variable binaria, 1= ha mejorado la calidad de vida del encuestado respecto a su infancia, 0=no ha mejorado. teniendo en cuenta las nueve dimensiones.\nFactor_de_elevacion : Factor de elevación para cada observación respecto a la población total.\nEstado_civil : Estado civil del encuestado. 0=No casado, 1=Casado\nNivel_Estudios : Nivel estudios del encuestado. 0=Nivel bajo, 1=Nivel superior\nAno_Nacimiento : Año de nacimiento del encuestado.\n\n\n\n\nSe pretende hacer una regresión logística que clasifique la variable respuesta MSE - Mejora de la Situación Económica del encuestado respecto a su adolescencia en casa- en función de varios predictores, tanto continuos (Ano_Nacimiento) como categóricos (Estado_civil, Nivel_Estudios).\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#dataset",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#dataset",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "",
    "text": "En este cuaderno vamos a analizar la existencia del llamado “ascensor social” en España a partir de los microdatos de la Encuesta de Condiciones de Vida (ECV) del año 2019. Dicho análisis nos sirve como excusa para tratar de mostrar en qué consiste una regresión logística y cómo llevarla a cabo en R.\nEn primer lugar, debemos definir qué es eso del “ascensor social” y cómo vamos a tratar de analizarlo nosotros. Éste se define como la posibilidad de ascender o descender de clase social. Aunque podemos considerar que la pertenencia a una determinada clase social - si es que éstas existen de forma estanca y perfectamente distinguible - se explica por una combinación de aspectos: nivel económico, cultural, de estudios, etc, nosotros nos vamos a centrar simplemente en el nivel económico.\nVer fichero Dataset_cleaning.rmd para ver cómo se han tratado dichos microdatos hasta obtener un conjunto de datos adecuado para la regresión logística, y que tomaremos aquí como punto de partida. En este cuaderno vamos a analizar el dataset llamado ECV.xlsx.\nConcretamente tenemos las siguientes variables:\n\nMSE (Mejora Situación Económica): Variable binaria, 1= ha mejorado la calidad de vida del encuestado respecto a su infancia, 0=no ha mejorado. teniendo en cuenta las nueve dimensiones.\nFactor_de_elevacion : Factor de elevación para cada observación respecto a la población total.\nEstado_civil : Estado civil del encuestado. 0=No casado, 1=Casado\nNivel_Estudios : Nivel estudios del encuestado. 0=Nivel bajo, 1=Nivel superior\nAno_Nacimiento : Año de nacimiento del encuestado.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "",
    "text": "Se pretende hacer una regresión logística que clasifique la variable respuesta MSE - Mejora de la Situación Económica del encuestado respecto a su adolescencia en casa- en función de varios predictores, tanto continuos (Ano_Nacimiento) como categóricos (Estado_civil, Nivel_Estudios).\n\nHacer un análisis exploratorio.\nIMPORTANTE: Convertir a factor las variables que lo sean.\nPlantear diversos modelos según variables incluidas.\nCompararlos con ANOVA y ROC CURVE.\nPara el modelo seleccionado, explicar los coeficientes, odds ratio,…",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#cargar-librerías",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#cargar-librerías",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer los excels\nlibrary(caret) # para la confusion matrix\nlibrary(Epi) # para la ROC curve",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#lectura-de-datos",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#lectura-de-datos",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Lectura de datos",
    "text": "Lectura de datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndatos1 &lt;- read_excel(\"../../../files/ECV.xlsx\", sheet = \"Datos\")",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#introducción-1",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#introducción-1",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Introducción",
    "text": "Introducción\nUn análisis de regresión logística es una técnica estadística multivariante que tiene como finalidad pronosticar o explicar los valores de una variable dependiente categórica a partir de una (regresión logística simple) o más (regresión logística múltiple) variables independientes categóricas o continuas. Dichas variables independientes reciben el nombre de covariables. Asimismo, a diferencia de lo que suele hacerse cuando tenemos una variable dependiente continua, cuando ésta es categórica, no interesa describir o pronosticar los valores concretos de dicha variable, sino la probabilidad de pertenecer a cada una de las categorías de la misma.\nAunque matemáticamente se pueda ajustar un modelo de regresión lineal clásico a la relación entre una variable dependiente categórica y una o varias covariables, cuando la variable dependiente es dicotómica (regresión logística binaria, caso más sencillo de regresión logística) no es apropiado utilizar un modelo de regresión lineal porque una variable dicotómica no se ajusta a una distribución normal, sino a una binomial. Ignorar esta cuestión podría llevar a obtener probabilidades imposibles: menores que cero o mayores que uno.\nPara evitar este problema, es preferible utilizar funciones que realicen predicciones comprendidas entre un máximo y un mínimo. Una de estas funciones - posiblemente la más empleada - es la curva logística o función sigmoide:\n\\[\\begin{align}\n\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\n\\end{align}\\]\nEs decir, estamos estimando con una regresión lineal el valor de \\(\\eta\\), que sí es una v.a. continua - a diferencia de Y que es binaria-.\nEsto es, \\(p=\\frac{e^\\eta}{1+e^\\eta}=\\frac{1}{1+e^{-\\eta}}\\). De esta forma, para valores positivos muy grandes de \\(\\eta\\) llamado odds, \\(e^{-\\eta}\\) es aproximadamente cero, por lo que el valor de la función es 1; mientras que para valores negativos muy grandes de \\(\\eta\\), \\(e^{-\\eta}\\) tiende a infinito, haciendo que el valor de la función sea 0.\nA continuación, para simplificar un poco las cosas, consideremos el modelo de regresión logística más sencillo: regresión logística binaria simple (una sola covariable):\n\\[ \\begin{align}\nP(Y=1)=\\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ϵ)}}\n\\end{align}\n\\]\nLa interpretación de esta función es muy similar a la de una regresión lineal: el coeficiente \\(\\beta_0\\) representa la posición de la curva sobre el eje horizontal o de abscisas (más hacia la izquierda o más hacia la derecha); mientras que \\(\\beta_1\\) representa la pendiente de la curva, es decir, cuán inclinada está en su parte central (cuanto más inclinada, mayor capacidad de discriminar entre los dos valores de la variable dependiente).\n\n\n\n\n\n\n\n\n\nEsta imagen, sacada del libro Análisis de datos en Ciencias Sociales y de la Salud III (Pardo et al., 2012) muestra tres regresiones logísticas con el mismo coeficiente , pero distinto poder discriminante (diferente ): la curva de la izquierda tendría un poder de discriminación perfecto, mientras que la de la derecha no discrimina nada (la del medio sería un caso intermedio entre los dos extremos). Por lo tanto, podemos decir que una covariable será mejor predictora cuanto mayor sea el coeficiente \\(\\beta\\) que genere. [Aclaración: en la imagen, \\(\\pi_1 = P(Y=1)\\)]\nSi estuviésemos ante una regresión logística múltiple, cada variable independiente recibiría una ponderación proporcional a su capacidad para predecir Y.\nEjemplo sencillo Vamos a mostrar como una variable binaria no tiene sentido predecirla con una Regresión Lineal sino Logística.\n\n# Generación de datos para el ejemplo\nset.seed(123)\nn &lt;- 200\nAltura &lt;- rnorm(n, mean = 165, sd = 10)\n\n# Crear una variable binaria 'Sexo' en función de Altura\nSexo &lt;- as.factor(ifelse(Altura + rnorm(n) &gt; 165, 1, 0))\ndatos_ejemplo &lt;- data.frame(Altura, Sexo)\n\n# Regresión lineal\nmodelo_lineal &lt;- lm(Sexo ~ Altura, data = datos_ejemplo)\n\n# Regresión logística\nmodelo_logistico &lt;- glm(Sexo ~ Altura, data = datos_ejemplo, family = binomial)\n\n\n# Regresión Lineal\npar(mfrow = c(1, 2))\nplot(datos_ejemplo$Altura, datos_ejemplo$Sexo, col = \"lightblue\", main = \"Ajuste por Regresión Lineal\")\nabline(modelo_lineal, col = \"navy\")\n\n# Regresión Logística\n\nplot(datos_ejemplo$Altura, as.numeric(datos_ejemplo$Sexo) - 1, col = \"lightblue\", main = \"Regresión Logística\", xlab = \"Altura\", ylab = \"Sexo\")\ncurve(predict(modelo_logistico, data.frame(Altura = x), type = \"response\"), add = TRUE, col = \"navy\", lwd = 2)\n\n\n\n\n\n\n\n\nEn este ejemplo, se muestra cómo un ajuste por regresión lineal no se adapta bien a datos binarios, produciendo predicciones que pueden ser mayores que 1 o menores que 0. En cambio, la regresión logística produce una curva en forma de S que se adapta mejor a los datos, con predicciones que están siempre entre 0 y 1. Esto demuestra que para problemas de clasificación binaria, la regresión logística es una mejor opción que la regresión lineal.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#bondad-de-ajuste-e-interpretación-modelo",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#bondad-de-ajuste-e-interpretación-modelo",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Bondad de Ajuste e Interpretación Modelo",
    "text": "Bondad de Ajuste e Interpretación Modelo\n\nInterpretación Modelo\nRecordar que el modelo tomaba la forma \\[\\eta=\\log \\left(\\frac{p}{1-p}\\right)= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots , \\quad \\text{with } \\quad p=P(Y=1)\\], es decir, estamos estimando el log(odds). Esto nos lleva a las siguientes apreciaciones:\nAunque tanto \\(P(Y=1)\\), como \\(Odds(Y=1)\\), como \\(\\operatorname{logit}(Y=1)\\) expresan la misma idea, están en distinta escala:\n\nLa probabilidad toma valores comprendidos entre 0 y 1.\nLa odds tiene un valor mínimo de cero y no tiene máximo.\nLa logit o log(odds) no tiene ni mínimo ni máximo.\n\nPor ejemplo, a una probabilidad de 0,5, le corresponde una odds de 1 y un logit de 0. Ahora bien, es cierto que razonar en términos de cambios en los logaritmos resulta poco intuitivo. Por ello, es preferible interpretar el cambio en las odds o en la razón de ventajas (también llamada odds ratio, razón de probabilidades o razón de momios).\nLa interpretación más frecuente es interpretar los signos de los coeficientes del modelo, es decir, los signos de \\(\\beta_1, \\ldots , \\beta_k\\).\n\nSi \\(\\beta_i &gt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en un aumento de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) aumenta, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\nSi \\(\\beta_i &lt;0\\) , se traduce en que un aumento de una unidad en la variable \\(x_i\\) -si es continua- o un cambio de categoría -si \\(x_i\\) es categórica- se traduce en una disminución de \\(\\beta_i\\) unidades el valor de logit. Es decir, la probabilidad \\(p\\) (que Y=1) disminuye, en función de \\[p=\\frac{e^\\eta}{1+e^\\eta}\\].\n\n\nUna pregunta importante en cualquier análisis de regresión es si el modelo propuesto se ajusta adecuadamente a los datos, lo que conduce naturalmente a la noción de una prueba formal para la falta de ajuste (o bondad de ajuste).\n\n\nMedidas Especifidad y Sensibilidad\nLa especificidad y la sensibilidad son medidas utilizadas para evaluar el rendimiento de un modelo predictivo, especialmente en problemas de clasificación binaria (donde solo hay dos clases). Las definimos como:\n\nSensibilidad (Sensitivity): Es la proporción de verdaderos positivos (casos positivos correctamente identificados) respecto al total de casos positivos reales. Es la capacidad del modelo para identificar correctamente los casos positivos.\nEspecificidad (Specificity): Es la proporción de verdaderos negativos (casos negativos correctamente identificados) respecto al total de casos negativos reales. Representa la capacidad del modelo para identificar correctamente los casos negativos.\n\nUn equilibrio entre ambas es deseable, pero depende del contexto específico del problema y de las consecuencias de los falsos positivos y falsos negativos. En el caso, por ejemplo, de detectar si un paciente tiene cáncer o no, parece más razonable centrarse en los Falsos Negativos, ya que un paciente que tiene cáncer no lo estamos detectando, lo que lleva un riesgo implícito muy alto.\n\n\n\n\n\n\n\n\n\n\nClasificado como Positivo\nClasificado como Negativo\nTotal\n\n\n\n\nRealmente Positivo\nVerdadero Positivo (VP)\nFalso Negativo (FN)\nVP + FN\n\n\nRealmente Negativo\nFalso Positivo (FP)\nVerdadero Negativo (VN)\nFP + VN\n\n\nTotal\nVP + FP\nFN + VN\n\n\n\n\nSensibilidad ( )\nEspecificidad: ( )\n\n\nCurva ROC\nLa curva ROC es una representación gráfica de la sensibilidad frente a la tasa de falsos positivos a varios umbrales de clasificación. Se utiliza comúnmente en análisis de clasificación para evaluar el rendimiento de un modelo.\nPara calcular el área bajo la curva ROC (AUC-ROC), se utiliza la tasa de falsos positivos y de falsos negativos. Cuanto más cerca esté el AUC-ROC de 1, mejor será el rendimiento del modelo, ya que indica una mayor capacidad de distinguir entre clases.\nEs una medida de bondad porque evalúa qué tan bien puede discriminar un modelo entre las clases positivas y negativas. Cuanto más se acerque el AUC a 1, mejor será la capacidad del modelo para distinguir entre las clases. Se utiliza para comparar y seleccionar modelos, donde un AUC mayor indica un mejor rendimiento predictivo.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#formulación",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#formulación",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Formulación",
    "text": "Formulación\nIMPORTANTE: Convertir a factor las variables que tengan que ser tratadas como tal, de lo contrario R las tratará como numéricas. Además, la variable respuesta debe tener los niveles codificados como \\(0\\) y \\(1\\) para poder usar la función glm.\n\ndatos1$Estado_civil &lt;- as.factor(datos1$Estado_civil)\ndatos1$Nivel_Estudios &lt;- as.factor(datos1$Nivel_Estudios)\ndatos1$MSE &lt;- as.factor(datos1$MSE)\n\nA continuación presentamos tres posibles modelos y posteriormente elegiremos uno de ellos.\n\nlmod1 : Queremos clasificar la MSE en función del Año de Nacimiento de la persona (numérica).\nlmod2 : Queremos clasificar la MSE en función del Año de Nacimiento de la persona (numérica) y el Estado_civil (categórica).\nlmod3 : Queremos clasificar la MSE en función del Año de Nacimiento de la persona (numérica), el Estado_civil (categórica) y el Nivel de Estudios (categórica) y .\n\n\n# lmod1\nlmod1 &lt;- glm(formula = MSE ~ Ano_Nacimiento, family = binomial(link = logit), data = datos1)\nsummary(lmod1)\n\n\nCall:\nglm(formula = MSE ~ Ano_Nacimiento, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    57.139246   4.361655   13.10   &lt;2e-16 ***\nAno_Nacimiento -0.029717   0.002211  -13.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16535  on 17462  degrees of freedom\nResidual deviance: 16348  on 17461  degrees of freedom\nAIC: 16352\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod2\nlmod2 &lt;- glm(formula = MSE ~ Ano_Nacimiento + Estado_civil, family = binomial(link = logit), data = datos1)\nsummary(lmod2)\n\n\nCall:\nglm(formula = MSE ~ Ano_Nacimiento + Estado_civil, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    49.122827   4.581715  10.721  &lt; 2e-16 ***\nAno_Nacimiento -0.025739   0.002319 -11.101  &lt; 2e-16 ***\nEstado_civil1   0.264739   0.043272   6.118 9.48e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16535  on 17462  degrees of freedom\nResidual deviance: 16310  on 17460  degrees of freedom\nAIC: 16316\n\nNumber of Fisher Scoring iterations: 4\n\n# lmod3\nlmod3 &lt;- glm(formula = MSE ~ Ano_Nacimiento + Estado_civil + Nivel_Estudios, family = binomial(link = logit), data = datos1)\nsummary(lmod3)\n\n\nCall:\nglm(formula = MSE ~ Ano_Nacimiento + Estado_civil + Nivel_Estudios, \n    family = binomial(link = logit), data = datos1)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     50.045838   4.629712  10.810  &lt; 2e-16 ***\nAno_Nacimiento  -0.026218   0.002344 -11.183  &lt; 2e-16 ***\nEstado_civil1    0.262245   0.043303   6.056  1.4e-09 ***\nNivel_Estudios1 -0.006125   0.052153  -0.117    0.907    \nNivel_Estudios2  0.065640   0.045581   1.440    0.150    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16535  on 17462  degrees of freedom\nResidual deviance: 16307  on 17458  degrees of freedom\nAIC: 16317\n\nNumber of Fisher Scoring iterations: 4\n\n\nEn este caso, el Modelo 2 tiene el AIC más bajo, lo que sugiere que podría ser el mejor ajuste entre los tres modelos. Sin embargo, es importante considerar otros aspectos y realizar pruebas adicionales si es necesario para validar el modelo seleccionado. Por otro lado, en términos de la Deviance podemos ver cosas parecidas. Vemos que al lmod1 si le añadimos la variable Estado_civil, la residual variance disminuye lo que parece indicar que está variable es significante. Sin embargo, cuando le añadimos la variable Nivel_Estudios, la disminución de la deviance es muy baja por lo que no parece que sea del todo relevante en función del resto de variables.\nPara este modelo vamos a calcular la matriz de confusión y el área ROC.\n\n# confusion matrices\npredicted2 &lt;- predict(lmod2, datos1[, c(\"Ano_Nacimiento\", \"Estado_civil\", \"Nivel_Estudios\")], type = \"response\")\nconfusionMatrix(data = as.factor(ifelse(predicted2 &gt; 0.179, 1, 0)), reference = datos1$MSE, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7120 1183\n         1 7176 1984\n                                          \n               Accuracy : 0.5213          \n                 95% CI : (0.5139, 0.5288)\n    No Information Rate : 0.8186          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0717          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6265          \n            Specificity : 0.4980          \n         Pos Pred Value : 0.2166          \n         Neg Pred Value : 0.8575          \n             Prevalence : 0.1814          \n         Detection Rate : 0.1136          \n   Detection Prevalence : 0.5245          \n      Balanced Accuracy : 0.5623          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n# The ROC function\n\n# lmod2\nROC(form = MSE ~ Estado_civil + Ano_Nacimiento, data = datos1, plot = \"ROC\", lwd = 3, cex = 1.5)\n\n\n\n\n\n\n\n\nObservamos una Especifidad del 50% y una Sensibilidad del 62%. Esto quiere decir que nuestro modelo es mejor evitando falsos negativos, que falsos positivos Es decir, que es peor evitando que digamos que la calidad de vida de una persona ha mejorado cuando realmente no lo ha hecho, que al revés.\nDestacar que el elemento Ir.eta que aparece arriba, es el punto de corte de la probabilidad. Es decir, si nuestra regresión logística predice que hay una probabilidad mayor de \\(0.179\\) de que haya mejorado la calidad de vida, nosotros lo clasificamos como que efectivamente ha mejorado y si es menor, lo clasificamos como que no.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#otras-consideraciones",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#otras-consideraciones",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Otras consideraciones",
    "text": "Otras consideraciones\nPodemos usar el presente modelo para predecir la probabilidad de cesárea en función de las variables predictoras de nuevas observaciones.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/ECV/ECV.html#interpretación-coeficientes",
    "href": "notebooks/Logistic Regression/ECV/ECV.html#interpretación-coeficientes",
    "title": "Regresión Logística: ECV_microdatos",
    "section": "Interpretación coeficientes",
    "text": "Interpretación coeficientes\nVamos a volver a sacar el summary del modelo para proceder a explicar todo bien de nuevo.\n\nsummary(lmod2)\n\n\nCall:\nglm(formula = MSE ~ Ano_Nacimiento + Estado_civil, family = binomial(link = logit), \n    data = datos1)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    49.122827   4.581715  10.721  &lt; 2e-16 ***\nAno_Nacimiento -0.025739   0.002319 -11.101  &lt; 2e-16 ***\nEstado_civil1   0.264739   0.043272   6.118 9.48e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16535  on 17462  degrees of freedom\nResidual deviance: 16310  on 17460  degrees of freedom\nAIC: 16316\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nAno_Nacimiento: Por cada incremento unitario en el año de nacimiento, el logaritmo de odds de éxito en MSE disminuye aproximadamente en 0.026, manteniendo constante el Estado Civil.\nEstado_civil1: Cuando el estado civil cambia de 0 (soltero) a 1 (casado), el logaritmo de odds de éxito en MSE aumenta aproximadamente en 0.265, manteniendo constante el año de nacimiento.\n\nInterpretación coeficiente Año Nacimiento\nEl rango del año de nacimiento abarca desde 1960 hasta 1993. La disminución en el logaritmo de odds de éxito en la mejora de la situación económica por cada año de aumento en el año de nacimiento, implica una disminución en la probabilidad de asociar esa persona a una mejora de la calidad de vida. Esto puede asociarse con el contexto histórico y socioeconómico de esas generaciones.\nLas personas nacidas en la década de 1960 provienen de un contexto en el que sus padres experimentaron condiciones económicas precarias, con la transición al estado de bienestar en esa época. A medida que avanzaron en su vida adulta, se beneficiaron considerablemente de ese cambio y disfrutaron de una mejor calidad de vida en comparación con su infancia.\nSin embargo, para las generaciones más recientes, nacidas en los años 80 y 90, ya crecieron en un entorno más establecido de bienestar. Es probable que no hayan experimentado cambios tan significativos en su calidad de vida a medida que envejecían. De hecho, es posible que en los últimos años, hayan presenciado un deterioro en las condiciones económicas, lo que explicaría la tendencia a una disminución en el logaritmo de odds de éxito en la mejora de la situación económica.\nInterpretación coeficiente Estado Civil\nEl coeficiente positivo para el estado civil, donde 0 representa soltero y 1 casado, sugiere que las personas casadas tienen una tendencia más positiva hacia la mejora de su situación económica en comparación con las personas solteras. Esto puede deberse a varios factores.\nEn contextos culturales y económicos, las personas casadas pueden tener una mayor estabilidad financiera y apoyo mutuo, lo que les permite tener un respaldo emocional y económico más sólido para enfrentar desafíos financieros. Además, el matrimonio suele implicar compartir responsabilidades financieras y planificación conjunta, lo que puede conducir a decisiones más estables y a largo plazo en términos económicos.\nNo obstante, esto es algo muy generalizado que puede que no sea así ya que la situación económica depende de muchos factores.",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "ECV",
      "Regresión Logística: ECV_microdatos"
    ]
  },
  {
    "objectID": "notebooks/Logistic Regression/salud/Dataset_cleaning.html",
    "href": "notebooks/Logistic Regression/salud/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente discriminar el sexo de una persona a partir de los microdatos de la Encuesta Nacional de Salud. Resultados Concretamente, se han tomado los datos relativos a 2017.\nLos microdatos de la Encuesta 2017 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176783&menu=resultados&secc=1254736195295&idp=1254735573175#!tabs-1254736195295. Tomamos el fichero relativo a Adultos (15 años y más). De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de un anaálisis de discriminante y el nombre que tomará en el dataset exportado.\n\nEDADa: Identificación del adulto seleccionado: Edad.\nSEXOa: Identificación del adulto seleccionado: Sexo.\nS109: Altura en cm.\nS110: Peso en kg.\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Microdatos\nlibrary(readxl)\ndatos &lt;- read_excel(\"/Users/davpero/Downloads/BECA/datos_ensalud17_xlsx/MICRODAT.CA.xlsx\")\ndatos &lt;- datos[, c(\"EDADa\", \"SEXOa\", \"S109\", \"S110\")]\nlibrary(dplyr)\ndatos &lt;- na.omit(datos)\n\n\n\n\n# Conevrtimos a factor variable sexo y la renombramos\ndatos$SEXO &lt;- as.factor(ifelse(datos$SEXOa == 1, 1, 0))\n\n# Numéricas las demas y las renombramos\ndatos$EDAD &lt;- as.numeric(datos$EDADa)\ndatos$Altura &lt;- as.numeric(datos$S109)\ndatos$Peso &lt;- as.numeric(datos$S110)\n\n# Los valores atípicos los quitamos, los que tienen un peso y una altura que no tiene sentido\ndatos$Peso[datos$Peso &gt; 220] &lt;- NA\ndatos$Altura[datos$Altura &gt; 220] &lt;- NA\ndatos &lt;- na.omit(datos)\n\n\ndatos &lt;- datos[, c(\"EDAD\", \"SEXO\", \"Altura\", \"Peso\")]\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos, \"../../../files/salud.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. salud.xlsx.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Logistic Regression",
      "Salud",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando el método de Componentes Principales. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado variado_provincias.xlsx. Este dataset presenta datos para las provincias, abordando información diversa sobre variables socioeconómicas. Contiene datos como número de explotaciones agrícolas, superficies agrícolas, indicadores de empleo, ejecuciones hipotecarias, empresas por sector, PIB per cápita y datos demográficos.\nConcretamente en este dataset tenemos las siguientes variables:\n\nprov: Nombre de la provincia o ciudad autónoma.\nneag: Número de explotaciones agrícolas (2009).\nsagt: Superficie agrícola total (2009).\nsagu: Superficie agrícola utilizada (2009).\nbenf: Beneficiarios de prestaciones por desempleo (2019).\nacem: Importe de todos los programas de Apoyo a la Creación de Empleo (2019).\nacti: Personas en situación laboral activa (2019).\nocup: Personas en situación laboral de ocupación (2019).\npara: Personas en situación laboral de desempleo (2019).\nejec: Ejecuciones hipotecarias de fincas urbanas de tipo vivienda (2019).\ninds: Número de empresas dentro del sector industrial (2020).\ncnst: Número de empresas dentro del sector de la construcción (2020).\nctrh: Número de empresas dentro del sector del Comercio, el transporte y la hostelería (2020).\nserv: Número de empresas dentro del sector servicios (2020).\npibc: Producto Interior Bruto per Cápita (2018).\nipib: Indice o ratio de PIB per cápita siendo el valor para el conjunto del Estado Español 1.\ninmi: Personas nacidas fuera de España y nacionalidad distinta a la española que inmigran instalándose en la provincia.\npobl: Población total independientemente de la nacionalidad.\nespa: Población de nacionalidad española.\nd_migr: Provincia de alta recepción de inmigración.\nd_prod: Provincia con un PIB per cápita superior al del conjunto del Estado.\nd_cons: Provincia con alta dedicación a la construcción.\nm_ieag: Intensividad de explotación agrícola.\n\n\n\n\nSe pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar las variables socioeconómicas de las provincias y ver cuáles son las más parecidas y las más diferentes.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#dataset",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#dataset",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado variado_provincias.xlsx. Este dataset presenta datos para las provincias, abordando información diversa sobre variables socioeconómicas. Contiene datos como número de explotaciones agrícolas, superficies agrícolas, indicadores de empleo, ejecuciones hipotecarias, empresas por sector, PIB per cápita y datos demográficos.\nConcretamente en este dataset tenemos las siguientes variables:\n\nprov: Nombre de la provincia o ciudad autónoma.\nneag: Número de explotaciones agrícolas (2009).\nsagt: Superficie agrícola total (2009).\nsagu: Superficie agrícola utilizada (2009).\nbenf: Beneficiarios de prestaciones por desempleo (2019).\nacem: Importe de todos los programas de Apoyo a la Creación de Empleo (2019).\nacti: Personas en situación laboral activa (2019).\nocup: Personas en situación laboral de ocupación (2019).\npara: Personas en situación laboral de desempleo (2019).\nejec: Ejecuciones hipotecarias de fincas urbanas de tipo vivienda (2019).\ninds: Número de empresas dentro del sector industrial (2020).\ncnst: Número de empresas dentro del sector de la construcción (2020).\nctrh: Número de empresas dentro del sector del Comercio, el transporte y la hostelería (2020).\nserv: Número de empresas dentro del sector servicios (2020).\npibc: Producto Interior Bruto per Cápita (2018).\nipib: Indice o ratio de PIB per cápita siendo el valor para el conjunto del Estado Español 1.\ninmi: Personas nacidas fuera de España y nacionalidad distinta a la española que inmigran instalándose en la provincia.\npobl: Población total independientemente de la nacionalidad.\nespa: Población de nacionalidad española.\nd_migr: Provincia de alta recepción de inmigración.\nd_prod: Provincia con un PIB per cápita superior al del conjunto del Estado.\nd_cons: Provincia con alta dedicación a la construcción.\nm_ieag: Intensividad de explotación agrícola.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "",
    "text": "Se pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar las variables socioeconómicas de las provincias y ver cuáles son las más parecidas y las más diferentes.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(factoextra) # fviz_cluster function\nlibrary(ggcorrplot) # Para funcion ggcorrplot\nlibrary(corrplot) # Para corrplot",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#lectura-datos",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../../files/provincias_variado.xlsx\", sheet = \"Datos\")\n\nEn primer lugar, cargamos los datos que vamos a utilizar. En este caso, se trata de un conjunto de datos compuesto por 52 filas y 23 columnas. Las filas corresponden a las 50 provincias de España, más Ceuta y Melilla.\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores faltantes, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación, no hay ningún NA en los datos que vamos a utilizar.\n\nsum(is.na(datos))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(datos)\n\n[1] 52 23\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [52 × 23] (S3: tbl_df/tbl/data.frame)\n $ prov  : chr [1:52] \"Almería\" \"Cádiz\" \"Córdoba\" \"Granada\" ...\n $ neag  : chr [1:52] \"22697\" \"10069\" \"36641\" \"41243\" ...\n $ sagt  : chr [1:52] \"327346\" \"540571\" \"1024515\" \"75827\" ...\n $ sagu  : chr [1:52] \"234621\" \"424849\" \"844019\" \"625674\" ...\n $ benf  : num [1:52] 38299 75582 62023 60393 41255 ...\n $ acem  : chr [1:52] \"4383367.9299999988\" \"5611899.9800000004\" \"9458693.4799999986\" \"4635804.6700000009\" ...\n $ acti  : num [1:52] 341 563 371 429 250 ...\n $ ocup  : num [1:52] 281 423 285 334 195 ...\n $ para  : num [1:52] 67.2 155 95.8 105.6 58.9 ...\n $ ejec  : num [1:52] 2026 454 193 906 269 ...\n $ inds  : num [1:52] 2135 2982 4196 3494 1378 ...\n $ cnst  : num [1:52] 5703 5916 5198 7240 2682 ...\n $ ctrh  : num [1:52] 18704 27266 20196 24650 11559 ...\n $ serv  : num [1:52] 17550 27210 18736 26000 10136 ...\n $ pibc  : num [1:52] 19919 18050 18525 18181 20273 ...\n $ ipib  : num [1:52] 0.773 0.7 0.719 0.705 0.787 ...\n $ inmi  : num [1:52] 8013 6119 2966 6628 3830 ...\n $ pobl  : num [1:52] 716820 1240155 782979 914678 521870 ...\n $ espa  : num [1:52] 570912 1194830 761876 853709 477032 ...\n $ d_migr: num [1:52] 1 0 0 0 0 0 1 0 1 0 ...\n $ d_prod: num [1:52] 0 0 0 0 0 0 0 0 1 0 ...\n $ d_cons: num [1:52] 0 0 0 0 0 0 0 0 1 1 ...\n $ m_ieag: num [1:52] 0 1 1 1 -1 -1 -1 1 0 0 ...\n\n# Descripción de las variables\nsummary(datos)\n\n     prov               neag               sagt               sagu          \n Length:52          Length:52          Length:52          Length:52         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n      benf            acem                acti              ocup       \n Min.   :  2073   Length:52          Min.   :  38.05   Min.   :  27.8  \n 1st Qu.:  9124   Class :character   1st Qu.: 149.77   1st Qu.: 133.6  \n Median : 24705   Mode  :character   Median : 276.54   Median : 241.2  \n Mean   : 35790                      Mean   : 442.83   Mean   : 380.4  \n 3rd Qu.: 49637                      3rd Qu.: 493.32   3rd Qu.: 425.2  \n Max.   :176384                      Max.   :3467.15   Max.   :3098.9  \n      para             ejec             inds            cnst      \n Min.   :  3.50   Min.   :   2.0   Min.   :   73   Min.   :  294  \n 1st Qu.: 17.77   1st Qu.:  64.5   1st Qu.: 1650   1st Qu.: 2752  \n Median : 42.92   Median : 255.0   Median : 2752   Median : 4992  \n Mean   : 68.86   Mean   : 528.6   Mean   : 3762   Mean   : 8079  \n 3rd Qu.: 77.59   3rd Qu.: 683.2   3rd Qu.: 4196   3rd Qu.: 8314  \n Max.   :411.68   Max.   :2859.0   Max.   :26217   Max.   :60387  \n      ctrh             serv             pibc            ipib       \n Min.   :  1960   Min.   :  1518   Min.   :18050   Min.   :0.7004  \n 1st Qu.:  8479   1st Qu.:  8869   1st Qu.:20268   1st Qu.:0.7864  \n Median : 15599   Median : 15584   Median :22399   Median :0.8692  \n Mean   : 23349   Mean   : 30280   Mean   :23960   Mean   :0.9297  \n 3rd Qu.: 27100   3rd Qu.: 30358   3rd Qu.:27906   3rd Qu.:1.0829  \n Max.   :158817   Max.   :311858   Max.   :36404   Max.   :1.4126  \n      inmi            pobl              espa             d_migr   \n Min.   :  255   Min.   :  84777   Min.   :  73221   Min.   :0.0  \n 1st Qu.: 1798   1st Qu.: 326390   1st Qu.: 299280   1st Qu.:0.0  \n Median : 3268   Median : 607321   Median : 559604   Median :0.5  \n Mean   : 9182   Mean   : 904350   Mean   : 807487   Mean   :0.5  \n 3rd Qu.: 8871   3rd Qu.:1025346   3rd Qu.: 917612   3rd Qu.:1.0  \n Max.   :97106   Max.   :6663394   Max.   :5781575   Max.   :1.0  \n     d_prod           d_cons           m_ieag        \n Min.   :0.0000   Min.   :0.0000   Min.   :-1.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:-1.00000  \n Median :0.0000   Median :0.0000   Median : 0.00000  \n Mean   :0.3462   Mean   :0.4808   Mean   : 0.03846  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 1.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   : 1.00000  \n\n\nVemos que hay alguna observación como Ceuta y Melilla con datos faltantes, luego los eliminamos y posteriormente convertimos a variable numérica las que han sido consideradas como carácter(debido a los fallos para Ceuta y Melilla) y deberían ser variables numéricas.\n\n# Ceuta y Melilla tienen datos faltantes luego eliminamos observaciones\ndatos &lt;- datos[datos$prov != \"Ceuta\", ]\ndatos &lt;- datos[datos$prov != \"Melilla\", ]\n\n# Convertimos a variables numéricas\ndatos$acem &lt;- as.numeric(datos$acem)\ndatos$neag &lt;- as.numeric(datos$neag)\ndatos$sagt &lt;- as.numeric(datos$sagt)\ndatos$sagu &lt;- as.numeric(datos$sagu)\n\nCorrelación: El que existan correlaciones muy elevadas en el conjunto de datos nos permitirá resumir la información en un menor número de componentes principales, pues éstas explicarán una mayor cantidad de información.\n\nggcorrplot(cor(datos[, 2:23]), type = \"lower\", lab = T, show.legend = T, lab_size = 1.9)\n\n\n\n\n\n\n\n\nEn este caso, se ha generado un gráfico entre variables, sin tener en cuenta la correlación de la variable consigo misma, pues siempre será del 100%. En términos absolutos, vemos lo siguiente:\n\nLas variables sagt y sagu presentan una correlación alta lo que parece razonable. puesto que ambas están relacionadas con superficies agrícolas.\nPor otro lado, se encuentran también bastante correlacionadas entre ellas benf, acem, acti, ocup, para, ejec, inds, cnst y ctrh, las cuales hablan sobre la actividad económica, el mercado laboral y la situación financiera.\nAdemás, las variables inmi, pobl, espa que hablan sobre población se encuentran correlacionadas con las variables socioeconómicas citadas antes.\n\nEn general es un dataset que presenta variables con bastante correlación luego esto, probablemente, nos permitirá reducir bastante el numero de variables finales.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#introducción-1",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de Componentes Principales (PCA) es una técnica para reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nSe busca encontrar nuevas variables (componentes) que estén incorrelacionadas y que capturen la máxima variabilidad de los datos. Esto se logra mediante combinaciones lineales de las variables originales. PCA es útil para entender relaciones, reducir dimensiones y manejar la alta correlación entre variables.\nPara aplicar PCA, se necesitan datos cuantitativos y es crucial escalar las variables (estandarizar = media cero y varianza uno). Esto garantiza que ninguna variable domine el análisis. Además, se puede trabajar con la matriz de correlaciones para abordar fuertes correlaciones entre variables, manteniendo así la información más relevante del conjunto de datos.\nLos pasos generales son:\n\nEstandarización de las variables: Es importante estandarizar las variables numéricas para que tengan media cero y desviación estándar uno. Esto es crucial para que ninguna variable domine el análisis debido a su escala.\nCálculo de la matriz de correlaciones o covarianzas: Dependiendo del enfoque, se puede trabajar con la matriz de correlaciones si se busca abordar fuertes correlaciones entre variables, o con la matriz de covarianzas si se busca la varianza total de las variables.\n\n\nNOTA: Aconsejable trabajar siempre con la matriz de correlaciones (a no ser que todas variables estén en las mismas unidades, que se podrá usar la matriz de covarianzas). De no seguir esta nota y usar la matriz de covarianzas, las variables que tienen mayores unidades dominarán la estructura de covarianza, lo que llevará a una representación inexacta de la variabilidad real de los datos.\n\n\nDescomposición de la matriz: Se descompone la matriz de correlaciones en sus vectores y valores propios. Los valores propios representan la cantidad de varianza explicada por cada componente principal, mientras que los vectores propios (autovectores) determinan la dirección de cada componente en el espacio multidimensional original.\nSelección de componentes: Los componentes se ordenan de manera descendente según la cantidad de varianza que explican. Los primeros componentes capturan la mayor variabilidad de los datos y se seleccionan para reducir la dimensionalidad manteniendo la información más relevante.\nTransformación de datos: Proyectar los datos originales en el espacio de los componentes principales para obtener las nuevas variables. Estas son combinaciones lineales de las variables originales y son ortogonales entre sí. Esta transformación lineal conserva la mayor parte de la información en un espacio de menor dimensión, lo que facilita el análisis y la visualización de los datos.\nInterpretación y visualización: Explorar la importancia de cada componente en términos de la variabilidad explicada. Se pueden interpretar los componentes para comprender qué aspectos de los datos capturan. Si es posible, representar gráficamente los datos en el espacio reducido de los componentes principales para obtener una mejor comprensión de las relaciones entre las observaciones.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#modelo",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#modelo",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Modelo",
    "text": "Modelo\nEn las siguientes lineas haremos que la variable prov se ponga como nombre de filas y posteriormente eliminaremos esa variable ya que ya la tendremos como nombre de filas.\n\nprov &lt;- datos$prov\ndatos &lt;- datos[, -1] # Eliminamos ahora\nrownames(datos) &lt;- prov # Como nombres de filas las provincias\n\nEscalamos los datos y calculamos la matriz de varianzas covarianzas, mostramos solo la diagonal (debería ser 1).\n\ndatos2 &lt;- scale(datos)\nsummary(datos2)\n\n      neag              sagt              sagu              benf        \n Min.   :-1.1411   Min.   :-1.3485   Min.   :-1.1733   Min.   :-0.8942  \n 1st Qu.:-0.6411   1st Qu.:-0.7553   1st Qu.:-0.7755   1st Qu.:-0.6871  \n Median :-0.3091   Median :-0.1222   Median :-0.1540   Median :-0.2694  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.5315   3rd Qu.: 0.7954   3rd Qu.: 0.5506   3rd Qu.: 0.3504  \n Max.   : 3.3341   Max.   : 2.7318   Max.   : 3.0203   Max.   : 3.5591  \n      acem                acti               ocup               para        \n Min.   :-0.617472   Min.   :-0.67366   Min.   :-0.64655   Min.   :-0.8301  \n 1st Qu.:-0.479423   1st Qu.:-0.49119   1st Qu.:-0.46230   1st Qu.:-0.6437  \n Median :-0.283360   Median :-0.26519   Median :-0.27382   Median :-0.3165  \n Mean   : 0.000000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.007602   3rd Qu.: 0.09655   3rd Qu.: 0.06362   3rd Qu.: 0.1086  \n Max.   : 4.544298   Max.   : 4.88489   Max.   : 4.94493   Max.   : 4.1806  \n      ejec              inds               cnst               ctrh         \n Min.   :-0.7899   Min.   :-0.73202   Min.   :-0.68723   Min.   :-0.72021  \n 1st Qu.:-0.6860   1st Qu.:-0.47413   1st Qu.:-0.48766   1st Qu.:-0.50005  \n Median :-0.4180   Median :-0.23058   Median :-0.29068   Median :-0.27136  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.3064   3rd Qu.: 0.06218   3rd Qu.: 0.02612   3rd Qu.: 0.09904  \n Max.   : 3.3679   Max.   : 4.80833   Max.   : 4.72343   Max.   : 4.40407  \n      serv               pibc              ipib              inmi          \n Min.   :-0.53903   Min.   :-1.2490   Min.   :-1.2490   Min.   :-0.531467  \n 1st Qu.:-0.41091   1st Qu.:-0.7794   1st Qu.:-0.7794   1st Qu.:-0.441624  \n Median :-0.27930   Median :-0.3078   Median :-0.3078   Median :-0.345592  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000000  \n 3rd Qu.:-0.01928   3rd Qu.: 0.7876   3rd Qu.: 0.7876   3rd Qu.:-0.007037  \n Max.   : 5.16964   Max.   : 2.5159   Max.   : 2.5159   Max.   : 5.113423  \n      pobl               espa              d_migr            d_prod       \n Min.   :-0.70681   Min.   :-0.72798   Min.   :-0.9899   Min.   :-0.7425  \n 1st Qu.:-0.50535   1st Qu.:-0.50227   1st Qu.:-0.9899   1st Qu.:-0.7425  \n Median :-0.24426   Median :-0.24694   Median : 0.0000   Median :-0.7425  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.07776   3rd Qu.: 0.09517   3rd Qu.: 0.9899   3rd Qu.: 1.3199  \n Max.   : 4.77031   Max.   : 4.76444   Max.   : 0.9899   Max.   : 1.3199  \n     d_cons            m_ieag    \n Min.   :-0.9899   Min.   :-1.2  \n 1st Qu.:-0.9899   1st Qu.:-1.2  \n Median : 0.0000   Median : 0.0  \n Mean   : 0.0000   Mean   : 0.0  \n 3rd Qu.: 0.9899   3rd Qu.: 1.2  \n Max.   : 0.9899   Max.   : 1.2  \n\ndiag(var(datos2))\n\n  neag   sagt   sagu   benf   acem   acti   ocup   para   ejec   inds   cnst \n     1      1      1      1      1      1      1      1      1      1      1 \n  ctrh   serv   pibc   ipib   inmi   pobl   espa d_migr d_prod d_cons m_ieag \n     1      1      1      1      1      1      1      1      1      1      1 \n\n\nAplicamos funcion PCA, notar que en este caso no haría falta los argumentos SCALE=TRUE y CENTER=TRUE puesto que ya hemos escalado dos datos en un paso previo. Por defecto en la función viene el valor de SCALE=FALSE y CENTER=TRUE.\n\npca &lt;- prcomp(datos2, center = TRUE, scale = TRUE) # Scale=T\n\nCalculamos los coeficientes de la ecuación para cada componente principal (Autovectores)\n\npca$rotation\n\n                PC1          PC2         PC3          PC4         PC5\nneag   -0.008203669 -0.343875267  0.04530181  0.225703033 -0.54756484\nsagt    0.093777760 -0.260685812  0.46692515  0.349015155  0.03406050\nsagu    0.065567030 -0.244985221  0.61799140  0.097155168  0.02080531\nbenf   -0.255547541 -0.169583054 -0.04259558 -0.052834575 -0.02213471\nacem   -0.267132603 -0.057835771  0.10183034  0.045780063  0.08580010\nacti   -0.281063977 -0.023526873  0.02628693  0.007544727  0.07970706\nocup   -0.280841146 -0.008682980  0.03451402  0.018319293  0.08680006\npara   -0.266087680 -0.130432450 -0.03284853 -0.075401384  0.02454609\nejec   -0.198849271 -0.099263733 -0.15594933  0.165381576 -0.45691677\ninds   -0.273655544 -0.025426864  0.02639828  0.104890737 -0.03003321\ncnst   -0.278652744  0.009442489  0.01045498  0.065833031  0.09797438\nctrh   -0.280865532 -0.034425301 -0.00195234  0.028043173  0.06635140\nserv   -0.278225349  0.001691252  0.04777668  0.010808461  0.13573865\npibc   -0.091357800  0.443739119  0.23824009 -0.056976256 -0.09063192\nipib   -0.091357800  0.443739119  0.23824009 -0.056976256 -0.09063192\ninmi   -0.276330714  0.024554052  0.01362495  0.036421615  0.07966626\npobl   -0.281059234 -0.031986886  0.01745440  0.009483160  0.07141424\nespa   -0.279916693 -0.041995882  0.02242889 -0.001685268  0.07871296\nd_migr -0.095504531  0.238686852 -0.03617703  0.252839323 -0.46455287\nd_prod -0.063526272  0.414171144  0.29841325 -0.067396952 -0.23173719\nd_cons  0.097819045  0.179380331  0.07889970  0.608464813  0.35133940\nm_ieag -0.003008870 -0.185386385  0.37995131 -0.562708171 -0.08435159\n                 PC6           PC7          PC8          PC9         PC10\nneag   -0.3439637614  0.3181931984  0.546633074 -0.042234946  0.061127320\nsagt   -0.1100455393 -0.4006638868 -0.117615869  0.039275911 -0.279051822\nsagu    0.0817036075 -0.0222447662 -0.171395080  0.026316824  0.157218184\nbenf    0.0137041574 -0.0865694329 -0.029661792 -0.559030776 -0.309307488\nacem   -0.1178721638 -0.0707233983 -0.044543831  0.298833863  0.257966427\nacti    0.0076332457 -0.0037803382  0.059270911  0.005709252 -0.006149006\nocup   -0.0006559353  0.0075506538  0.064643582  0.060016993  0.027810869\npara    0.0727358249 -0.0886662722  0.017717750 -0.382779446 -0.248294230\nejec    0.0680105833  0.3147912020 -0.719248381  0.125707090 -0.074181682\ninds   -0.0628750541  0.0694423188 -0.072739992  0.084531854  0.006424920\ncnst    0.0292065917  0.0536723970  0.048498272  0.056468107  0.155876623\nctrh   -0.0048832446  0.0017937693 -0.009848608 -0.004766928  0.012155773\nserv   -0.0012741045 -0.0048713916  0.087208561  0.112787351  0.132475098\npibc   -0.1948383804  0.1087456789  0.051279330  0.143863566 -0.368945603\nipib   -0.1948383804  0.1087456789  0.051279330  0.143863566 -0.368945603\ninmi    0.0914218309 -0.0061780392  0.133400292  0.163491527  0.196306238\npobl   -0.0080275067 -0.0002658156  0.043324687 -0.007132425 -0.043975544\nespa   -0.0277124999  0.0033821074  0.059065585 -0.031254895 -0.097557785\nd_migr  0.6343714682 -0.4062202201  0.228793096  0.091421146 -0.016735762\nd_prod -0.1578225102 -0.0552752059 -0.137948082 -0.524360180  0.542818315\nd_cons  0.3051347838  0.5279990672  0.073643342 -0.224962399 -0.032322918\nm_ieag  0.4779735058  0.3740589092  0.096068698  0.045373694 -0.057025896\n                PC11          PC12         PC13         PC14          PC15\nneag   -0.0011642406  0.0535876293 -0.003587603  0.008702605  0.0088207614\nsagt   -0.5384363343  0.0201525721  0.152777613  0.016949628  0.0368195759\nsagu    0.6913268975 -0.0009838613  0.025612818 -0.049154388 -0.0235504136\nbenf    0.0778059553  0.0166171043 -0.223463020 -0.437661655  0.1844294743\nacem   -0.1434755408  0.2115156338 -0.800851717 -0.079880449 -0.0085045450\nacti    0.0090474886  0.0899730086  0.105952933  0.167775346 -0.0310804490\nocup    0.0002516397  0.0802453472  0.125817085  0.182996570  0.0004190218\npara    0.0702432830  0.1639853286 -0.045396107  0.092039239 -0.3246330860\nejec   -0.0518914404  0.1874420729  0.093989284  0.015986936  0.0139851888\ninds   -0.0525696075 -0.8441067042 -0.060276232 -0.068972403 -0.3237750008\ncnst   -0.0311681278 -0.1282127274  0.223356927 -0.238665029  0.7194147327\nctrh    0.0323340838 -0.1997179251 -0.010190351  0.020915853  0.1254129755\nserv   -0.0254289130  0.1911372018  0.149746563  0.048866433  0.0410875909\npibc    0.0767242340  0.0474288876 -0.024309177 -0.084114391  0.0154515186\nipib    0.0767242340  0.0474288876 -0.024309177 -0.084114391  0.0154515186\ninmi   -0.0800589037  0.2402968139  0.362203691 -0.540082596 -0.4584377875\npobl    0.0367541275  0.0262302384  0.084541442  0.308217121 -0.0003495038\nespa    0.0670822639  0.0268354928  0.021513582  0.499046521  0.0288103770\nd_migr  0.0827407189 -0.0361408516 -0.101217861  0.050613416  0.0473954559\nd_prod -0.2132910478 -0.0034363236  0.042725328  0.105011778 -0.0347451470\nd_cons -0.0919821263  0.0468918077 -0.127529588  0.021556443 -0.0425306346\nm_ieag -0.3279387644 -0.0510319010 -0.048713110  0.021136187  0.0289352769\n               PC16         PC17         PC18          PC19         PC20\nneag    0.028280641  0.031270874  0.003937319 -0.0113906345  0.003567678\nsagt    0.014486739  0.011924533  0.025953716 -0.0002686759 -0.006380276\nsagu    0.007013293 -0.012975035 -0.012380037  0.0026777669  0.001585427\nbenf   -0.441599019 -0.066918737  0.052027801 -0.0038209310  0.002160965\nacem    0.085530140 -0.082530476 -0.015013315  0.0294884298  0.019430189\nacti   -0.072631066 -0.183444129 -0.191019557 -0.4706669141 -0.058428689\nocup   -0.161635945 -0.214744834 -0.189329803 -0.5529285131 -0.041279368\npara    0.654490486  0.133100176 -0.277084961  0.0765155301 -0.007649426\nejec   -0.010062902  0.015277930 -0.006916257  0.0112412604 -0.014062637\ninds   -0.067715629 -0.027844625 -0.221419616  0.0877284129 -0.005087322\ncnst    0.369077443 -0.186531284 -0.184396274  0.1666115674 -0.006561403\nctrh    0.207940901  0.494244724  0.665266888 -0.3445469570 -0.091813887\nserv   -0.348805373  0.695217496 -0.373281347  0.2321195460 -0.044270279\npibc    0.030506268  0.016345407 -0.006390075  0.0034842214  0.001992897\nipib    0.030506268  0.016345407 -0.006390075  0.0034842214  0.001992897\ninmi    0.005747106 -0.194045761  0.261419680  0.1294941066 -0.060912866\npobl   -0.090754164 -0.118341886  0.211162818  0.2094261635  0.834022532\nespa   -0.140389714 -0.271730744  0.260203084  0.4438655342 -0.533269338\nd_migr -0.029490415  0.015541335 -0.010622030  0.0021557611 -0.001170864\nd_prod -0.005451515 -0.011843054  0.027909507  0.0001228453 -0.004411171\nd_cons -0.004407126  0.008864201  0.008448205 -0.0035154671  0.003884002\nm_ieag -0.016990069  0.006379824  0.020955305 -0.0026142462  0.002256564\n                PC21          PC22\nneag    2.325632e-04 -9.424210e-17\nsagt   -2.289433e-04  6.983831e-17\nsagu    6.422103e-05  7.280692e-17\nbenf   -7.159194e-03 -1.307441e-16\nacem    4.356527e-04 -1.752889e-16\nacti    7.488779e-01 -5.137830e-15\nocup   -6.573444e-01 -1.534483e-15\npara   -8.170016e-02  9.325957e-17\nejec   -2.769442e-04 -9.508544e-18\ninds    1.329953e-03  4.498068e-16\ncnst   -3.981757e-03  5.246307e-16\nctrh   -1.114550e-05 -2.453126e-15\nserv    3.314180e-03  5.811103e-16\npibc   -1.612389e-04 -7.071068e-01\nipib   -1.612389e-04  7.071068e-01\ninmi   -4.256775e-03  1.455391e-15\npobl    9.689444e-03  5.405497e-15\nespa   -1.459524e-02  9.795462e-16\nd_migr  1.544843e-04 -4.905046e-17\nd_prod  1.396581e-04 -8.815050e-17\nd_cons  1.484576e-04 -1.175607e-16\nm_ieag  1.993842e-04 -1.266090e-17\n\n\nPodemos observar aquí nuestras variables en el nuevo sistema de cordenadas (las componentes principales), dando lugar a ecuaciones de cada eje como combinación lineal del total de variables. Analizar el vector que crea cada componente y cuáles son los pesos que tienen las variables en cada componente, ayuda a interpretar qué tipo de información recoge cada una de ellas.\nExtraemos las nuevas coordenadas de los individuos (puntuaciones)\nAdemás, podemos ver las puntuaciones, que son las coordenadas de cada observación original (provincia) sobre los nuevos ejes construidos (componentes principales). Esto corresponde a un cambio de coordenadas bajo el paradigma del Álgebra Lineal.\n\npca$x\n\n                               PC1          PC2         PC3          PC4\nAlmería                 0.14401049 -0.999448416 -1.77192463 -0.118079207\nCádiz                  -0.22289523 -2.155904201 -0.55296161 -1.681361510\nCórdoba                 0.71449576 -3.057392226  0.93475214 -0.756650372\nGranada                 0.06790381 -2.584691109 -0.69960531 -1.361734406\nHuelva                  1.33834239 -0.664855530 -1.89207804 -0.457302506\nJaén                    1.01640243 -2.477648245 -1.48041105  0.660901059\nMálaga                 -2.05022515 -0.647035376 -2.24235793 -0.007862319\nSevilla                -2.51838871 -3.454148645  1.18516391 -0.602085790\nHuesca                  1.97302285  1.580980476  1.94498310  0.956907786\nTeruel                  2.60699292  0.258668522  0.78934459  0.287302608\nZaragoza               -0.18690810  0.467955728  2.63600211 -0.601906654\nAsturias                0.12191259 -0.774813465 -0.94021810 -0.128738650\nBalears                -1.24991745  2.114845576  0.07006078  0.300984950\nPalmas, Las            -0.81168302 -0.345405772 -1.56426460 -1.821112545\nSta. Cruz de Tenerife  -0.63600483 -0.069946598 -1.94786926 -1.052209647\nCantabria               1.25502202  0.430431827 -1.20251616  0.663447751\nÁvila                   2.43630213 -0.270369492  0.03781298  0.025430154\nBurgos                  1.34815470  1.413677429  1.42118486 -2.278312648\nLeón                    1.83063398 -0.654370228 -0.03188379  1.354119970\nPalencia                2.26422590  1.523585181  1.57162124 -0.858533967\nSalamanca               2.10888368 -1.591010860  1.13009061 -1.212836056\nSegovia                 2.52541173 -0.009549252 -0.44096012  0.037138818\nSoria                   2.27848934  2.318162660  0.33886677  1.040220677\nValladolid              1.18172729  0.717811692  1.48437900 -1.854683128\nZamora                  2.66712842 -0.622525273 -0.56993495  1.106715098\nAlbacete                1.70526297 -0.861167392 -1.11820623  0.103679629\nCiudad Real             1.93806454 -2.103835255  1.87532831  1.535218859\nCuenca                  2.39881775 -0.442048801  1.01283707  1.325191706\nGuadalajara             2.06687855  0.015477628 -0.60047816  0.686257932\nToledo                  0.93628031 -2.175445418  1.10303675  1.302358185\nBarcelona             -14.43502394  0.413154985  0.22204504  1.201208938\nGirona                 -0.12932483  2.632853916 -0.56893082  0.954778051\nLleida                  1.03296497  1.675225165  1.26233986  0.948179545\nTarragona              -0.33434264  3.173908867 -0.65007300  0.886774348\nAlacant                -2.85383440 -1.018910215 -1.67455504  1.193787578\nCastelló                0.48607692  1.156090115 -0.23849448 -1.126779290\nValència               -5.17978283 -2.362336409 -0.75443785  1.041869611\nBadajoz                 1.21111842 -3.893142228  2.89809711  0.024951961\nCáceres                 2.11564555 -2.413147162  1.98230441  0.656030710\nCoruña                 -0.18622973 -0.103238763 -1.30365527  1.011604182\nLugo                    2.01680666 -0.103425232 -0.59904942  0.172112826\nOurense                 1.99027889  0.262567528 -1.39082866 -0.274850133\nPontevedra             -0.01002219 -0.243940971 -1.28185296 -2.239128490\nMadrid                -16.12140103  0.902158686  1.86756185 -0.849950394\nMurcia                 -1.85947052 -1.524920020 -1.00131095  0.373358496\nNafarroa                0.80889134  2.565210494  1.14689919  1.343295884\nAraba                   1.01810652  3.850363575  0.82656999 -2.283694207\nBizkaia                -0.48643729  3.684450868 -0.47675186  0.391112994\nGuipuzkoa               0.38787288  4.218645911 -0.28984018  0.328997700\nRioja, La               1.27976318  2.248445727 -0.45583122 -0.346126087\n                               PC5          PC6          PC7          PC8\nAlmería               -1.840263396  0.886133457 -0.056662841 -1.085204437\nCádiz                  0.756066742  0.517666990 -0.257598155 -0.359648407\nCórdoba               -0.154830039 -0.290122082 -0.188321638  0.404000171\nGranada               -0.798198585 -0.026545672  1.149542692  0.284650740\nHuelva                 0.604130564 -1.018121203 -0.945005131 -0.235238633\nJaén                  -1.197458468 -2.262710389 -0.161631350  1.566272311\nMálaga                 0.620812876  0.700955115 -1.842471247 -0.007950675\nSevilla               -0.019022190 -0.104760288 -0.139214460 -0.747133714\nHuesca                -0.527287103  0.344718514 -0.414019387  0.131654763\nTeruel                 1.453666345 -0.006398788  0.187887840 -0.554265574\nZaragoza              -1.349476887  0.105854042 -0.798490696  0.308599256\nAsturias               0.417186834 -1.473557954 -0.550880628  0.078461481\nBalears                0.001180814  0.666788598  0.329707870  0.465098024\nPalmas, Las            0.037166453  1.711105673 -0.527767935  0.471516283\nSta. Cruz de Tenerife  0.016664519  1.022360024 -0.892992082  0.608069656\nCantabria              1.288344159 -0.617367455  0.502612810 -0.241069794\nÁvila                  0.248360446  2.042086195  0.137871607  0.487495687\nBurgos                -0.381804337 -0.738620046  0.718334373 -0.227140181\nLeón                   1.257000727 -0.643998755 -0.116416370 -0.380177406\nPalencia               0.829595024  0.114485013  0.847358158 -0.517599965\nSalamanca              0.448436046  0.054743227 -0.366775698 -0.332977811\nSegovia                1.277456309  0.115359511  0.615321278 -0.073372300\nSoria                 -0.056124324  0.169805115 -0.789668153 -0.089107637\nValladolid            -0.015817826 -0.697262679 -0.029660364 -0.308577787\nZamora                 1.308369747 -0.418113227 -0.097555340 -0.215479343\nAlbacete               0.345562465 -1.475724995 -1.118613908  0.025375203\nCiudad Real            0.395304638 -0.562800701  0.271786652  0.218761429\nCuenca                -0.017585746  0.980944273 -0.494104701  0.543383475\nGuadalajara            0.383219616  1.568482029 -0.287585588 -0.029561787\nToledo                -0.839826590  1.690680064  0.391614591  0.091012766\nBarcelona              0.454037480 -0.801543343 -0.369706138 -1.249723637\nGirona                -0.375599413  0.137947557  0.086249917 -0.521881814\nLleida                -1.046605240  0.314339312  0.254279681 -0.236157269\nTarragona             -0.649585007  0.032463340  0.252586920 -0.988335272\nAlacant               -0.406396995  1.478312711  1.024738902 -0.090312256\nCastelló              -1.415190361 -1.556676980  0.891367928 -0.741857270\nValència              -2.758813451 -0.520886030  0.941295532  0.580421262\nBadajoz               -0.054632531 -0.389481396 -0.908762690 -0.155223276\nCáceres                0.737089594  0.404934088  0.444874558 -0.072856327\nCoruña                 0.707018419 -1.016559755  1.095008250  0.324070476\nLugo                   0.665197372 -0.345710848  1.163656474  0.640596538\nOurense                1.147913834  0.051167996  1.087178660  0.376159086\nPontevedra             0.726154514  0.311651887  0.553082995 -0.430980108\nMadrid                 1.757215127  0.291809278  0.176676476  1.131436202\nMurcia                -1.808383985  0.520085843 -0.007004105 -0.719125224\nNafarroa              -0.338398466 -0.419173713 -0.465700947  0.400354360\nAraba                 -1.232869320  0.025797808  0.018873931  0.536342782\nBizkaia                0.322389139 -0.028605796 -0.051739371  0.474788319\nGuipuzkoa              0.045977400 -0.245898878  0.064907831  0.396027192\nRioja, La             -0.967346938 -0.600036686 -1.328467005  0.066410442\n                              PC9         PC10         PC11         PC12\nAlmería                0.47579425  0.031379967  0.067687517  0.440758111\nCádiz                 -0.88921529 -0.264584186 -0.124231333  0.033726501\nCórdoba               -0.39479499 -0.000661029 -0.040609116 -0.297038584\nGranada               -0.46355596  0.508736899  0.721187555 -0.007275494\nHuelva                -0.14893625 -0.145514646 -0.221190664  0.137937169\nJaén                  -0.36287568  0.185979503 -0.576208796  0.056542425\nMálaga                -0.39046858  0.099209284  0.141258917  0.070688374\nSevilla               -1.17384978 -1.016992593  0.239923800  0.134541552\nHuesca                -0.21214549  0.101260268 -0.150619415  0.096337563\nTeruel                 0.52834561 -0.606954667  0.049351575  0.088751882\nZaragoza               0.25974911  0.165877434 -0.121559515 -0.123259262\nAsturias               0.42069749  0.019092483  0.444795169  0.045736154\nBalears               -0.64452829  0.348524662 -0.298421945 -0.026367015\nPalmas, Las           -0.12289782 -0.124598530 -0.100568801  0.087594118\nSta. Cruz de Tenerife  0.07040546  0.094335764  0.187721857  0.171260482\nCantabria              0.25800144 -0.087050774  0.210415333  0.185091734\nÁvila                  0.34751023  0.183158228 -0.235315445 -0.100906679\nBurgos                 0.08982974  0.506075200  0.783906107 -0.025064155\nLeón                   0.18775816  0.017027705  0.160106912  0.080944591\nPalencia              -0.40799024  0.278420589 -0.586503742  0.081427618\nSalamanca              0.66245322  0.069013989  0.115793263 -0.192557299\nSegovia                0.26377553  0.045433780 -0.055500728  0.042629686\nSoria                 -0.39548961  0.448084300  0.126778938  0.129563909\nValladolid            -0.04441208  0.471043203 -0.376241813 -0.057844901\nZamora                 0.03898128  0.344062280  0.216171634  0.039450260\nAlbacete               0.41689554 -0.203739064 -0.789071275 -0.140619948\nCiudad Real           -0.01441631 -0.262018267  0.142942750  0.078037280\nCuenca                 0.46824496 -0.214039364  0.265163896 -0.026486841\nGuadalajara            0.36980149  0.217380824 -0.007654602  0.137350919\nToledo                 0.06924745  0.088362452 -0.235341252 -0.183538227\nBarcelona              0.28370357  0.308185056 -0.168211407 -0.655783799\nGirona                -0.30877670  0.334107366  0.019395725  0.139027420\nLleida                -0.13038594  0.095264785 -0.229996593  0.220331404\nTarragona             -0.22490139 -0.329658271 -0.223568581  0.457355703\nAlacant               -0.46358071 -0.007011361 -0.053955398 -0.572715113\nCastelló              -0.17673516  0.144756598 -0.238364848  0.172055689\nValència               0.45311372 -0.243562961  0.034737735  0.144613633\nBadajoz                0.07982642  0.101819643  0.233372242 -0.025940982\nCáceres               -0.06054827  0.028835927 -0.267002137  0.078212742\nCoruña                 0.08743362 -0.100109590  0.229686348  0.014155849\nLugo                   0.21088527 -0.054371431 -0.065019501  0.002428980\nOurense                0.10922977  0.164311446 -0.155325711 -0.092258760\nPontevedra             0.42949388 -0.098396703 -0.232400919 -0.387139657\nMadrid                 0.28638213  0.086440463 -0.027846605  0.638415149\nMurcia                 0.48492509 -0.145308861  0.144571521 -0.039782422\nNafarroa              -0.27003359 -0.280474425  0.291341255 -0.191965449\nAraba                  0.58117654 -0.961482155 -0.153581965 -0.125305920\nBizkaia               -0.52802332 -0.323376848  0.333688501 -0.293725334\nGuipuzkoa             -0.14881051 -0.514567563  0.319057881 -0.323540722\nRioja, La              0.04371102  0.498293190  0.255255677 -0.115850337\n                              PC13         PC14          PC15          PC16\nAlmería                0.243256304  0.023197548  0.0918354752 -0.0864457317\nCádiz                  0.058838642  0.226679222 -0.1334812512  0.0276735027\nCórdoba               -0.082782684 -0.093383607 -0.0209288936 -0.0699858197\nGranada                0.074704621 -0.046549251 -0.0108945631 -0.0205651179\nHuelva                 0.193895641 -0.109812043 -0.0604883225 -0.1312752870\nJaén                  -0.155290169  0.008539545  0.0193559326 -0.0472725390\nMálaga                 0.113937586 -0.104167043  0.2347149437  0.0032440254\nSevilla               -0.279142401  0.049771125  0.0483271642 -0.0807590026\nHuesca                -0.015187386  0.045845707 -0.0038943232 -0.0000289523\nTeruel                 0.114446897 -0.183100299 -0.0591818945  0.0155027336\nZaragoza              -0.007316974  0.079574879  0.0135980360  0.0090214890\nAsturias              -0.103268919  0.149765834  0.0974963950  0.1246925963\nBalears                0.067412217 -0.312786181  0.2527344599  0.0706809591\nPalmas, Las           -0.340577799  0.054812963 -0.0685313116  0.2297666058\nSta. Cruz de Tenerife -0.089252701 -0.226520085 -0.2164235530  0.0369171135\nCantabria             -0.288407669 -0.006698810  0.0218048670 -0.0116105406\nÁvila                 -0.220370725  0.115376013  0.0387523918 -0.1064725169\nBurgos                -0.057614449 -0.122168057 -0.0236274156  0.0083248286\nLeón                  -0.039545102  0.017638522 -0.0021933969  0.0646999008\nPalencia              -0.098517836  0.007542103 -0.0942570636 -0.0065010393\nSalamanca              0.358592918 -0.031080675  0.0197916061 -0.0209360706\nSegovia               -0.033822299 -0.047194691 -0.0847764781 -0.0225991634\nSoria                 -0.060867274  0.072913264 -0.1017211904 -0.0485846831\nValladolid            -0.037886772  0.123702005  0.0056208119  0.0199385765\nZamora                 0.150489581  0.020646501 -0.1047602744 -0.0060099717\nAlbacete               0.306045350  0.001369938 -0.0327167190  0.0471789455\nCiudad Real            0.271635520 -0.030959851 -0.0471297014  0.1080990728\nCuenca                -0.003677958 -0.018001270  0.0117999112 -0.0107916798\nGuadalajara           -0.286227495  0.108304062 -0.0026570347 -0.0936267361\nToledo                 0.159341618  0.157705076  0.0460566429  0.0480747498\nBarcelona             -0.461872434 -0.058871190 -0.0612856830  0.0010192472\nGirona                -0.145867549 -0.048662093  0.0830727014  0.0002817880\nLleida                -0.062957303  0.018103580 -0.0006148988  0.0334752851\nTarragona             -0.165055214 -0.087586282 -0.0240318943  0.0440645931\nAlacant                0.532278711 -0.155960165 -0.0775878862  0.0044472527\nCastelló               0.393719215  0.040820347  0.0133198844  0.0376363920\nValència              -0.316885416 -0.023996092 -0.1237916806  0.0011157925\nBadajoz               -0.205176771 -0.144432481  0.0809322042  0.0219498492\nCáceres               -0.082734251 -0.063117768  0.0548941442 -0.0157765960\nCoruña                -0.128303420  0.148392486  0.2077448796  0.0002856532\nLugo                  -0.023708395 -0.019398422 -0.0002416294 -0.0031629889\nOurense               -0.175629670 -0.015063975 -0.0540800888 -0.0168161067\nPontevedra            -0.016956952  0.103058755  0.1502294457 -0.0200890991\nMadrid                 0.431138869  0.079723890 -0.0020457785 -0.0507573883\nMurcia                 0.248271351  0.104235265 -0.0173682690  0.0310126071\nNafarroa               0.241538000  0.014325265 -0.0579872456 -0.0109843279\nAraba                 -0.130451064 -0.145799946  0.0592446243 -0.0268608784\nBizkaia                0.191397603  0.224462209  0.0267948573  0.0258592618\nGuipuzkoa             -0.040766676  0.043677863 -0.0368317525 -0.0628978921\nRioja, La              0.005181081  0.055126309 -0.0545911850 -0.0441526926\n                               PC17         PC18          PC19          PC20\nAlmería                0.0290603932  0.023297482 -0.0261788636  0.0021592582\nCádiz                 -0.0573358452  0.020537862  0.0242553850  0.0110931846\nCórdoba               -0.0116373353 -0.047303902  0.0094304309 -0.0009709308\nGranada                0.0555551741 -0.021137305  0.0114845390 -0.0057839489\nHuelva                -0.0668103091  0.006994846 -0.0204765617  0.0029332837\nJaén                   0.0269862219 -0.010160811 -0.0076101351  0.0132002321\nMálaga                 0.0971477762  0.095364029  0.0321342957  0.0109561717\nSevilla                0.0298117912 -0.049693063 -0.0176335912 -0.0117112244\nHuesca                 0.0671876017  0.018546486  0.0053989330 -0.0051672590\nTeruel                 0.0183776583 -0.016495600  0.0101574794  0.0004769062\nZaragoza              -0.0759782539  0.033006164 -0.0176598284  0.0123128589\nAsturias              -0.0037136818  0.020392665  0.0183658100  0.0059505213\nBalears               -0.1270417678 -0.050749115 -0.0079195331 -0.0104075047\nPalmas, Las            0.0653780205 -0.066881548 -0.0256405118  0.0043369059\nSta. Cruz de Tenerife -0.0506080490  0.076044887 -0.0313227418 -0.0219382357\nCantabria             -0.0516286389 -0.021123954 -0.0042540018  0.0099561552\nÁvila                  0.0367798461  0.006988128  0.0028674196  0.0011440612\nBurgos                -0.0065373066 -0.009819506  0.0073406099  0.0081256418\nLeón                  -0.0291366556 -0.006688017  0.0051578876  0.0001156972\nPalencia               0.0008175788  0.067492861  0.0101435389  0.0017472671\nSalamanca             -0.0030847479  0.007636628  0.0089367166 -0.0086519231\nSegovia                0.0096177927 -0.015281774 -0.0127344868  0.0047183153\nSoria                  0.0377207743 -0.006429529  0.0117062106 -0.0103247456\nValladolid            -0.0436617085  0.030107916 -0.0208394860  0.0039401719\nZamora                 0.0025741863 -0.027819156  0.0040603510 -0.0037825961\nAlbacete               0.0111057266 -0.055612628  0.0134175239 -0.0120152120\nCiudad Real            0.0353877687  0.010403098 -0.0121534524  0.0027465264\nCuenca                 0.0614068055 -0.008334674 -0.0095199686 -0.0012502887\nGuadalajara           -0.0459104329 -0.045364678 -0.0189013014  0.0062093887\nToledo                -0.0213896936 -0.044381177  0.0231274520 -0.0095785929\nBarcelona              0.0596478456  0.008450191 -0.0197780746 -0.0000885052\nGirona                 0.0049317323 -0.051576982  0.0094151185  0.0071898729\nLleida                 0.0593307112  0.026020817 -0.0118541063 -0.0061720694\nTarragona             -0.0447424817  0.005296798  0.0359760839  0.0009822919\nAlacant                0.0142503120  0.006607664 -0.0024707108  0.0163688078\nCastelló               0.0889779142 -0.013551168 -0.0218059226 -0.0039166944\nValència              -0.0506912239  0.058879432  0.0400020927 -0.0108557448\nBadajoz               -0.0450806290  0.021763198 -0.0006869546  0.0047945629\nCáceres                0.0016302717  0.044902961  0.0016832324  0.0026281758\nCoruña                -0.0232121134  0.011928371 -0.0272604925 -0.0117490811\nLugo                   0.0558768195  0.038304089 -0.0248784625  0.0016975742\nOurense                0.0127508472 -0.009264926  0.0274629751  0.0054288723\nPontevedra            -0.0085150921  0.023858922  0.0184202969 -0.0209693638\nMadrid                -0.0014938079 -0.035053495  0.0093977092  0.0014234824\nMurcia                -0.0942960417 -0.029298396 -0.0092425686  0.0123501848\nNafarroa              -0.0069749897  0.007711255 -0.0050800672 -0.0034177350\nAraba                  0.0432025338 -0.016301915  0.0083365903  0.0098990113\nBizkaia               -0.0576446976  0.069886689 -0.0144708332 -0.0031350775\nGuipuzkoa             -0.0104548562 -0.010284076  0.0043133862  0.0035820479\nRioja, La              0.0120662556 -0.071816043  0.0173805884 -0.0065806985\n                               PC21          PC22\nAlmería               -1.769168e-03 -5.659712e-17\nCádiz                 -4.102063e-05  2.091484e-16\nCórdoba               -2.930831e-03 -1.345576e-16\nGranada               -1.380124e-03  6.235390e-17\nHuelva                -7.879173e-04  8.023850e-18\nJaén                   1.165412e-04 -3.469890e-16\nMálaga                 4.558678e-04  3.687160e-16\nSevilla                1.516729e-03 -4.816864e-16\nHuesca                 8.136195e-04  2.157779e-16\nTeruel                -5.043557e-04  3.653912e-16\nZaragoza              -7.124357e-04  3.948470e-16\nAsturias              -3.586382e-04  1.089640e-16\nBalears                3.003175e-04 -7.137619e-16\nPalmas, Las           -6.451246e-04 -5.453732e-16\nSta. Cruz de Tenerife  2.909563e-04  6.114977e-17\nCantabria             -4.717381e-04  2.658787e-17\nÁvila                  9.427784e-04  8.880847e-17\nBurgos                 1.045217e-03  1.694051e-16\nLeón                   8.818798e-05  2.593243e-16\nPalencia               1.343752e-04  1.586339e-16\nSalamanca             -4.823569e-04  2.670583e-16\nSegovia                3.979363e-04 -1.524239e-16\nSoria                  4.416088e-04 -3.839118e-16\nValladolid             9.652706e-04  3.131621e-16\nZamora                 3.077992e-04  2.815086e-16\nAlbacete               5.132887e-04  1.240523e-16\nCiudad Real            4.625934e-04  2.318143e-16\nCuenca                 5.127048e-04  1.369508e-16\nGuadalajara            8.714265e-04  4.841850e-17\nToledo                -1.171346e-03  4.150121e-16\nBarcelona             -7.789622e-04 -6.194659e-16\nGirona                -1.755479e-04 -1.070159e-16\nLleida                -1.352872e-03 -8.370366e-17\nTarragona             -5.358905e-04  1.135030e-16\nAlacant                1.656053e-03 -3.307671e-17\nCastelló               7.785871e-04 -3.793194e-16\nValència               9.704024e-04  1.818606e-16\nBadajoz                8.804314e-04 -1.163872e-17\nCáceres               -5.698375e-04  1.323958e-17\nCoruña                 5.184729e-04 -3.072331e-16\nLugo                  -2.016251e-04 -2.227411e-16\nOurense               -4.945385e-04  1.257808e-16\nPontevedra             4.975490e-04  1.544283e-16\nMadrid                 6.439874e-05  5.786848e-16\nMurcia                 8.088999e-04  3.027072e-16\nNafarroa              -7.896884e-04 -9.989832e-17\nAraba                  1.158608e-04  7.955217e-17\nBizkaia               -1.039056e-03  1.805423e-17\nGuipuzkoa             -1.413750e-04 -4.448294e-18\nRioja, La              8.665755e-04  2.112864e-16\n\n\nDando una interpretación a cada eje, podremos determinar qué perfil tiene cada provincia dentro del estudio.\nVarianza explicada por cada componente principal\nUna vez calculadas las componentes principales, es de interés conocer la varianza explicada por cada una, ya que el principal objetivo que se sigue con PCA es maximizar la cantidad de información explicada por las componentes.\n\nsummary(pca)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.5372 1.9301 1.30707 1.07415 0.94777 0.87697 0.67992\nProportion of Variance 0.5687 0.1693 0.07766 0.05244 0.04083 0.03496 0.02101\nCumulative Proportion  0.5687 0.7380 0.81570 0.86815 0.90898 0.94394 0.96495\n                           PC8     PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     0.54393 0.39540 0.32673 0.2965 0.22661 0.21293 0.10955\nProportion of Variance 0.01345 0.00711 0.00485 0.0040 0.00233 0.00206 0.00055\nCumulative Proportion  0.97840 0.98550 0.99036 0.9943 0.99669 0.99875 0.99929\n                          PC15    PC16    PC17    PC18    PC19    PC20\nStandard deviation     0.08876 0.05995 0.04795 0.03760 0.01773 0.00854\nProportion of Variance 0.00036 0.00016 0.00010 0.00006 0.00001 0.00000\nCumulative Proportion  0.99965 0.99981 0.99992 0.99998 1.00000 1.00000\n                            PC21      PC22\nStandard deviation     0.0008774 1.745e-16\nProportion of Variance 0.0000000 0.000e+00\nCumulative Proportion  1.0000000 1.000e+00\n\n\n\nStandard deviation: muestra las desviaciones estándar de cada componente principal. Si elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\nProportion of Variance: es la proporción de la varianza total que explica cada componente principal y quizá, es la fila más importante de nuestros resultados. Como los autovalores están ordenados de mayor a menor y así son construidas las componentes principales, la primera componente principal es la que mayor porcentaje de variabilidad explica, un 56%. Así, la varianza explicada por la componentes van en orden decreciente, teniendo que la segunda componente explica un 16% y la tercera, un 7%.\nCumulative proportion: es la varianza acumulada y se calcula progresivamente sumando la Proportion of Variance anterior. En vista de estos resultados, vemos que la primera componente agrupa el 56% de la variación, y que necesitamos 3 componentes para alcanzar el 81%.\n\nSi elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\n\n# Autovalues\npca$sdev^2 # varianza de cada componente\n\n [1] 1.251174e+01 3.725281e+00 1.708420e+00 1.153788e+00 8.982736e-01\n [6] 7.690832e-01 4.622873e-01 2.958583e-01 1.563377e-01 1.067539e-01\n[11] 8.791102e-02 5.135091e-02 4.534131e-02 1.200170e-02 7.877692e-03\n[16] 3.593488e-03 2.299134e-03 1.413591e-03 3.143363e-04 7.292756e-05\n[21] 7.697824e-07 3.044759e-32",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#selección-de-componentes",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#selección-de-componentes",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Selección de componentes",
    "text": "Selección de componentes\nGraficando el valor de la varianza de cada componente principal, podemos observar los resultados comentados anteriormente, que las primeras componentes son las que más varianza explican y que a medida que se añaden más, la varianza explicada por cada una es menor.\n\nfviz_eig(pca, main = \"Varianza de cada componente\", choice = \"eigenvalue\", addlabels = T)\n\n\n\n\n\n\n\n\no como el porcentaje de varianza explicada por cada componente sobre el total.\n\nfviz_screeplot(pca, addlabels = TRUE, main = \"Porcentaje de varianza explicada por cada componente (%)\")\n\n\n\n\n\n\n\n\nA continuación, representamos las varianzas acumuladas:\n\nplot(summary(pca)$importance[3, ], type = \"o\", col = \"darkblue\", lwd = 3, main = \"Porcentaje de varianza acumulada\", xlab = \"Componente Principal\", ylab = \"Porcentaje de varianza acumulada\")\n\n\n\n\n\n\n\n\nDeterminar el número de componentes que elegir para continuar con el análisis no tiene unas normas determinadas a seguir. Respecto a ello, existen varios criterios con sus respectivas propuestas.\n\nUna opción para determinar el número de componentes principales que seleccionar, es coger aquellas tal que expliquemos un % determinado de la variabilidad de los datos que nosotros prefijemos. Generalmente se pone como umbral mínimo un 80%, entonces necesitaríamos elegir 3 componentes.\nOtra posibilidad es seguir el criterio de Kaisser, que escoge aquellas componentes cuyo autovalor sea superior a 1 (cuando las variables han sido generadas a partir de la matriz de correlaciones). Según este criterio y mirando el gráfico anterior de la varianza (igual a eigenvalues), elegiríamos las 4 primeras componentes.\nPara relajar el criterio de Kaisser, existe la modificación de Jollife, que elige aquellas componentes cuyo autovalor sea superior a 0.7. Esta modificación, nos permite elegir 6 componentes.\n\nEn este caso, nos podríamos quedar con las 3 primeras componentes principales, ya que un 80% de variabilidad es bastante ilustrativo para este ejemplo que queremos hacer. Por tanto, en lugar de trabajar con las 22 variables originales, trabajaremos con 3 variables nuevas, que son combinaciones de ellas.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#interpretación",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#interpretación",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Interpretación",
    "text": "Interpretación\nHemos decidido quedarnos con 3 componentes principales, que explican el 81% de la variabilidad total. Para realizar su interpretación, volvemos a ver los coeficientes de las ecuaciones de los componentes, observando cuáles son los valores más altos (en valor absoluto), para así poder dar una interpretación a cada eje.\n\npca$rotation[, 1:3]\n\n                PC1          PC2         PC3\nneag   -0.008203669 -0.343875267  0.04530181\nsagt    0.093777760 -0.260685812  0.46692515\nsagu    0.065567030 -0.244985221  0.61799140\nbenf   -0.255547541 -0.169583054 -0.04259558\nacem   -0.267132603 -0.057835771  0.10183034\nacti   -0.281063977 -0.023526873  0.02628693\nocup   -0.280841146 -0.008682980  0.03451402\npara   -0.266087680 -0.130432450 -0.03284853\nejec   -0.198849271 -0.099263733 -0.15594933\ninds   -0.273655544 -0.025426864  0.02639828\ncnst   -0.278652744  0.009442489  0.01045498\nctrh   -0.280865532 -0.034425301 -0.00195234\nserv   -0.278225349  0.001691252  0.04777668\npibc   -0.091357800  0.443739119  0.23824009\nipib   -0.091357800  0.443739119  0.23824009\ninmi   -0.276330714  0.024554052  0.01362495\npobl   -0.281059234 -0.031986886  0.01745440\nespa   -0.279916693 -0.041995882  0.02242889\nd_migr -0.095504531  0.238686852 -0.03617703\nd_prod -0.063526272  0.414171144  0.29841325\nd_cons  0.097819045  0.179380331  0.07889970\nm_ieag -0.003008870 -0.185386385  0.37995131\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \"PC17\", \"PC18\", \"PC19\", \"PC20\", \"PC21\", \"PC22\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nSi nos fijamos en los pesos más altos, podemos darle una interpretación a cada eje. Por ejemplo:\n\nLa primera componente explica un 56.9% de la variación. Hay valores absolutos bastante similares y elevados (en rojo), que son los relativos a las variables socio-demográficas que hemos comentado antes, y son los que esencialmente está capturando la Componente 1. Teniendo en cuenta los signos podemos concluir que todas ellas influyen negativamente en la componente 1 ya que la correlación es negativa.\nLa segunda componente explica un 16.9% de variación adicional, los pesos más elevados corresponden con las variables pibpc e ibipc y puesto que tienen signo positivo, estás están correlacionadas positivamente con la Componente 2. Vemos que esta componente describe principalmente esas dos variables, que hablan del comportamiento económico del país.\nLa tercera componente explica un 7.8% de variación adicional, los pesos más elevados corresponden con las variables sagt e sagu (correlacionada positivamente) y que describe principalmente las características agrícolas del país.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#representación-gráfica",
    "href": "notebooks/Reduccion Dimension/PCA/provincias_variado/provincias_variado.html#representación-gráfica",
    "title": "Reducción Dimensionalidad - PCA: provincias_variado",
    "section": "Representación gráfica",
    "text": "Representación gráfica\nGráfico de las variables\nRepresentamos sobre las dos primeras componentes principales las variables originales. En el eje de abscisas se representa la PC1 y en el eje de ordenadas, la PC2. Para interpretar correctamente las variables tenemos que fijarnos en la longitud de la flecha y en el ángulo que forman respecto a los ejes y entre ellos mismos.\n\nÁngulo vector - eje: cuanto más paralelo es un vector al eje, más ha contribuido a dicha componente principal.\nÁngulo entre dos vectores: si es pequeño representa una alta correlación entre las variables implicadas (y por tanto, observaciones con valores altos en una variable, tendrá valores altos en la otra). Si el ángulo es cercano a 90º indica que las variables están incorreladas y los ángulos opuestos indican correlación negativa entre ellas.\nLongitud: cuanto mayor es la longitud de un vector, mayor varianza de la variable está contenida en el biplot, es decir, mejor representada está en el gráfico.\n\nEn el gráfico, diferenciamos por colores las variables según su calidad de representación en las dos primeras componentes. Cuanto más cerca esté una variable del círculo de correlaciones, mejor será su representación, por lo que las variables que estén muy cerca del centro de la gráfica son las menos importantes para las dos primeras componentes.\n\nfviz_pca_var(pca, axes = c(1, 2), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\nfviz_pca_var(pca, axes = c(1, 3), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\n\n\nSi nos fijamos en el eje de abscisas, todas variables que hemos visto correlacionadas con la componente 1 de manera alta se situan con menor ángulo respecto a él, indicando que han contribuido a la formación de la PC1. Además hay muy poco ángulo entre ellas puesto que se encuentran altamente correlacionadas entre sí.\nEn cuanto al eje de ordenadas, vemos que las variables que forman un menor ángulo respecto a él, son las que más contribuían a la formación de la PC2, tanto de forma positiva como negativa.\n\nRESUMEN DE RESULTADOS\nLos gráficos obtenidos muestran una visualización de las variables en función de las componentes principales 1 y 2, y 1 y 3. Las variables que tienen una correlación alta con la primera componente principal están más cerca del eje horizontal y las variables que tienen una alta correlación con la segunda componente principal están más cerca del eje vertical. Las variables que se encuentran cerca del centro del gráfico tienen una correlación baja con ambas componentes principales, aún así nos guiaremos con la tabla para decidir qué variables están mejor explicadas por cada una de las componentes. En resumen, estos gráficos proporcionan una representación visual de las relaciones entre las variables en función de las dos primeras componentes principales y las dos segundas, lo que puede ayudar a identificar patrones y tendencias en los datos.\nEn el siguiente gráfico podemos ver las correlaciones de dichas variables con las componentes principales, como ya hemos comentado.\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \"PC17\", \"PC18\", \"PC19\", \"PC20\", \"PC21\", \"PC22\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nEn cuanto a este gráfico, es llamativo como las dos o tres primeras componentes son las más importantes en el PCA, sobre todo la PC1 que es la que mas variables resume.\nEn resumen, las nuevas componentes han permitido identificar patrones y características de las provincias en términos de la situación sanitaria y indicadores relativos a esta.\nGráfico de los individuos\nTras observar la representación de las variables, en este apartado vemos la representación de los individuos sobre los nuevos ejes, con la idea de que aquellos con características similares, se agrupan cerca al tener puntuaciones parecidas. Las provincias con valores cercanos a la media se situarán cerca del centro del gráfico (0,0).\nVemos que los más lejanos son Madrid y Barcelona en el gráfico de las primeras dos componentes.\n\n# Sobre PC1 y PC2\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(1, 2))\n\n\n\n\n\n\n\n\n\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(2, 3))\n\n\n\n\n\n\n\n\nLas provincias que se encuentren cerca indican similitud para las componentes que se están representando en el gráfico.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Provincias Variado",
      "Reducción Dimensionalidad - PCA: provincias_variado"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "",
    "text": "A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando el método de Componentes Principales. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado Situacion_sanitaria.xlsx. Este dataset presenta datos para las Comunidades Autónomas, abordando sobre aspectos demográficos y de salud. Contiene variables como la tasa de natalidad y mortalidad, el índice de envejecimiento, tasas de profesionales de la salud (médicos y enfermeros) por cada 100,000 habitantes, el porcentaje de inaccesibilidad a medicamentos recetados por razones económicas y la tasa media de morbilidad hospitalaria debido a enfermedades. Concretamente en este dataset tenemos las siguientes variables:\n\nCCAA: Comunidad Autónoma\nT_nat: Tasa de natalidad\nT_mort: Tasa de mortalidad\nInd_envej: Índice de envejecimiento\nMédicos: Tasa de médicos por cada 100.000 habitantes\nEnfermeros: Tasa de enfermeros por cada 100.000 habitantes\nInacmedicam: Inaccesibilidad a los medicamentos recetados por motivos económicos (%)\nTasa_enf: Tasa de Morbilidad Hospitalaria por 100.000 habitantes debido a enfermedades, que he construido como una media (equiponderada) de las tasas de morbilidad hospitalaria debido a enfermedades endocrinas, circulatorias, digestivas, respiratorias…\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.\n\n\n\nSe pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar la situación sanitaria y ver qué comunidades autónomas son las más parecidas y las más diferentes.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#dataset",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#dataset",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado Situacion_sanitaria.xlsx. Este dataset presenta datos para las Comunidades Autónomas, abordando sobre aspectos demográficos y de salud. Contiene variables como la tasa de natalidad y mortalidad, el índice de envejecimiento, tasas de profesionales de la salud (médicos y enfermeros) por cada 100,000 habitantes, el porcentaje de inaccesibilidad a medicamentos recetados por razones económicas y la tasa media de morbilidad hospitalaria debido a enfermedades. Concretamente en este dataset tenemos las siguientes variables:\n\nCCAA: Comunidad Autónoma\nT_nat: Tasa de natalidad\nT_mort: Tasa de mortalidad\nInd_envej: Índice de envejecimiento\nMédicos: Tasa de médicos por cada 100.000 habitantes\nEnfermeros: Tasa de enfermeros por cada 100.000 habitantes\nInacmedicam: Inaccesibilidad a los medicamentos recetados por motivos económicos (%)\nTasa_enf: Tasa de Morbilidad Hospitalaria por 100.000 habitantes debido a enfermedades, que he construido como una media (equiponderada) de las tasas de morbilidad hospitalaria debido a enfermedades endocrinas, circulatorias, digestivas, respiratorias…\n\nSi tomamos demasiadas variables es difícil visualizar relaciones entre ellas. Otro problema que se presenta es la fuerte correlación. Se hace necesario, pues, reducir el número de variables sin perder información. Es importante resaltar el hecho de que el concepto de mayor información se relaciona con el de mayor variabilidad o varianza.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#descripción-del-trabajo-a-realizar",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "",
    "text": "Se pretende hacer un Análisis de Reducción de la Dimensionalidad empleando el procedimiento de Componentes Principales. El objetivo es conocer que variables independientes son de interés para estudiar la situación sanitaria y ver qué comunidades autónomas son las más parecidas y las más diferentes.\n\nHacer un análisis exploratorio explorando matriz de correlaciones.\nVer si es necesario escalar/centrar los datos antes de aplicar pca y decidir si hacerlo con matriz de correlaciones o covarianzas.\nSeleccionar un determinado número de componentes y ver como influyen las variables en estas.\nInterpretar componentes y resultados.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#cargar-librerías",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#cargar-librerías",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerias\nlibrary(readxl) # Para leer los excels\nlibrary(dplyr) # Para tratamiento de dataframes\nlibrary(ggplot2) # Nice plots\nlibrary(factoextra) # fviz_cluster function\nlibrary(ggcorrplot) # Para funcion ggcorrplot\nlibrary(corrplot) # Para corrplot",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#lectura-datos",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#lectura-datos",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndatos &lt;- read_excel(\"../../../../files/Situacion_sanitaria.xlsx\", sheet = \"Datos\")\n\nEn este caso, se trata de un conjunto de datos compuesto por 17 filas y 38 columnas. Las filas corresponden a las 17 Comunidades Autónomas de España.\nAntes de comenzar a aplicar la técnica, comprobamos si hay valores perdidos, por si fuera necesario realizar algún preproceso. En este caso, y como vemos a continuación, no hay ningún NA en los datos que vamos a utilizar.\n\nsum(is.na(datos))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(datos)\n\n[1] 17  8\n\n# Tipo de variables que contiene\nstr(datos)\n\ntibble [17 × 8] (S3: tbl_df/tbl/data.frame)\n $ CCAA       : chr [1:17] \"Andalucía\" \"Aragón\" \"Asturias, Principado de\" \"Baleares, Islas\" ...\n $ T_nat      : num [1:17] 8.21 7.28 5.05 8.08 6.37 6.1 5.96 7.51 8.09 7.47 ...\n $ T_mort     : num [1:17] 8.35 10.28 12.64 6.67 7.1 ...\n $ Ind_envej  : num [1:17] 101.1 143.5 218.7 99.7 114.1 ...\n $ Médicos    : num [1:17] 492 689 654 484 506 ...\n $ Enfermeros : num [1:17] 565 682 735 545 653 ...\n $ Inacmedicam: num [1:17] 0.63 1.23 0.18 0.93 1.77 0.17 1.22 0.23 0.89 1.92 ...\n $ Tasa_enf   : num [1:17] 545 748 834 702 538 ...\n\n# Descripción de las variables\nsummary(datos)\n\n     CCAA               T_nat           T_mort         Ind_envej     \n Length:17          Min.   :5.050   Min.   : 6.670   Min.   : 85.98  \n Class :character   1st Qu.:6.370   1st Qu.: 8.350   1st Qu.:114.07  \n Mode  :character   Median :7.470   Median : 9.550   Median :122.57  \n                    Mean   :7.294   Mean   : 9.385   Mean   :136.60  \n                    3rd Qu.:8.090   3rd Qu.:10.330   3rd Qu.:150.49  \n                    Max.   :9.540   Max.   :12.640   Max.   :218.65  \n    Médicos        Enfermeros      Inacmedicam       Tasa_enf    \n Min.   :446.9   Min.   : 458.7   Min.   :0.170   Min.   :537.5  \n 1st Qu.:511.4   1st Qu.: 584.1   1st Qu.:0.730   1st Qu.:654.1  \n Median :554.0   Median : 681.6   Median :1.220   Median :712.4  \n Mean   :574.1   Mean   : 688.0   Mean   :1.115   Mean   :705.9  \n 3rd Qu.:649.3   3rd Qu.: 767.8   3rd Qu.:1.530   3rd Qu.:776.2  \n Max.   :689.3   Max.   :1047.9   Max.   :2.240   Max.   :833.8  \n\n\nVemos que estas variables (a excepción de las CCAA) son todas de tipo numérico, y además, podemos obtener información como la media, desviación típica, los cuartiles y el histograma de cada una. Si no hubiésemos estandarizado las variables, las variables Médicos y Enfermeros dominarían las componentes principales puesto que toman unos valores muy grandes en valor absoluto.\nCorrelación: El que existan correlaciones muy elevadas en el conjunto de datos nos permitirá resumir la información en un menor número de componentes principales, pues éstas explicarán una mayor cantidad de información.\n\nggcorrplot(cor(datos[, 2:8]), type = \"lower\", lab = T, show.legend = T)\n\n\n\n\n\n\n\n\nEn este caso, se ha generado un gráfico entre variables, sin tener en cuenta la correlación de la variable consigo misma, pues siempre será del 100%. En términos absolutos, vemos lo siguiente:\n\nHay varias correlaciones moderadas/altas como entre las variables T_nat y T_mort con 74% y T_nat e Ind_envej con 88%. Por otro lado con signo contrario, Médicos y Enfermeros 0.7%.\nHay varias correlaciones medias como puede ser entre Médicos y Tasa_enf con un 59%, Indice_envej y Tasa_enf con 64% o T_mort y Tasa_enf con 58%.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#introducción-1",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#introducción-1",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Introducción",
    "text": "Introducción\nEl Análisis de Componentes Principales (PCA) es una técnica para reducir la complejidad de conjuntos de datos con múltiples variables. Su objetivo es transformar variables correlacionadas en un conjunto menor de dimensiones sin perder la mayor parte de la información original.\nSe busca encontrar nuevas variables (componentes) que estén incorrelacionadas y que capturen la máxima variabilidad de los datos. Esto se logra mediante combinaciones lineales de las variables originales. PCA es útil para entender relaciones, reducir dimensiones y manejar la alta correlación entre variables.\nPara aplicar PCA, se necesitan datos cuantitativos y es crucial escalar las variables (estandarizar = media cero y varianza uno). Esto garantiza que ninguna variable domine el análisis. Además, se puede trabajar con la matriz de correlaciones para abordar fuertes correlaciones entre variables, manteniendo así la información más relevante del conjunto de datos.\nLos pasos generales son:\n\nEstandarización de las variables: Es importante estandarizar las variables numéricas para que tengan media cero y desviación estándar uno. Esto es crucial para que ninguna variable domine el análisis debido a su escala.\nCálculo de la matriz de correlaciones o covarianzas: Dependiendo del enfoque, se puede trabajar con la matriz de correlaciones si se busca abordar fuertes correlaciones entre variables, o con la matriz de covarianzas si se busca la varianza total de las variables.\n\n\nNOTA: Aconsejable trabajar siempre con la matriz de correlaciones (a no ser que todas variables estén en las mismas unidades, que se podrá usar la matriz de covarianzas). De no seguir esta nota y usar la matriz de covarianzas, las variables que tienen mayores unidades dominarán la estructura de covarianza, lo que llevará a una representación inexacta de la variabilidad real de los datos.\n\n\nDescomposición de la matriz: Se descompone la matriz de correlaciones en sus vectores y valores propios. Los valores propios representan la cantidad de varianza explicada por cada componente principal, mientras que los vectores propios (autovectores) determinan la dirección de cada componente en el espacio multidimensional original.\nSelección de componentes: Los componentes se ordenan de manera descendente según la cantidad de varianza que explican. Los primeros componentes capturan la mayor variabilidad de los datos y se seleccionan para reducir la dimensionalidad manteniendo la información más relevante.\nTransformación de datos: Proyectar los datos originales en el espacio de los componentes principales para obtener las nuevas variables. Estas son combinaciones lineales de las variables originales y son ortogonales entre sí. Esta transformación lineal conserva la mayor parte de la información en un espacio de menor dimensión, lo que facilita el análisis y la visualización de los datos.\nInterpretación y visualización: Explorar la importancia de cada componente en términos de la variabilidad explicada. Se pueden interpretar los componentes para comprender qué aspectos de los datos capturan. Si es posible, representar gráficamente los datos en el espacio reducido de los componentes principales para obtener una mejor comprensión de las relaciones entre las observaciones.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#modelo",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#modelo",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Modelo",
    "text": "Modelo\nEn las siguientes lineas haremos que la variable CCAA se ponga como nombre de filas y posteriormente eliminaremos esa variable ya que ya la tendremos como nombre de filas.\n\nCCAA &lt;- datos$CCAA\ndatos &lt;- datos[, -1] # Eliminamos ahora\nrownames(datos) &lt;- CCAA # Como nombres de filas las CCAA\n\nEscalamos los datos y calculamos la matriz de varianzas covarianzas, mostramos solo la diagonal (debería ser 1).\n\ndatos2 &lt;- scale(datos)\nsummary(datos2)\n\n     T_nat             T_mort           Ind_envej          Médicos       \n Min.   :-1.9754   Min.   :-1.54244   Min.   :-1.3485   Min.   :-1.6493  \n 1st Qu.:-0.8131   1st Qu.:-0.58810   1st Qu.:-0.6002   1st Qu.:-0.8126  \n Median : 0.1554   Median : 0.09356   Median :-0.3737   Median :-0.2598  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.7013   3rd Qu.: 0.53665   3rd Qu.: 0.3701   3rd Qu.: 0.9752  \n Max.   : 1.9780   Max.   : 1.84885   Max.   : 2.1860   Max.   : 1.4942  \n   Enfermeros        Inacmedicam         Tasa_enf       \n Min.   :-1.62982   Min.   :-1.5146   Min.   :-1.95741  \n 1st Qu.:-0.73853   1st Qu.:-0.6173   1st Qu.:-0.60143  \n Median :-0.04542   Median : 0.1678   Median : 0.07583  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.56682   3rd Qu.: 0.6645   3rd Qu.: 0.81849  \n Max.   : 2.55746   Max.   : 1.8020   Max.   : 1.48703  \n\ndiag(var(datos2))\n\n      T_nat      T_mort   Ind_envej     Médicos  Enfermeros Inacmedicam \n          1           1           1           1           1           1 \n   Tasa_enf \n          1 \n\n\nAplicamos función PCA, notar que en este caso no haría falta los argumentos SCALE=TRUE y CENTER=TRUE puesto que ya hemos escalado dos datos en un paso previo. Por defecto en la función viene el valor de SCALE=FALSE y CENTER=TRUE.\n\npca &lt;- prcomp(datos2, center = TRUE, scale = TRUE) # Scale=T\n\nCalculamos los coeficientes de la ecuación para cada componente principal (Autovectores)\n\npca$rotation\n\n                   PC1        PC2          PC3         PC4          PC5\nT_nat       -0.4270118  0.3083847 -0.008988199  0.50247373 -0.348115698\nT_mort       0.4701640 -0.2430823 -0.113411927  0.01113627 -0.455987748\nInd_envej    0.5036940 -0.1972515 -0.163033713 -0.10631657  0.049561195\nMédicos      0.3435595  0.5298832  0.176924105  0.13246428  0.623322284\nEnfermeros   0.2330590  0.5761666  0.404540649 -0.37517212 -0.510098955\nInacmedicam -0.1295046  0.3910394 -0.811238715 -0.40406462 -0.004102112\nTasa_enf     0.3921454  0.2030906 -0.327739876  0.64383523 -0.140306382\n                    PC6         PC7\nT_nat        0.40168978  0.43296643\nT_mort       0.62648091 -0.32629331\nInd_envej   -0.02085919  0.81646163\nMédicos      0.40041787 -0.05896305\nEnfermeros  -0.21521711  0.05281068\nInacmedicam  0.08658092 -0.03777880\nTasa_enf    -0.48123047 -0.17824227\n\n\nPodemos observar aquí nuestras variables en el nuevo sistema de coordenadas (las componentes principales), dando lugar a ecuaciones de cada eje como combinación lineal del total de variables. Analizar el vector que crea cada componente y cuáles son los pesos que tienen las variables en cada componente, ayuda a interpretar qué tipo de información recoge cada una de ellas.\nPor ejemplo, la primera componente principal (PC1), presenta la siguiente ecuación, como combinación lineal de las siete variables originales:\n$ PC_1 = -0.42 {T_nat} + 0.47 {T_mort} + 0.50Ind_envej + 0.34 Médicos + 0.23 Enfermeros -0.12 Inacmedicam + 0.39 Tasa_enf $\nExtraemos las nuevas coordenadas de los individuos (puntuaciones)\nAdemás, podemos ver las puntuaciones, que son las coordenadas de cada observación original (Comunidad Autónoma) sobre los nuevos ejes construidos (componentes principales). Esto corresponde a un cambio de coordenadas bajo el paradigma del Álgebra Lineal.\n\npca$x\n\n                                    PC1         PC2         PC3         PC4\nAndalucía                   -2.29780864 -1.17108641  0.91626309 -0.20305801\nAragón                       1.00976268  0.77424874 -0.15274606  0.43908296\nAsturias, Principado de      4.02433299 -1.03317546  0.49808668  0.37042340\nBaleares, Islas             -2.13246462 -0.54804063 -0.03645048  0.75801563\nCanarias                    -1.83025928 -0.41486990 -0.21320240 -2.06824011\nCantabria                    1.41475192 -0.21256737  1.88652836 -0.65976823\nCastilla y León              2.70411492 -0.18487892 -0.62652953 -0.18199384\nCastilla-La Mancha          -1.32760977 -1.94021471  1.06434279 -0.04561078\nCataluña                    -0.33553404  0.70136546  0.27442715  1.02159787\nComunidad Valenciana        -1.04717225 -0.08800225 -1.28286604 -0.35378816\nExtremadura                  0.51098330 -0.31670565  0.56679839 -0.05840686\nGalicia                      1.79132178 -1.49115973 -1.75060101 -0.19506453\nMadrid, Comunidad de        -0.61827496  1.70803736  1.25152616  0.76867280\nMurcia, Región de           -2.97739482 -0.02854523 -1.04491055  0.87136976\nNavarra, Comunidad Foral de -0.08425661  3.17323869 -0.15708545 -1.05193105\nPaís Vasco                   1.12618630  1.06177566 -0.42329241  0.08245285\nRioja, La                    0.06932110  0.01058035 -0.77028869  0.50624630\n                                     PC5          PC6         PC7\nAndalucía                   -0.006424862  0.573688653  0.14890360\nAragón                       0.665940100  0.695838881 -0.20598823\nAsturias, Principado de      0.225286205 -0.183607951  0.07438375\nBaleares, Islas              0.211985810 -0.922992167  0.03286732\nCanarias                     0.689522809 -0.396000896 -0.07004163\nCantabria                    0.177812778  0.155155518 -0.07658536\nCastilla y León             -0.038962514  0.146942499  0.11730269\nCastilla-La Mancha          -0.632263777  0.112965899 -0.02533346\nCataluña                    -0.328778916 -0.570291134 -0.12593594\nComunidad Valenciana         0.100452783 -0.005465301 -0.14143116\nExtremadura                 -0.863974957  0.020484318 -0.09478002\nGalicia                     -0.001413730 -0.153263421  0.16331588\nMadrid, Comunidad de         0.724845530 -0.162259381  0.16899413\nMurcia, Región de            0.076244013  0.631672722  0.09459935\nNavarra, Comunidad Foral de -0.802968126 -0.003989889  0.13676882\nPaís Vasco                   0.184217821  0.035413303 -0.06901077\nRioja, La                   -0.381520967  0.025708347 -0.12802897\n\n\nPor ejemplo, en un gráfico bidimensional con los dos primeros ejes, Andalucía toma la posición (-2.29,-1.17), o por otro lado, el País Vasco se encuentra en (1.12, 1.06). Dando una interpretación a cada eje, podremos determinar qué perfil tiene cada CC.AA. dentro del estudio.\nVarianza explicada por cada componente principal\nUna vez calculadas las componentes principales, es de interés conocer la varianza explicada por cada una, ya que el principal objetivo que se sigue con PCA es maximizar la cantidad de información explicada por las componentes.\n\nsummary(pca)\n\nImportance of components:\n                         PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.882 1.2322 0.9619 0.77556 0.47878 0.41008 0.12381\nProportion of Variance 0.506 0.2169 0.1322 0.08593 0.03275 0.02402 0.00219\nCumulative Proportion  0.506 0.7229 0.8551 0.94104 0.97379 0.99781 1.00000\n\n\n\nStandard deviation: muestra las desviaciones estándar de cada componente principal. Si elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\nProportion of Variance: es la proporción de la varianza total que explica cada componente principal y quizá, es la fila más importante de nuestros resultados. Como los autovalores están ordenados de mayor a menor y así son construidas las componentes principales, la primera componente principal es la que mayor porcentaje de variabilidad explica, un 37%. Así, la varianza explicada por la componentes van en orden decreciente, teniendo que la segunda componente explica un 31% y la tercera, un 12%.\nCumulative proportion: es la varianza acumulada y se calcula progresivamente sumando la Proportion of Variance anterior. En vista de estos resultados, vemos que la primera componente agrupa el 37% de la variación, y que necesitamos 3 componentes para alcanzar el 80%.\n\nSi elevamos al cuadrado estas desviaciones, tenemos la varianza (el autovalor correspondiente). Es decir, la varianza explicada por cada componente corresponde con los autovalores de la matriz de covarianzas de los datos estandarizados.\n\n# Autovalues\npca$sdev^2 # varianza de cada componente\n\n[1] 3.54217490 1.51832920 0.92527315 0.60150104 0.22922822 0.16816380 0.01532969",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#selección-de-componentes",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#selección-de-componentes",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Selección de componentes",
    "text": "Selección de componentes\nGraficando el valor de la varianza de cada componente principal, podemos observar los resultados comentados anteriormente, que las primeras componentes son las que más varianza explican y que a medida que se añaden más, la varianza explicada por cada una es menor.\n\nfviz_eig(pca, main = \"Varianza de cada componente\", choice = \"eigenvalue\", addlabels = T)\n\n\n\n\n\n\n\n\no como el porcentaje de varianza explicada por cada componente sobre el total.\n\nfviz_screeplot(pca, addlabels = TRUE, main = \"Porcentaje de varianza explicada por cada componente (%)\")\n\n\n\n\n\n\n\n\nA continuación, representamos las varianzas acumuladas:\n\nplot(summary(pca)$importance[3, ], type = \"o\", col = \"darkblue\", lwd = 3, main = \"Porcentaje de varianza acumulada\", xlab = \"Componente Principal\", ylab = \"Porcentaje de varianza acumulada\")\n\n\n\n\n\n\n\n\nDeterminar el número de componentes que elegir para continuar con el análisis no tiene unas normas determinadas a seguir. Respecto a ello, existen varios criterios con sus respectivas propuestas.\n\nUna opción para determinar el número de componentes principales que seleccionar, es coger aquellas tal que expliquemos un % determinado de la variabilidad de los datos que nosotros prefijemos. Generalmente se pone como umbral mínimo un 80%, entonces necesitaríamos elegir 3 componentes.\nOtra posibilidad es seguir el criterio de Kaisser, que escoge aquellas componentes cuyo autovalor sea superior a 1 (cuando las variables han sido generadas a partir de la matriz de correlaciones). Según este criterio y mirando el gráfico anterior de la varianza (igual a eigenvalues), elegiríamos las dos primeras componentes.\nPara relajar el criterio de Kaisser, existe la modificación de Jollife, que elige aquellas componentes cuyo autovalor sea superior a 0.7. Esta modificación, nos permite elegir igualmente 3 componentes.\n\nEn este caso, nos podríamos quedar con las 3 primeras componentes principales, ya que es el número en el que coincide el mayor número de criterios. Por tanto, en lugar de trabajar con las 7 variables originales, trabajaremos con 3 variables nuevas, que son combinaciones de ellas.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#interpretación",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#interpretación",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Interpretación",
    "text": "Interpretación\nHemos decidido quedarnos con 3 componentes principales, que explican el 85% de la variabilidad total. Para realizar su interpretación, volvemos a ver los coeficientes de las ecuaciones de los componentes, observando cuáles son los valores más altos (en valor absoluto), para así poder dar una interpretación a cada eje.\n\npca$rotation[, 1:3]\n\n                   PC1        PC2          PC3\nT_nat       -0.4270118  0.3083847 -0.008988199\nT_mort       0.4701640 -0.2430823 -0.113411927\nInd_envej    0.5036940 -0.1972515 -0.163033713\nMédicos      0.3435595  0.5298832  0.176924105\nEnfermeros   0.2330590  0.5761666  0.404540649\nInacmedicam -0.1295046  0.3910394 -0.811238715\nTasa_enf     0.3921454  0.2030906 -0.327739876\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nSi nos fijamos en los pesos más altos, podemos darle una interpretación a cada eje. Por ejemplo:\n\nLa primera componente explica un 50% de la variación. Hay valores absolutos bastante similares y elevados, que son los correspondientes con las variables Ind_envej, T_mort, T_nat y Tasa_enf. Por lo tanto, parece que la primera componente recoge información demográfica. Teniendo en cuenta los signos podemos concluir que las CC.AA. que se sitúen a la derecha del eje serán aquellas con mayor Tasa de mortalidad, mayor Índice de envejecimiento, mayor Tasa de incidencia de enfermedades en la población, y en contraposición, menor Tasa de natalidad.\nEn la segunda componente, los pesos más elevados corresponden con las variables Médicos y Enfermeros, representando de alguna forma, los recursos sanitarios de las CCAA. Ambas variables contribuyen de forma positiva al eje, por lo que cuanto más a la derecha del eje se sitúe una CC.AA., mayores recursos de personal sanitario posee.\nPara la tercera componente, el peso más elevado, corresponde a la variable medidora de la inaccesibilidad de la población a los medicamentos recetados . La variable puntúa negativamente en el eje, de forma que las Comunidades con mayor valor en esta componente, son aquellas con menor inaccesibilidad a los medicamentos.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#representación-gráfica",
    "href": "notebooks/Reduccion Dimension/PCA/Situacion_sanitaria/Situacion_sanitaria.html#representación-gráfica",
    "title": "Reducción Dimensionalidad - PCA: Situacion_sanitaria",
    "section": "Representación gráfica",
    "text": "Representación gráfica\nGráfico de las variables\nRepresentamos sobre las dos primeras componentes principales las variables originales. En el eje de abscisas se representa la PC1 y en el eje de ordenadas, la PC2. Para interpretar correctamente las variables tenemos que fijarnos en la longitud de la flecha y en el ángulo que forman respecto a los ejes y entre ellos mismos.\n\nÁngulo vector - eje: cuanto más paralelo es un vector al eje, más ha contribuido a dicha componente principal.\nÁngulo entre dos vectores: si es pequeño representa una alta correlación entre las variables implicadas (y por tanto, observaciones con valores altos en una variable, tendrá valores altos en la otra). Si el ángulo es cercano a 90º indica que las variables están incorreladas y los ángulos opuestos indican correlación negativa entre ellas.\nLongitud: cuanto mayor es la longitud de un vector, mayor varianza de la variable está contenida en el biplot, es decir, mejor representada está en el gráfico.\n\nEn el gráfico, diferenciamos por colores las variables según su calidad de representación en las dos primeras componentes. Cuanto más cerca esté una variable del círculo de correlaciones, mejor será su representación, por lo que las variables que estén muy cerca del centro de la gráfica son las menos importantes para las dos primeras componentes.\n\nfviz_pca_var(pca, axes = c(1, 2), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\nfviz_pca_var(pca, axes = c(1, 3), col.var = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE)\n\n\n\n\n\n\n\n\n\nSi nos fijamos en el eje de abscisas, vemos como T_mort, Ind_envej, Tasa_enf y T_nat son las variables con menor ángulo respecto a él, indicando que han contribuido a la formación de la PC1. Las tres primeras variables se sitúan a la derecha del eje (contribución positiva), mientras que T_nat lo hace a la izquierda (contribución negativa). Es llamativo el pequeño ángulo formado por T_mort e Ind_envej, y es debido a la alta correlación entre ambas, que recordamos que era del 92%).\nEn cuanto al eje de ordenadas, vemos que las variables que forman un menor ángulo respecto a él, son Médicos y Enfermeros, siendo las que más contribuían a la formación de la PC2, ambas de forma positiva.\n\nRESUMEN DE RESULTADOS\nLos gráficos obtenidos muestran una visualización de las variables en función de las componentes principales 1 y 2, y 1 y 3. Las variables que tienen una correlación alta con la primera componente principal están más cerca del eje horizontal y las variables que tienen una alta correlación con la segunda componente principal están más cerca del eje vertical. Las variables que se encuentran cerca del centro del gráfico tienen una correlación baja con ambas componentes principales, aún así nos guiaremos con la tabla para decidir qué variables están mejor explicadas por cada una de las componentes. En resumen, estos gráficos proporcionan una representación visual de las relaciones entre las variables en función de las dos primeras componentes principales y las dos segundas, lo que puede ayudar a identificar patrones y tendencias en los datos.\nEn el siguiente gráfico podemos ver las correlaciones de dichas variables con las componentes principales, como ya hemos comentado.\n\ncorr_var &lt;- pca$rotation %*% diag(pca$sdev)\ncolnames(corr_var) &lt;- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\")\ncorrplot(corr_var)\n\n\n\n\n\n\n\n\nEn cuanto a este gráfico, es llamativo como las dos o tres primeras componentes son las más importantes en el PCA, sobre todo, la PC1.\nEn resumen, las nuevas componentes han permitido identificar patrones y características de las comunidades autónomas en términos de la situación sanitaria y indicadores relativos a esta.\nGráfico de los individuos\nTras observar la representación de las variables, en este apartado vemos la representación de los individuos sobre los nuevos ejes, con la idea de que aquellos con características similares, se agrupan cerca al tener puntuaciones parecidas. Las comunidades con valores cercanos a la media se situarán cerca del centro del gráfico (0,0).\nRepresentando los individuos sobre PC1 y PC2, vemos que Comunidades como Canarias y Baleares, o Aragón y el País Vasco están muy próximas entre sí, indicando que tienden a tener un nivel sanitario similar.\n\n# Sobre PC1 y PC2\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(1, 2))\n\n\n\n\n\n\n\n\n\nfviz_pca_ind(pca, col.ind = \"cos2\", gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), repel = TRUE, axes = c(1, 3))\n\n\n\n\n\n\n\n\nDel mismo modo, podemos representar las Comunidades sobre PC1 y PC3.\nEn este caso, las CC.AA. con menor porcentaje de población con inaccesibilidad a los medicamentos son Cantabria, Madrid y Castilla-La Macha, mientras que las Comunidades en las que un mayor porcentaje de población sufre estos problemas son Galicia, la Comunidad Valenciana o Murcia.\nBiplot\nPara poder extraer fácilmente los perfiles, podemos combinar las variables e individuos en un solo gráfico que nos permita identificar qué Comunidades se encuentran en una situación parecida y además, que nos permita identificar sus características.\nEl biplot permite la representación conjunta de los individuos y las variables sobre los nuevos ejes. Para que el resultado sea fácilmente interpretable, debemos tener pocas variables e individuos en el conjunto de datos.\n\nfviz_pca_biplot(pca, repel = TRUE, col.var = \"deeppink\", col.ind = \"#696969\")\n\n\n\n\n\n\n\n\nLos ejes inferior e izquierdo representan las puntuaciones o scores de cada observación sobre las componentes principales. Por ejemplo, las coordenadas de Andalucía son (-2.29, -1.17) o las de Madrid, (-0.61, 1.7).\nSobre ellos, se han superpuesto a escala, las variables tenidas en cuenta en el análisis.\nPerfiles\n\nRespecto a los extremos del eje PC1, observamos dos grupos diferenciados. Por una parte, (Murcia, Canarias Baleares y Andalucía) son las CC.AA. con mayor tasa de natalidad; mientras que (Asturias, Galicia, Castilla y León) son las que tienen una población más envejecida y con mayor tasa de morbilidad por enfermedades.\nRespecto al eje PC2, se tiene que Navarra es la Comunidad con mayor personal sanitario, seguida por Madrid. Entre las Comunidades con menores recursos sanitarios se encuentran Castilla-La Mancha, Galicia y Andalucía.\n\nTambién se puede destacar que La Rioja se encuentra justo en el origen de coordenadas (0,0), pudiendo considerarse representante de la media española en cuanto a las características demográficas y sanitarias.",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "PCA",
      "Situacion Sanitaria",
      "Reducción Dimensionalidad - PCA: Situacion_sanitaria"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/Dataset_cleaning.html",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/pisa_factorial/Dataset_cleaning.html",
    "title": "Procesamos el dataset para adaptarlo a lo de arriba",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente realizar un Análisis Factorial a partir de los microdatos del informe PISA(informe del Programa para la Evaluación Internacional de los Estudiantes) en España en el año 2018.\nPara ello, usaremos datos de la prueba PISA del año 2018 que pueden descargarse en el siguiente link: https://www.educacionyfp.gob.es/inee/bases-datos/evaluaciones-internacionales/pisa.html. Se pueden descargar en el siguiente link en formato SPSS o SAS y nosotros transformaremos a un excel los que nos interesan.\nTomamos el fichero “Datos_PISA_ESP.csv”\nEspecíficamente, estos datos proceden del Cuestionario de contexto del alumno, cuestionario que deben rellenar todos los alumnos que pasan PISA y que trata de medir la existencia de variables socioeconómicas, metacognitivas, motivacionales e, incluso, emocionales, que pueden tener impacto sobre el rendimiento académico. Entre las variables relacionadas con la motivación podemos encontrar:\n\nLa afición por la lectura.\nLa actitud hacia la Educación.\nLa competitividad.\nLa perseverancia.\nEl miedo al fracaso.\nLa autoeficacia.\nLa orientación a metas de aproximación a la maestría.\n\nPero, para simplificar más las cosas y no trabajar con un número considerable de factores, nos vamos a centrar simplemente en tres: la competitividad, la perseverancia y el miedo al fracaso.\nLos items (preguntas) que miden respectivamente la competitividad, al perseverancia y el miedo al fracaso de los estudiantes son:\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST181Q02HA: Disfruto trabajando en situaciones que requieren competir con los demás.\nST181Q03HA: Es importante para mí hacerlo mejor que los demás al realizar una tarea.\nST181Q04HA: Me esfuerzo mucho cuando estoy compitiendo contra los demás.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones sobre ti mismo?\n\n\nST182Q03HA: Me siento satisfecho cuando me esfuerzo todo lo que puedo.\nST182Q04HA: Cuando inicio una tarea continúo hasta terminarla.\nST182Q05HA: Cuando hago algo, parte de mi satisfacción se debe a que he mejorado mis resultados anteriores.\nST182Q06HA: Si algo no se me da bien, prefiero seguir esforzándome para mejorar, en lugar de hacer otra cosa que sí se me da bien.\n\n\n¿Hasta qué punto estás de acuerdo con las siguientes afirmaciones?\n\n\nST183Q01HA: Cuando me he equivocado, me preocupa lo que otras personas piesen de mí.\nST183Q02HA: Cuando me he equivocado, me preocupa no tener el talento suficiente.\nST183Q03HA: Cuando me he equivocado, dudo sobre mis planes para el futuro.\n\nLa escala de respuesta para estos tres conjuntos de ítems es la misma: 1 - Totalmente en desacuerdo, 2 - En desacuerdo, 3 - De acuerdo y 4 - Totalmente de acuerdo.\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Librería tratamiento dataframes\nlibrary(dplyr)\n\n\n# Microdatos\n# Fichero Personas\ndatos &lt;- read.csv(\"/Users/davpero/Downloads/Datos_PISA_ESP.csv\", sep=\";\", dec=\",\")\n#Este csv esta separado por ; y los decimales son ,\ndim(datos) #Tenemos 35943 observaciones y 37 columnas\nhead(datos)\n#Vamos a extraer simplemente las columnas de identificador del alumno y las de los items que nos interesan (los que miden la competitividad, la perseverancia y el miedo al fracaso):\ndatos &lt;- datos[,c(\"CNTSTUID\", \"ST181Q02HA\", \"ST181Q03HA\", \"ST181Q04HA\", \"ST182Q03HA\", \"ST182Q04HA\", \"ST182Q05HA\", \"ST182Q06HA\", \"ST183Q01HA\", \"ST183Q02HA\", \"ST183Q03HA\")]\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos, \"../../../../files/pisa_factorial.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. pisa_factorial.xlsx\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "Pisa Factorial",
      "Procesamos el dataset para adaptarlo a lo de arriba"
    ]
  },
  {
    "objectID": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/Dataset_cleaning.html",
    "href": "notebooks/Reduccion Dimension/Analisis Factorial/ECV_factorial/Dataset_cleaning.html",
    "title": "Procesamos el dataset para adaptarlo a lo de arriba",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente realizar un Análisis Factorial a partir de los microdatos de la Encuesta de Condiciones de Vida (ECV) del año 2013.\nPara ello, usaremos datos transversales de la ECV del año 2013. Los microdatos de la ECV 2013 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176807&menu=resultados&idp=1254735976608#!tabs-1254736195153. El .zip que se descargue contiene dos subcarpetas; datos_ecv2013 que será el que usemos; disreg_ecv2013 que no lo utilizaremos por el momento.\nDentro de la carpeta datos_ecv2013, vamos a tomar:\n\nFichero P: datos detallados de los adultos (esudb13p.csv). En dicho fichero se encuentran las preguntas del módulo Transmisión intergeneracional de la pobreza y con el analizaremos la estructura factorial de la siguiente batería de preguntas relacionadas con el bienestar:\n\n¿Cuál es su grado de satisfacción global con…\n\n… su vida en la actualidad? (variable PW010).\n… la situación económica en su hogar? (PW030).\n… su vivienda? (PW040).\n… su trabajo actual? (PW100).\n… el tiempo que dispone para hacer lo que le gusta? (PW120).\n… sus relaciones personales? (PW160).\n… las áreas recreativas o verdes de la zona en la que vive? (PW200).\n… la calidad de la zona en la que vive? (PW210).\n\nEn todos los casos la escala de respuesta va del 0 (Nada satisfecho) al 10 (Plenamente satisfecho).\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Librería tratamiento dataframes\nlibrary(dplyr)\n\n\n# Microdatos\n# Fichero Personas\ndatos &lt;- read.csv(\"/Users/davpero/Desktop/esudb13p.csv\", sep = \",\")\nVamos a crear una variable en el conjunto de datos datos_Fichero_P que sea igual a la variable identificadora del hogar HB030. Para ello, debemos quitar los dos últimos dígitos a la variable PB030. De esta forma, tendremos dos variables identificadoras idénticas que nos permitirán realizar la unión entre tablas.\n# PB030 identificador único de la persona\ndatos &lt;- datos[, c(\"PB030\", \"PW010\", \"PW030\", \"PW040\", \"PW100\", \"PW120\", \"PW160\", \"PW200\", \"PW210\")]\nhead(datos)\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos, \"../../../../files/ECV_factorial.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. ECV_factorial.xlsx\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Reduccion Dimension",
      "Analisis Factorial",
      "ECV Factorial",
      "Procesamos el dataset para adaptarlo a lo de arriba"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "",
    "text": "A continuación se va a explicar como modelizar una serie temporal con un ARIMA y todas las consideraciones que se deben tener en cuenta. Se llevará a cabo un ejemplo práctico a partir de un conjunto de datos, mostrando como interpretar los resultados que se van obteniendo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado ipc_series.xlsx. Este dataset presenta el IPC Español desde el año 2002 hasta el 2022 con una periodicidad mensual. El objetivo será analizar dicha serie temporal mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nIPC: Índice de Precios al consumo.\nfecha: Fecha correspondiente.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene el IPC de España mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#dataset",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#dataset",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado ipc_series.xlsx. Este dataset presenta el IPC Español desde el año 2002 hasta el 2022 con una periodicidad mensual. El objetivo será analizar dicha serie temporal mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nIPC: Índice de Precios al consumo.\nfecha: Fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene el IPC de España mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#librerías",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#librerías",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "Librerías",
    "text": "Librerías\nEn este apartado se van a cargar todas las librerías necesarias para ejecutar el resto del código. Se recomienda instalarlas en caso de no disponer de ellas.\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts\nlibrary(purrr) # Hacer combinaciones de listas\nlibrary(tseries) # Para hacer el adf.test de estacionariedad\n\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/ipc_series.xlsx\", sheet = \"Datos\")\n\ndata$ipc &lt;- data$`Pobl. Total`\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 252   2\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [252 × 2] (S3: tbl_df/tbl/data.frame)\n $ Fecha: POSIXct[1:252], format: \"2022-12-01\" \"2022-11-01\" ...\n $ IPC  : num [1:252] 110 110 110 109 110 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#introducción-1",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#introducción-1",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "Introducción",
    "text": "Introducción\nEl modelo ARIMA es uno de los modelos más comunes y poderosos para el análisis de series temporales. Se utiliza para modelar y predecir datos que exhiben comportamientos de tendencia y estacionalidad.\n\nComponentes del modelo ARIMA:\n\nAR (Auto-regresivo): Representa la relación entre una observación actual y un número determinado de observaciones anteriores (retardos). p es el componente autoregresivo, que indica cuántas observaciones pasadas influyen en la observación actual. Para determinar el valor de p, se puede utilizar el gráfico de la función de autocorrelación parcial (PACF) de la serie diferenciada. Las barras que se salen significativamente del intervalo de confianza pueden indicar el orden de p que debería considerarse. Se recomienda ser conservador y elegir un número reducido de los valores más prominentes.\nI (Integrated):. Representa el número de diferencias necesarias para hacer estacionaria la serie temporal. d es el número de diferenciaciones necesarias para hacer que la serie sea estacionaria. Esto se determina mediante pruebas estadísticas como el test de Dickey-Fuller aumentado (ADF test). Es importante tener cuidado de no sobrediferenciar la serie, lo que se puede observar en el gráfico de la función de autocorrelación (ACF) si los valores comienzan a ser negativos rápidamente.\nMA (Media Móvil): Representa la relación entre una observación actual y un error de predicción residual de observaciones anteriores. q es el componente de media móvil, que indica cuántos términos de los residuos anteriores influyen en la observación actual. Para determinar el valor de q, se utiliza el gráfico de la función de autocorrelación (ACF) de la serie diferenciada. Los términos MA son esencialmente errores de pronóstico retrasados, y el ACF muestra cuántos términos MA se necesitan para eliminar la autocorrelación en la serie estacionaria. Se sugiere seleccionar tantos términos MA como los lags que estén significativamente por encima del intervalo de confianza.\n\n\nSi la serie está ligeramente por debajo del nivel de diferenciación adecuado (subdiferenciada), se pueden agregar uno o más términos de AR adicionales. Por otro lado, si la serie está sobrediferenciada, se puede considerar agregar términos MA adicionales para mejorar el modelo.\nHasta ahora, hemos restringido nuestra atención a datos no estacionales y modelos ARIMA no estacionales. Sin embargo, los modelos ARIMA también son capaces de modelar una amplia gama de datos estacionales.\nUn modelo SARIMA estacional se forma incluyendo términos estacionales adicionales en los modelos ARIMA que hemos visto hasta ahora. Está escrito de la siguiente manera:",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#modelo",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#modelo",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "Modelo",
    "text": "Modelo\nSabiendo que los datos tienen frecuencia mensual y parece razonable pensar que la población puede tener la misma tendencia cada año en los mismos meses (por ej en verano crecer y en invierno decrecer), vamos a considerar diferenciar la serie 12 veces. Es decir, plantearemos un modelo Seasonal ARIMA. La dibujamos de nuevo a ver si es necesario diferenciar una vez más para eliminar la tendencia.\n\n# Serie diferenciada\n# dif 12 veces\ntt &lt;- diff(tss, 12)\nplot(tt)\n\n\n\n\n\n\n\n\nNecesitamos remover la tendencia luego diferenciamos de nuevo.\n\n# Serie diferenciada\ntt2 &lt;- diff(tt, 1)\nplot(tt2)\n\n\n\n\n\n\n\n\n\n# Las dibujamos\npar(mfrow = c(2, 1), mar = c(3, 3, 3, 2) + 0.1)\nplot(tt)\nplot(tt2)\n\n\n\n\n\n\n\n# TEST para ver estacionariedad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(tt, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  tt\nDickey-Fuller = -3.5734, Lag order = 6, p-value = 0.03643\nalternative hypothesis: stationary\n\nadf.test(tt2, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  tt2\nDickey-Fuller = -4.4132, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nYa se ha ido la tendencia luego parece razonable pensar que la serie ya es estacionaria aunque la varianza aumente un poco al final, es decir, tomamos D=1, d=1. Examinemos ahora ACF/PACF para determinar los p,q y P,Q.\n\n# ACF plot\nAcf(tt2, main = \"ACF para la serie diferenciada 2 vez\", lag.max = 50, ylim = c(-1, 1))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de ACF vemos que hay 1 o 2 barras que se salen notablemente del límite deseado (dentro del periodo m=12 meses), luego tomaríamos como q= 1, o 2.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de ACF vemos que hay 1 barra que se sale notablemente del límite deseado (dentro de lags múltiplos m=12 meses), luego tomaríamos como Q= 1.\n\n\n# PACF plot\nPacf(tt2, main = \"PACF para la serie diferenciada 2 vez\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de PACF vemos que hay 2 o 3 barras que se salen notablemente del límite deseado (dentro del periodo m=12 meses), luego tomaríamos como p= 2, o 3.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de PACF vemos que hay 1 barra que se sale notablemente del límite deseado (dentro de lags múltiplos m=12 meses), luego tomaríamos como P= 1.\n\n\n# Modelo seleccionado\narima_model_1 &lt;- arima(tss, order = c(2, 1, 1), seasonal = c(1, 1, 1))\n\n\n# Resumen\nsummary(arima_model_1)\n\n\nCall:\narima(x = tss, order = c(2, 1, 1), seasonal = c(1, 1, 1))\n\nCoefficients:\n         ar1      ar2      ma1    sar1     sma1\n      0.9010  -0.0384  -0.7035  0.0250  -0.8353\ns.e.  0.1456   0.0859   0.1288  0.1052   0.0821\n\nsigma^2 estimated as 0.1352:  log likelihood = -106.94,  aic = 225.88\n\nTraining set error measures:\n                        ME     RMSE       MAE          MPE      MAPE      MASE\nTraining set -0.0001529009 0.358482 0.2290193 -0.002083539 0.2460203 0.5095319\n                    ACF1\nTraining set 0.003566202\n\n# Diagnóstico del modelo\ncheckresiduals(arima_model_1)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,1)(1,1,1)[12]\nQ* = 27.523, df = 19, p-value = 0.09305\n\nModel df: 5.   Total lags used: 24\n\n# H0: no hay autocorrelación residual en los residuos del model\n# Queremos ver que se prueba H0\nBox.test(residuals(arima_model_1), type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  residuals(arima_model_1)\nX-squared = 0.0032432, df = 1, p-value = 0.9546\n\n# Aceptamos H0 puesto que es &gt;0.05\n\n\nEl Ljung-Box test tiene un p_val&gt;0.05 luego podemos aceptar H0 y asumir que los errores son independientes. Habría que ver para otros modelos. Esto hará que puede que el ajuste del todo no sea bueno.\nGráfico Residuos vs. Índice: Este gráfico muestra los residuos en función del índice de las observaciones. Idealmente, los residuos deberían estar dispersos alrededor de cero sin ningún patrón discernible (parece que si lo están).\nGráfico Autocorrelación de Residuos: muestra la autocorrelación de los residuos a diferentes rezagos. Se espera que los residuos no estén correlacionados entre sí, lo que se reflejaría en que los puntos estén dentro de las bandas de confianza. Sólo sobresale una línea luego no está del todo mal.\nHistograma de Residuos: Muestra la distribución de los residuos. Si los residuos se distribuyen normalmente alrededor de cero, el histograma debería parecerse a una distribución normal. (parece que si, aunque con ciertos valores que hacen las colas muy largas)",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#sarima-automático",
    "href": "notebooks/Series Temporales/ARIMA/ipc_series/ipc_series.html#sarima-automático",
    "title": "Series Temporales - ARIMA: ipc_series",
    "section": "SARIMA automático",
    "text": "SARIMA automático\nExiste una función que permite identificar los parámetros de manera automática.\n\n# Identificar los parámetros del modelo SARIMA de manera automática\nautomatic &lt;- auto.arima(tss)\nsummary(automatic)\n\nSeries: tss \nARIMA(0,1,1)(1,0,0)[12] \n\nCoefficients:\n         ma1    sar1\n      0.2830  0.7506\ns.e.  0.0619  0.0458\n\nsigma^2 = 0.1741:  log likelihood = -140.79\nAIC=287.58   AICc=287.68   BIC=298.16\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.04178451 0.4148127 0.2936567 0.04664786 0.3196839 0.1420576\n                     ACF1\nTraining set -0.003052993\n\ncheckresiduals(automatic)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(1,0,0)[12]\nQ* = 28.217, df = 22, p-value = 0.1686\n\nModel df: 2.   Total lags used: 24\n\n\n\narima_model_4 &lt;- arima(data.train, order = c(0, 1, 1), seasonal = c(1, 0, 0))\npred &lt;- forecast(arima_model_4, h = 4)\naccuracy(pred, data.test)\n\n                      ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set  0.04679622 0.4116954 0.288638  0.05081935 0.315117 0.1437884\nTest set     -1.20693923 1.2998107 1.206939 -1.09877743 1.098777 0.6012511\n                    ACF1 Theil's U\nTraining set -0.01175436        NA\nTest set     -0.02672161  10.13391\n\n# Gráfico con los datos originales y las predicciones de los modelos\nplot(tss,\n  xlab = \"Fecha\", ylab = \"Valor\", main = \"Comparación de Predicciones ARIMA\"\n)\npoints(pred$mean, col = \"green\", pch = 16, cex = 0.5) # Predicciones con arima_model1 en rojo\nlines(tss, col = \"blue\", lwd = 2) # Serie original en azul\nlines(fitted(arima_model_4), col = \"red\")\n\nlegend(\"bottomright\", legend = c(\"Original\", \"SARIMA auto. \", \"Predicciones\"), col = c(\"blue\", \"red\", \"green\"), lty = 1)\n\nlines(pred$lower[, 2], col = \"green\", lty = \"solid\")\nlines(pred$upper[, 2], col = \"green\", lty = \"solid\")\n\n\n\n\n\n\n\n\nVemos que el modelo propuesto por nosotros funciona algo mejor el modelo automático. Esto evidencia que como primer approach se puede usar la función auto.arima() obteniendo buenos resultados. No obstante es aconsejable estudiar el problema más a fondo para estudiar todas las posibles modelizaciones y tomar la que mejor se ajuste a nuestras expectativas.\nLas últimas observaciones no se predicen con tanta precisión porque puede que haya habido un cambio de comportamiento en la serie en estos últimos periodos.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Ipc Series",
      "Series Temporales - ARIMA: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "",
    "text": "A continuación se va a explicar como modelizar una serie temporal con un ARIMA y todas las consideraciones que se deben tener en cuenta. Se llevará a cabo un ejemplo práctico a partir de un conjunto de datos, mostrando como interpretar los resultados que se van obteniendo.\n\n\nEn este cuaderno vamos a analizar el dataset llamado navarra.xlsx. Este dataset presenta los datos de población de la Comunidad Autónoma de Navarra a 1 de Enero y 1 de Julio de cada año desde 1971. La serie temporal se encuentra desagregada por Sexos, mostrando para Hombres, Mujeres y el total de la población. El objetivo de este estudio es intentar modelar dichas series mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nccaa: Navarra siempre.\nsexo: Hombres, Mujeres, Total.\nfecha: Fecha correspondiente.\npoblación: Cifras de población.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene la población de Navarra mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#dataset",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#dataset",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado navarra.xlsx. Este dataset presenta los datos de población de la Comunidad Autónoma de Navarra a 1 de Enero y 1 de Julio de cada año desde 1971. La serie temporal se encuentra desagregada por Sexos, mostrando para Hombres, Mujeres y el total de la población. El objetivo de este estudio es intentar modelar dichas series mediante un modelo ARIMA.\nConcretamente en este dataset tenemos las siguientes variables:\n\nccaa: Navarra siempre.\nsexo: Hombres, Mujeres, Total.\nfecha: Fecha correspondiente.\npoblación: Cifras de población.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene la población de Navarra mediante un modelo ARIMA.\n\nExplorar patrones en la serie temporal.\nVer que la serie sea estacionaria.\nAplicar diferenciación si es necesario para estacionarizar la serie.\nIdentificar modelos ARIMA/SARIMA utilizando información de la exploración y funciones ACF y PACF.\nAjustar varios modelos ARIMA/SARIMA y seleccionar el mejor según métricas de ajuste.\nEvaluar la significancia estadística de los coeficientes del modelo ARIMA.\nInterpretar los coeficientes para comprender su influencia en la serie temporal.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#librerías",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#librerías",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "Librerías",
    "text": "Librerías\nEn este apartado se van a cargar todas las librerías necesarias para ejecutar el resto del código. Se recomienda instalarlas en caso de no disponer de ellas.\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts\nlibrary(purrr) # Hacer combinaciones de listas\nlibrary(tseries) # Para hacer el adf.test de estacionariedad\nlibrary(stats) # Para usar función fitted()\n\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/navarra.xlsx\",\n  sheet = \"Datos\", col_types = c(\n    \"date\",\n    \"numeric\", \"numeric\", \"numeric\"\n  )\n)\n\ndata$Total &lt;- data$`Pobl. Total`\ndata$H &lt;- data$`Pobl. Hombres`\ndata$M &lt;- data$`Pobl. Mujeres`\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPor otra parte, para tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 101   7\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [101 × 7] (S3: tbl_df/tbl/data.frame)\n $ Fecha        : POSIXct[1:101], format: \"2021-01-01\" \"2020-07-01\" ...\n $ Pobl. Total  : num [1:101] 662032 661008 659907 655668 652797 ...\n $ Pobl. Hombres: num [1:101] 327940 327374 326911 324729 323191 ...\n $ Pobl. Mujeres: num [1:101] 334092 333634 332996 330939 329606 ...\n $ Total        : num [1:101] 662032 661008 659907 655668 652797 ...\n $ H            : num [1:101] 327940 327374 326911 324729 323191 ...\n $ M            : num [1:101] 334092 333634 332996 330939 329606 ...\n\n\nVemos que estas variables (a excepción de las CCAA) son todas de tipo numérico, y además, podemos obtener información como la media, desviación típica, los cuartiles y el histograma de cada una.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#introducción-1",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#introducción-1",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "Introducción",
    "text": "Introducción\nEl modelo ARIMA es uno de los modelos más comunes y poderosos para el análisis de series temporales. Se utiliza para modelar y predecir datos que exhiben comportamientos de tendencia y estacionalidad.\n\nComponentes del modelo ARIMA:\n\nAR (Auto-regresivo): Representa la relación entre una observación actual y un número determinado de observaciones anteriores (retardos). p es el componente autoregresivo, que indica cuántas observaciones pasadas influyen en la observación actual. Para determinar el valor de p, se puede utilizar el gráfico de la función de autocorrelación parcial (PACF) de la serie diferenciada. Las barras que se salen significativamente del intervalo de confianza pueden indicar el orden de p que debería considerarse. Se recomienda ser conservador y elegir un número reducido de los valores más prominentes.\nI (Integrated):. Representa el número de diferencias necesarias para hacer estacionaria la serie temporal. d es el número de diferenciaciones necesarias para hacer que la serie sea estacionaria. Esto se determina mediante pruebas estadísticas como el test de Dickey-Fuller aumentado (ADF test). Es importante tener cuidado de no sobrediferenciar la serie, lo que se puede observar en el gráfico de la función de autocorrelación (ACF) si los valores comienzan a ser negativos rápidamente.\nMA (Media Móvil): Representa la relación entre una observación actual y un error de predicción residual de observaciones anteriores. q es el componente de media móvil, que indica cuántos términos de los residuos anteriores influyen en la observación actual. Para determinar el valor de q, se utiliza el gráfico de la función de autocorrelación (ACF) de la serie diferenciada. Los términos MA son esencialmente errores de pronóstico retrasados, y el ACF muestra cuántos términos MA se necesitan para eliminar la autocorrelación en la serie estacionaria. Se sugiere seleccionar tantos términos MA como los lags que estén significativamente por encima del intervalo de confianza.\n\n\nSi la serie está ligeramente por debajo del nivel de diferenciación adecuado (subdiferenciada), se pueden agregar uno o más términos de AR adicionales. Por otro lado, si la serie está sobrediferenciada, se puede considerar agregar términos MA adicionales para mejorar el modelo.\nHasta ahora, hemos restringido nuestra atención a datos no estacionales y modelos ARIMA no estacionales. Sin embargo, los modelos ARIMA también son capaces de modelar una amplia gama de datos estacionales.\nUn modelo SARIMA estacional se forma incluyendo términos estacionales adicionales en los modelos ARIMA que hemos visto hasta ahora. Está escrito de la siguiente manera:",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#modelo",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#modelo",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "Modelo",
    "text": "Modelo\nSabiendo que los datos tienen frecuencia bianual y parece razonable pensar que la población puede tener la misma tendencia cada año en los mismos meses (por ej en verano crecer y en invierno decrecer), vamos a considerar diferenciar la serie 2 veces. Es decir, plantearemos un modelo Seasonal ARIMA. La dibujamos de nuevo a ver si es necesario diferenciar una vez más para eliminar la tendencia.\n\n# Series diferenciadas\nt1 &lt;- diff(tss, 2)\n\n\n# Las dibujamos\nplot(t1)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t1, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t1\nDickey-Fuller = -2.141, Lag order = 4, p-value = 0.5182\nalternative hypothesis: stationary\n\n\nNo es estacionaria luego valoramos el volver a diferenciar para ver si conseguimos la estacionaridad.\n\nt2 &lt;- diff(t1, 1)\n\n# Las dibujamos\n\nplot(t2)\n\n\n\n\n\n\n\n# TEST para ver estacionaridad\n# H0= NO es estacionaria\n# hace uso del paquete tseries\nadf.test(t2, alternative = \"stationary\")\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  t2\nDickey-Fuller = -3.3652, Lag order = 4, p-value = 0.06458\nalternative hypothesis: stationary\n\n\nAl trazar la serie diferenciada, ya vemos un patrón oscilante alrededor de 0, sin una tendencia fuerte visible (aunque vemos que las últimas observaciones aumenta la variabilidad).. Esto sugiere que la diferenciación es suficiente y debe incluirse en el modelo. Además el test de estacionariedad ya lo pasa.\nYa se ha ido la tendencia luego parece razonable pensar que la serie ya es estacionaria aunque la varianza aumente un poco al final, es decir, tomamos D=1, d=1. Examinemos ahora ACF/PACF para determinar los p,q y P,Q.\n\n# ACF plot\nAcf(t2, main = \"ACF para la serie diferenciada 1 vez\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de ACF vemos que hay 1 barra que se salen notablemente del límite deseado (dentro del periodo m=6 meses), luego tomaríamos como q= 1.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de ACF vemos que hay 2 barras que se sale notablemente del límite deseado (dentro de lags múltiplos m=2 ), luego tomaríamos como Q= 1.\n\n\n# PACF plot\nPacf(t2, main = \"PACF para la serie diferenciada 1 vez\", lag.max = 50, ylim = c(-0.5, 0.5))\n\n\n\n\n\n\n\n\n\nPrimeros Lags (parámetros NO estacionales)En el gráfico de PACF vemos que hay 1 barra que se salen notablemente del límite deseado (dentro del periodo m= 2), luego tomaríamos como p=1.\nLags múltiplos de 12 (parámetros estacionales) En el gráfico de PACF vemos que hay 1 barra que se sale notablemente del límite deseado (dentro de lags múltiplos m=2 ), luego tomaríamos como P= 1.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#arima-automático",
    "href": "notebooks/Series Temporales/ARIMA/Navarra/navarra_series.html#arima-automático",
    "title": "Series Temporales - ARIMA: navarra",
    "section": "ARIMA automático",
    "text": "ARIMA automático\nExiste una función que permite identificar los parámetros de manera automática. Generalmente se suele usar como primer approach está función para después modificar según evidencia los parámetros.\n\n# Identificar los parámetros del modelo ARIMA de manera automática\nauto_arima &lt;- auto.arima(tss)\nauto_arima\n\nSeries: tss \nARIMA(1,1,0)(1,0,0)[2] with drift \n\nCoefficients:\n         ar1    sar1      drift\n      0.7638  0.6215  1768.3175\ns.e.  0.0700  0.0875   660.9324\n\nsigma^2 = 404531:  log likelihood = -787\nAIC=1581.99   AICc=1582.41   BIC=1592.41\n\n\nNos sugiere ARIMA(1,1,0)(1,0,0)[2]\n\ncheckresiduals(auto_arima)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,0)(1,0,0)[2] with drift\nQ* = 8.0719, df = 3, p-value = 0.04455\n\nModel df: 2.   Total lags used: 5\n\n\n\n# Modelo\nauto_arima_train &lt;- arima(data.train, order = c(1, 1, 0), seasonal = c(1, 0, 0))\npred2 &lt;- forecast(auto_arima_train, h = 4)\naccuracy(pred2, data.test)\n\n                     ME      RMSE       MAE         MPE       MAPE       MASE\nTraining set   113.0672  594.2899  375.8904  0.02056445 0.06537697 0.09470859\nTest set     -1972.8003 2765.0937 1972.8003 -0.29829306 0.29829306 0.49706278\n                  ACF1 Theil's U\nTraining set 0.1239726        NA\nTest set     0.2243908  1.217847\n\n\n\n# Gráfico con los datos originales y las predicciones de los modelos\nplot(tss,\n  xlim = c(1971, 2021 + 5), ylim = c(min(tss, pred2$pred), max(tss, pred2$pred)),\n  xlab = \"Fecha\", ylab = \"Valor\", main = \"Comparación de Predicciones ARIMA\"\n)\npoints(pred2$mean, col = \"red\", pch = 16, cex = 0.5) # Predicciones con arima_model1 en rojo\nlines(tss, col = \"blue\", lwd = 2) # Serie original en azul\nlines(fitted(auto_arima_train), col = \"red\")\n\nlegend(\"bottomright\", legend = c(\"Original\", \"ARIMA automático \", \"ARIMA manual\"), col = c(\"blue\", \"red\", \"green\"), lty = 1)\n\nlines(pred2$lower[, 2], col = \"red\", lty = \"solid\")\nlines(pred2$upper[, 2], col = \"red\", lty = \"solid\")\n\n\n# ARIMA manual previo\nlines(fitted(arima_model_4), col = \"green\", lwd = 0.5)\npoints(pred$mean, col = \"green\", pch = 1, cex = 0.5) # Predicciones con arima_model1 en rojo\n\nlines(pred$lower[, 2], col = \"green\", lty = \"solid\")\nlines(pred$upper[, 2], col = \"green\", lty = \"solid\")\n\n\n\n\n\n\n\n\nVemos que prácticamente ambos modelos predicen de la misma manera, aunque cambien ligeramente los parámetros de uno y otro. No obstante se podrían plantear modelos que intenten mejorar algunas métricas como la independencia de los residuos.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "ARIMA",
      "Navarra",
      "Series Temporales - ARIMA: navarra"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "",
    "text": "En este notebook se va a exponer como llevar a cabo el análisis de una serie temporal mediante un modelo Holt Winters, esto es, un suavizado exponencial triple. Para ello se verá la teoría que sustenta este método y se mostrará un caso práctico con un conjunto de datos real.\n\n\nEn este cuaderno vamos a analizar el dataset llamado ipc_series.xlsx. Este dataset presenta el IPC Español desde el año 2002 hasta el 2022 con una periodicidad mensual. El objetivo será analizar dicha serie temporal mediante un modelo Holt Winters.\nConcretamente en este dataset tenemos las siguientes variables:\n\nIPC: Índice de Precios al consumo.\nfecha: Fecha correspondiente.\n\n\n\n\nSe pretende ajustar una serie temporal que contiene el IPC de España mediante un modelo Holt Winters.\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#dataset",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#dataset",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado ipc_series.xlsx. Este dataset presenta el IPC Español desde el año 2002 hasta el 2022 con una periodicidad mensual. El objetivo será analizar dicha serie temporal mediante un modelo Holt Winters.\nConcretamente en este dataset tenemos las siguientes variables:\n\nIPC: Índice de Precios al consumo.\nfecha: Fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene el IPC de España mediante un modelo Holt Winters.\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#cargar-librerías",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#cargar-librerías",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#lectura-datos",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#lectura-datos",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndata &lt;- read_excel(\"../../../../files/ipc_series.xlsx\", sheet = \"Datos\")\ndata$ipc &lt;- data$`Pobl. Total`\n\n\nsum(is.na(data)) # No missing data\n\n[1] 0\n\n\nPara tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 252   2\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [252 × 2] (S3: tbl_df/tbl/data.frame)\n $ Fecha: POSIXct[1:252], format: \"2022-12-01\" \"2022-11-01\" ...\n $ IPC  : num [1:252] 110 110 110 109 110 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#comparación-con-medias-móviles",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#comparación-con-medias-móviles",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "Comparación con Medias Móviles",
    "text": "Comparación con Medias Móviles\n\nAmbos tienen aproximadamente la misma distribución de error de pronóstico cuando \\(\\alpha = 2/(k + 1)\\).\nDifieren en que el suavizado exponencial tiene en cuenta todos las observaciones pasados, mientras que las ** medias móviles** solo tiene en cuenta k observaciones pasadas.\nComputacionalmente hablando, también difieren en que el método de medias móviles requiere que se conserven las últimas \\(k\\) observaciones de datos, mientras que el suavizado exponencial solo necesita conservar el valor suavizado para el \\(t\\) más reciente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#hyperparameter-tunning",
    "href": "notebooks/Series Temporales/Holt Winters/ipc_series/ipc_series.html#hyperparameter-tunning",
    "title": "Series Temporales - Holt Winters: ipc_series",
    "section": "Hyperparameter tunning",
    "text": "Hyperparameter tunning\nAunque en este notebook en la función Holt.Wintersse han dejado los parámetros alpha, betay gamma autoajustarse, un enfoque alternativo podría ser establecer una rejilla de valores creando diferentes modelos con diferentes valores y comparando el MPE y el MAPE para ver que modelo tiene mejores métricas. Se tomarían como adecuados los parámetros del modelo con mejores métricas.\nVeamos un ejemplo, vamos a buscar el gamma óptimo.\n\n# Rejilla de gammas\ngamma &lt;- seq(0.1, 0.9, 0.05)\n\n# Inicializamos\nRMSE &lt;- NA\n\n\n# Bucle que analiza los modelos con todos los gammas propuestos (autoajustando el resto de hiperparámetros)\nfor (i in seq_along(gamma)) {\n  mod1.bucle &lt;- HoltWinters(data.train, gamma = gamma[i], seasonal = \"additive\")\n\n  future &lt;- forecast(mod1.bucle, h = 5)\n  RMSE[i] &lt;- accuracy(future, data.test)[2, 4]\n}\n\n# Copiamos el RMSE de todos\nerror &lt;- data.frame(gamma, RMSE)\n\n# Buscamos el mínimo\nminimum &lt;- error[which(error$RMSE == min(error$RMSE)), ]\n\nggplot(error, aes(gamma, RMSE)) +\n  geom_line() +\n  geom_point(data = minimum, color = \"blue\", size = 2) +\n  ggtitle(\"gamma's impact on forecast errors\",\n    subtitle = \"gamma = 0.75 minimizes RMSE\"\n  )\n\n\n\n\n\n\n\n\nSi actualizamos nuestro modelo con este parámetro “óptimo” de gamma, observamos que reducimos nuestro MAPE baja 1.02%. Esto representa una mejora pequeña, pero a menudo las mejoras pequeñas pueden tener grandes implicaciones comerciales. Sin embargo como veremos a continuación el análisis de los residuos no es tan bueno como nos gustaría.\nVeamos concretamente las métricas:\n\nmod1.bucle &lt;- HoltWinters(data.train, gamma = 0.65, seasonal = \"additive\")\nfuture &lt;- forecast(mod1.bucle, h = 5)\naccuracy(future, data.test)\n\n                    ME      RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.0188261 0.4041728 0.2919254 0.01730844 0.3217349 0.1562249\nTest set     0.7536863 1.2846364 1.1303475 0.68364023 1.0265511 0.6049096\n                  ACF1 Theil's U\nTraining set 0.2347026        NA\nTest set     0.4193531  2.492672\n\ncheckresiduals(mod1.bucle)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from HoltWinters\nQ* = 85.006, df = 24, p-value = 9.512e-09\n\nModel df: 0.   Total lags used: 24\n\n\nVamos a graficarlo a ver que las predicciones futuras:\n\nautoplot(future)",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Ipc Series",
      "Series Temporales - Holt Winters: ipc_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "",
    "text": "En este notebook se va a exponer como llevar a cabo el análisis de una serie temporal mediante un modelo Holt Winters, esto es, un suavizado exponencial triple. Para ello se verá la teoría que sustenta este método y se mostrará un caso práctico con un conjunto de datos real.\n\n\nSe pretende ajustar una serie temporal que contiene los habitantes de la Comunidad de Madrid mediante un modelo Holt Winters:\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.\n\n\n\n\nEn este cuaderno vamos a analizar el dataset llamado Madrid.xlsx. Este dataset presenta la población de la Comunidad de Madrid desde 1971 hasta 2022 con una periodicidad bianual (Enero y Julio). El objetivo será analizar dicha serie temporal mediante un modelo Holt-Winters (suavizado exponencial triple) .\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha correspondiente.\nPobl. Hombres: Fecha correspondiente.\nPobl. Mujeres: Fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#descripción-del-trabajo-a-realizar",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "",
    "text": "Se pretende ajustar una serie temporal que contiene los habitantes de la Comunidad de Madrid mediante un modelo Holt Winters:\n\nVisualizar la serie para intuir su tendencia y estacionalidad.\nRazonar por qué es factible un modelo Holt-Winters (suavizado exponencial triple).\nDividir la serie en Train/Test para poder evaluar después el modelo.\nAjustar hiperparámetros e interpretar coeficientes.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#dataset",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#dataset",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado Madrid.xlsx. Este dataset presenta la población de la Comunidad de Madrid desde 1971 hasta 2022 con una periodicidad bianual (Enero y Julio). El objetivo será analizar dicha serie temporal mediante un modelo Holt-Winters (suavizado exponencial triple) .\nConcretamente en este dataset tenemos las siguientes variables:\n\nFecha: Fecha correspondiente.\nPobl. Hombres: Fecha correspondiente.\nPobl. Mujeres: Fecha correspondiente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#cargar-librerías",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#cargar-librerías",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\n# Librerías\nlibrary(forecast) # para predecir observaciones futuras\nlibrary(ggplot2) # Nice plots\nlibrary(readxl) # Para leer excels\nlibrary(stats) # Para crear objetos ts",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#lectura-datos",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#lectura-datos",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\nCargamos entonces el conjunto de datos:\n\ndata &lt;- readxl::read_excel(\"../../../../files/madrid.xlsx\",\n  sheet = \"Datos\", col_types = c(\n    \"date\",\n    \"numeric\", \"numeric\", \"numeric\"\n  )\n)\ndata$Total &lt;- data$`Pobl. Total`\ndata$H &lt;- data$`Pobl. Hombres`\ndata$M &lt;- data$`Pobl. Mujeres`\n\n\nsum(is.na(data))\n\n[1] 0\n\n\nPara tener una noción general que nos permita describir el conjunto con el que vamos a trabajar, podemos extraer su dimensión, el tipo de variables que contiene o qué valores toma cada una.\n\n# Dimensión del conjunto de datos\ndim(data)\n\n[1] 104   7\n\n# Tipo de variables que contiene\nstr(data)\n\ntibble [104 × 7] (S3: tbl_df/tbl/data.frame)\n $ Fecha        : POSIXct[1:104], format: \"2022-07-01\" \"2022-01-01\" ...\n $ Pobl. Total  : num [1:104] 6825005 6769373 6738361 6755828 6757042 ...\n $ Pobl. Hombres: num [1:104] 3268934 3243712 3228906 3236830 3238421 ...\n $ Pobl. Mujeres: num [1:104] 3556072 3525661 3509455 3518998 3518621 ...\n $ Total        : num [1:104] 6825005 6769373 6738361 6755828 6757042 ...\n $ H            : num [1:104] 3268934 3243712 3228906 3236830 3238421 ...\n $ M            : num [1:104] 3556072 3525661 3509455 3518998 3518621 ...",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#comparación-con-medias-móviles",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#comparación-con-medias-móviles",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "Comparación con Medias Móviles",
    "text": "Comparación con Medias Móviles\n\nAmbos tienen aproximadamente la misma distribución de error de pronóstico cuando \\(\\alpha = 2/(k + 1)\\).\nDifieren en que el suavizado exponencial tiene en cuenta todos las observaciones pasados, mientras que las ** medias móviles** solo tiene en cuenta k observaciones pasadas.\nComputacionalmente hablando, también difieren en que el método de medias móviles requiere que se conserven las últimas \\(k\\) observaciones de datos, mientras que el suavizado exponencial solo necesita conservar el valor suavizado para el \\(t\\) más reciente.",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#hyperparameter-tunning",
    "href": "notebooks/Series Temporales/Holt Winters/Madrid/Madrid.html#hyperparameter-tunning",
    "title": "Series Temporales - Holt Winters: madrid_series",
    "section": "Hyperparameter tunning",
    "text": "Hyperparameter tunning\nAunque en este notebook en la función Holt.Wintersse han dejado los parámetros alpha, betay gamma autoajustarse, un enfoque alternativo podría ser establecer una rejilla de valores creando diferentes modelos con diferentes valores y comparando el MPE y el MAPE para ver que modelo tiene mejores métricas. Se tomarían como adecuados los parámetros del modelo con mejores métricas.\nVeamos un ejemplo, vamos a buscar el gamma óptimo.\n\n# Rejilla de gammas\ngamma &lt;- seq(0.1, 1, 0.01)\n\n# Inicializamos\nRMSE &lt;- NA\n\n\n# Bucle que analiza los modelos con todos los gammas propuestos (autoajustando el resto de hiperparámetros)\nfor (i in seq_along(gamma)) {\n  mod1.bucle &lt;- HoltWinters(data.train, gamma = gamma[i], seasonal = \"additive\")\n\n  future &lt;- forecast(mod1.bucle, h = 5)\n  RMSE[i] &lt;- accuracy(future, data.test)[2, 5]\n}\n\n# Copiamos el RMSE de todos\nerror &lt;- data.frame(gamma, RMSE)\n\n# Buscamos el mínimo\nminimum &lt;- error[which(error$RMSE == min(error$RMSE)), ]\n\nggplot(error, aes(gamma, RMSE)) +\n  geom_line() +\n  geom_point(data = minimum, color = \"blue\", size = 2) +\n  ggtitle(\"gamma's impact on forecast errors\",\n    subtitle = \"gamma = 0.65 minimizes RMSE\"\n  )\n\n\n\n\n\n\n\n\nSi actualizamos nuestro modelo con este parámetro “óptimo” de gamma, observamos que reducimos nuestro MAPE baja 1.85%. Esto representa una mejora pequeña, pero a menudo las mejoras pequeñas pueden tener grandes implicaciones comerciales. Sería aconsejable analizar si dicha mejora se ha producido también en otras métricas.\nVeamos concretamente las métricas:\n\nmod1.bucle &lt;- HoltWinters(data.train, alpha = 0.34, seasonal = \"additive\")\nfuture &lt;- forecast(mod1.bucle, h = 5)\naccuracy(future, data.test)\n\n                       ME      RMSE      MAE           MPE      MAPE      MASE\nTraining set     111.1714  17874.43  11541.7 -3.463583e-06 0.2090477 0.1848127\nTest set     -125852.8534 137716.44 125852.9 -1.857896e+00 1.8578957 2.0152326\n                  ACF1 Theil's U\nTraining set 0.6544554        NA\nTest set     0.3905034  4.642285\n\ncheckresiduals(mod1.bucle)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from HoltWinters\nQ* = 48.913, df = 4, p-value = 6.089e-10\n\nModel df: 0.   Total lags used: 4\n\n\nEste modelo parece tener mejores métricas que el encontrado de manera automática. Vamos a graficarlo a ver que las predicciones futuras:\n\nautoplot(future)",
    "crumbs": [
      "Notebooks",
      "Series Temporales",
      "Holt Winters",
      "Madrid",
      "Series Temporales - Holt Winters: madrid_series"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo una Regresión Lineal a partir de un conjunto de datos, explicando las hipótesis que se deben satisfacer y un posterior análisis de bondad del modelo. Por último se explicará como utilizar el modelo creado para predecir nuevas observaciones.\n\n\nEn este cuaderno vamos a analizar el dataset llamado matrimonios_reg.xlsx. Se quiere predecir el número de matrimonios en las ciudades españolas (de entre 50.000 y 300.000 habitantes) a partir del número de mujeres que las habitan y el número de nacimientos que ha habido ese año. Concretamente, se han tomado los datos relativos a 2022. Este análisis parece razonable a simple vista ya que el matrimonio la mayor parte de las veces va a acompañado de un nacimiento en los meses anteriores o posteriores. Los datos han sido extraídos de la Operación 8867 Movimiento Natural de la Población (IOE 8867), que se encuentra dentro de la temática Demografía y Población. Ha sido necesario un preprocesado para adaptar los datos a los fines de este estudio, que puede ser encontrado en Limpieza de datos\nConcretamente tenemos las siguientes variables:\n\nMunicipios: Códigos de los municipios.\nCiudad: Nombres de los municipios.\nMatrimonios: Número de matrimonios en cada municipio.\nMujeres: Número total de habitantes mujeres en cada municipio.\nNacimientos:Número de nacimientos en cada municipio.\nTotal: Número total de habitantes en cada municipio.\n\n\n\n\nElaborar una regresión lineal que explique la variable Matrimonios en función de Nacimientos y Mujeres. Interpretar y ver si incluir todas variables o no.\n1. Análisis exploratorio.\n2. Plantear las hipótesis de una regresión (incluyendo todas variables).\n3. Analizar el modelo planteado y su ajuste de bondad.\n4. Comprobar hipótesis de regresión.\n5. Conclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#dataset",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#dataset",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "",
    "text": "En este cuaderno vamos a analizar el dataset llamado matrimonios_reg.xlsx. Se quiere predecir el número de matrimonios en las ciudades españolas (de entre 50.000 y 300.000 habitantes) a partir del número de mujeres que las habitan y el número de nacimientos que ha habido ese año. Concretamente, se han tomado los datos relativos a 2022. Este análisis parece razonable a simple vista ya que el matrimonio la mayor parte de las veces va a acompañado de un nacimiento en los meses anteriores o posteriores. Los datos han sido extraídos de la Operación 8867 Movimiento Natural de la Población (IOE 8867), que se encuentra dentro de la temática Demografía y Población. Ha sido necesario un preprocesado para adaptar los datos a los fines de este estudio, que puede ser encontrado en Limpieza de datos\nConcretamente tenemos las siguientes variables:\n\nMunicipios: Códigos de los municipios.\nCiudad: Nombres de los municipios.\nMatrimonios: Número de matrimonios en cada municipio.\nMujeres: Número total de habitantes mujeres en cada municipio.\nNacimientos:Número de nacimientos en cada municipio.\nTotal: Número total de habitantes en cada municipio.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "",
    "text": "Elaborar una regresión lineal que explique la variable Matrimonios en función de Nacimientos y Mujeres. Interpretar y ver si incluir todas variables o no.\n1. Análisis exploratorio.\n2. Plantear las hipótesis de una regresión (incluyendo todas variables).\n3. Analizar el modelo planteado y su ajuste de bondad.\n4. Comprobar hipótesis de regresión.\n5. Conclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#cargar-librerías",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#cargar-librerías",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(readxl) # Para leer los excels\nlibrary(kableExtra) # Para dar formato a las tablas html\nlibrary(knitr) # Formato tablas en html\nlibrary(gridExtra) # Para el layout de los gráficos\nlibrary(car) # Para el bonfferroni test\nlibrary(corrplot) # Para el gráfico de correlaciones\nlibrary(ggplot2)\nlibrary(lmtest) # Test Homocedasticidad\nlibrary(caret) # Para crear partición\nlibrary(dplyr) # Filtrar datos",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#lectura-datos",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#lectura-datos",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\ndata &lt;- read_excel(\"../../../files/matrimonios_reg.xlsx\", sheet = \"Datos\")\n\n\nc(\n  anyNA(data), # Any missing data\n  any(data == 0)\n) # Any value equal to 0\n\n[1] FALSE FALSE\n\n\nNo hay ningún NA leugo no será necesario escalar imputar ningún valor.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#análisis",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#análisis",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Análisis",
    "text": "Análisis\nRealizando un resumen numérico vemos que todas las variables toman valores mayores que 0 y con un máximo dependiendo de la naturaleza de la variable.\n\nsummary_stats &lt;- function(data) {\n  # Filtra solo las columnas numéricas del dataframe\n  num_data &lt;- data[, sapply(data, is.numeric)]\n\n  # Calcula las estadísticas descriptivas para cada columna\n  stats &lt;- sapply(num_data, function(col) {\n    c(\n      Min = format(round(min(col, na.rm = TRUE), 2), nsmall = 2),\n      Q1 = format(round(quantile(col, 0.25, na.rm = TRUE), 2), nsmall = 2),\n      Mean = format(round(mean(col, na.rm = TRUE), 2), nsmall = 2),\n      Median = format(round(median(col, na.rm = TRUE), 2), nsmall = 2),\n      Q3 = format(round(quantile(col, 0.75, na.rm = TRUE), 2), nsmall = 2),\n      Max = format(round(max(col, na.rm = TRUE), 2), nsmall = 2)\n    )\n  })\n\n  # Transpone el resultado y conviértelo en un dataframe\n  result &lt;- as.data.frame(t(stats))\n\n  # Agrega una columna con los nombres de las variables\n  result &lt;- cbind(variable = rownames(result), result)\n\n  # Establece los nombres de las columnas\n  colnames(result) &lt;- c(\"variable\", \"Min\", \"Q1\", \"Mean\", \"Median\", \"Q3\", \"Max\")\n\n  return(result)\n}\n\n\n# Ejemplo de uso con un data frame 'df'\n# Reemplaza 'df' con el nombre de tu data frame\nresult_df &lt;- summary_stats(data)\n\n# Convierte la salida a una tabla utilizando kable\nkable(result_df, format = \"pipe\", caption = \"Table showing the measures of interest.\")\n\n\n# Histogram Matrimonios\nhistogram &lt;- ggplot(data, aes(x = Matrimonios)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Matrimonios\", x = \"Matrimonios\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot Matrimonios\nboxplot &lt;- ggplot(data, aes(x = \"d\", y = Matrimonios)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Matrimonios\", x = \"\", y = \"Matrimonios\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n# Histogram Mujeres\nhistogram2 &lt;- ggplot(data, aes(x = Mujeres)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Mujeres\", x = \"Mujeres\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot Mujeres\nboxplot2 &lt;- ggplot(data, aes(x = \"d\", y = Mujeres)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Mujeres\", x = \"\", y = \"Mujeres\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n# Histogram Nacimientos\nhistogram3 &lt;- ggplot(data, aes(x = Nacimientos)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Nacimientos\", x = \"Nacimientos\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot Nacimientos\nboxplot3 &lt;- ggplot(data, aes(x = \"d\", y = Nacimientos)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Nacimientos\", x = \"\", y = \"Nacimientos\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n# Histogram Total\nhistogram4 &lt;- ggplot(data, aes(x = Total)) +\n  geom_histogram(fill = \"deepskyblue2\", color = \"navy\", bins = 5) +\n  labs(title = \"Histogram of Total\", x = \"Total\", y = \"Frequency\") +\n  theme_minimal(base_size = 8)\n\n# Box Plot Total\nboxplot4 &lt;- ggplot(data, aes(x = \"d\", y = Total)) +\n  geom_boxplot(fill = \"deepskyblue2\", color = \"navy\") +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Box Plot of Total\", x = \"\", y = \"Total\") +\n  theme_minimal(base_size = 8) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n# Arrange plots vertically\ngrid.arrange(histogram, boxplot, histogram2, boxplot2, histogram3, boxplot3, histogram4, boxplot4, nrow = 2, ncol = 4, widths = c(0.3, 0.2, 0.3, 0.2))",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#hipótesis-y-indicadores-de-bondad",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#hipótesis-y-indicadores-de-bondad",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Hipótesis y indicadores de bondad",
    "text": "Hipótesis y indicadores de bondad\nPara que una regresión lineal proporcione un buen ajuste a los datos debe cumplir una serie de requisitos que por tanto deben ser verificados al llevar a cabo el estudio. Recordar que la regresión lineal se expresa como: \\[\n\\mathbf{Y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\n\\] donde \\(\\mathbf{Y}\\) es la variable respuesta, \\(\\mathbf{X}\\) los predictores (hay \\(k\\) variables predictoras), \\(\\boldsymbol{\\beta}\\) los coeficientes de la regresión y \\(\\boldsymbol{\\varepsilon}\\) el error. \\[\n\\mathbf{Y}=\\left[\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array}\\right] \\quad \\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1 k} \\\\\n1 & x_{21} & \\ldots & x_{2 k} \\\\\n\\vdots & \\ddots & \\vdots & \\\\\n1 & x_{n 1} & \\ldots & x_{n k}\n\\end{array}\\right] \\quad \\boldsymbol{\\beta}=\\left[\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{array}\\right] \\quad \\boldsymbol{\\varepsilon}=\\left[\\begin{array}{c}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{array}\\right]\n\\] Las hipótesis que se deben cumplir son:\n\nLinealidad: La media de la respuesta es función lineal de los predictores. En términos matemáticos: \\[E\\left[\\mathbf{Y} \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k \\]\nIndependencia de errores: Los errores \\(\\varepsilon_i\\) deben ser independientes, es decir, \\(Cov[\\varepsilon_i,\\varepsilon_j] =0, \\; \\forall i\\neq j\\).\nHomocedasticidad: La varianza del error debe ser constante.\n\n\\[Var\\left[\\varepsilon_i \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\sigma^2 \\quad \\forall \\;i \\]\n\nNormalidad : Los errores deben estar distribuidos normalmente, es decir, \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\; \\forall i\\).\n\nPara analizar la bondad, hay algunos indicadores como el Coeficiente de Determinación o \\(R^2\\) que representa el porcentaje de variabilidad de la variable respuesta que es capaz de explicar el modelo. Es decir, si toma valor 1 hay una dependencia lineal exacta entre los predictores y la variable respuesta y por tanto las predicciones serán perfectas. Por el contrario, si toma valor 0 habrá que desechar el modelo puesto que no es capaz de predecir con nada de exactitud.\nEn caso de que haya más de un predictor (\\(k &gt;1\\), Regresión Lineal Múltiple), es más recomendable usar el Coeficiente de Determinación Ajustado \\(R^2\\_adj\\) como indicador de bondad, pues el \\(R^2\\) puede inflarse artificialmente debido a la presencia de varios predictores. Su interpretación es similar.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#modelo",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#modelo",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Modelo",
    "text": "Modelo\nEn este caso nos encontramos ante una Regresión Lineal Múltiple puesto que tenemos más de una variable predictora. Inicialmente vamos a considerar un modelo con todas variables predictoras para intentar predecir Matrimonios y veremos si este modelo cumple las hipótesis necesarias y cuan bueno es.\n\nset.seed(785248)\n#  80%/20% train/test pariticion\ntrainIndex &lt;- createDataPartition(data$Matrimonios, list = FALSE, p = 0.8)\ndata_train &lt;- data[trainIndex, ]\ndata_test &lt;- data[-trainIndex, ]\n# Modelo inicial\nlm1 &lt;- lm(Matrimonios ~ Nacimientos + Mujeres, data = data_train)\nlm1\n\n\nCall:\nlm(formula = Matrimonios ~ Nacimientos + Mujeres, data = data_train)\n\nCoefficients:\n(Intercept)  Nacimientos      Mujeres  \n  28.364455    -0.008467     0.006424  \n\nsummary(lm1)\n\n\nCall:\nlm(formula = Matrimonios ~ Nacimientos + Mujeres, data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-163.460  -33.633   -7.279   30.070  154.947 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.3644547 10.8801761   2.607   0.0104 *  \nNacimientos -0.0084675  0.0324383  -0.261   0.7946    \nMujeres      0.0064237  0.0004282  15.002   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 54.78 on 106 degrees of freedom\nMultiple R-squared:  0.937, Adjusted R-squared:  0.9358 \nF-statistic: 788.6 on 2 and 106 DF,  p-value: &lt; 2.2e-16\n\n\nA primera vista vemos un valor de Múltiple R-squared: 0.93, lo cual es bastante alto y por tanto nuestro modelo parece capturar bien la variabilidad de la variable respuesta, concretamente un \\(93\\%\\). Sin embargo, en los sucesivos modelos que planteemos no podemos usar como criterio de comparación el \\(R-squared\\) pues aumenta a la vez que lo hace el número de variables, y por tanto para comparar modelos entre si se debe usar el Adjusted R-squared (que tiene en cuenta el número de variables).\nEn la línea de los residuos no parece haber contraindicaciones a que estos sigan una distribución normal centrada en cero puesto que tenemos unas medidas de dispersión bastante simétricas. No obstante, más adelante se procederán a hacer los test pertinentes.\nEn la última linea se lleva a cabo un Test de Significación Global \\(F-Test\\) lo que considera es la hipótesis nula de\n\\[H_0: \\beta_i =0\\; \\forall i\n\\\\\nH_1: al\\; menos \\; un \\; \\beta_i \\neq 0\\].\nPara un nivel de confianza de \\(0.95\\) podemos rechazar la hipótesis nula (puesto que p_val&lt; 0.05) y por tanto aceptar la alternativa, lo cual es buena señal.\nNo obstante es necesario analizar que se cumplan las hipótesis iniciales para poder asegurar que estamos ante un buen modelo.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-de-bonferroni-datos-atípicos",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-de-bonferroni-datos-atípicos",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Test de Bonferroni (datos atípicos)",
    "text": "Test de Bonferroni (datos atípicos)\nLa idea principal es verificar si los residuos de las observaciones son significativamente diferentes de cero. Si un residuo tiene un valor “studentizado” grande en comparación con una distribución t, puede considerarse como un posible valor atípico. Esto se debe a que teóricamente se demuestra que los residuos “studentizados” \\(r^* _i \\sim t_{n-k-1}\\) con k el número de predictores. En este caso parece haber indicación de sólo un valor atípico.\n\naa &lt;- outlierTest(lm1)\noutlierTest(lm1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n23 -3.336085          0.0011755      0.12813",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-homocedasticidad",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-homocedasticidad",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Test homocedasticidad",
    "text": "Test homocedasticidad\nEn términos sencillos, la Prueba de Breusch-Pagan evalúa si la varianza de los errores en un modelo de regresión es constante o si varía a lo largo de los valores de las variables predictoras. Una violación de la homocedasticidad puede afectar la validez de las inferencias realizadas a partir del modelo.\nEl test funciona de la siguiente manera: se obtienen los residuos al cuadrado y se realiza una regresión auxiliar para determinar si hay una relación significativa entre los residuos al cuadrado y las variables predictoras. Si se encuentra evidencia significativa, puede indicar la presencia de heterocedasticidad.\n\nbptest(Matrimonios ~ Nacimientos + Mujeres, data = data_train, varformula = ~ fitted.values(lm1), studentize = FALSE)\n\n\n    Breusch-Pagan test\n\ndata:  Matrimonios ~ Nacimientos + Mujeres\nBP = 11.687, df = 1, p-value = 0.0006294\n\n\n\\[H_0: Var[\\varepsilon_i ] =\\sigma^2 \\; \\forall \\;i\n\\\\\nH_1: Var[\\varepsilon_i ] \\neq \\sigma^2 \\; \\forall \\;i\\].\nSi el valor p obtenido de la prueba de Breusch-Pagan es \\(&lt;0.05\\), interpretaríamos esto como evidencia suficiente para rechazar la hipótesis nula de homocedasticidad (a nivel de significancia de \\(0.05\\)). En otras palabras, tendríamos suficiente evidencia estadística para decir que hay heterocedasticidad en los errores del modelo de regresión.\nEn términos prácticos, esto sugiere que la varianza de los errores parece no ser constante a lo largo de los valores de las variables predictoras.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#normalidad-de-residuos",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#normalidad-de-residuos",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Normalidad de residuos",
    "text": "Normalidad de residuos\nEl Test de Shapiro es una prueba de normalidad que se utiliza para evaluar si una muestra proviene de una población con una distribución normal. La hipótesis nula del test es que la muestra sigue una distribución normal. Si el valor p obtenido en la prueba es menor que el nivel de significancia (comúnmente establecido en \\(0.05\\)), se rechaza la hipótesis nula, indicando que la muestra no sigue una distribución normal.\n\\[H_0: \\varepsilon_i  \\sim N(\\;,\\;) \\; \\forall\n\\\\\nH_1: \\varepsilon_i  \\nsim N(\\;,\\;) \\; \\forall \\].\n\nshapiro.test(lm1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm1$residuals\nW = 0.98659, p-value = 0.3493\n\n\nAceptamos la normalidad de los residuos puesto que el \\(p-value&gt;0.05\\).",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-linealidad",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#test-linealidad",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Test linealidad",
    "text": "Test linealidad\nLa hipótesis alternativa analiza si la inclusión de términos cuadráticos (potencia 2) de las variables predictoras mejora significativamente el modelo en comparación con un modelo que solo incluye términos lineales.\n\nresettest(Matrimonios ~ Nacimientos + Mujeres, data = data_train, power = 2, type = \"regressor\")\n\nHay evidencia significativa de que el modelo Matrimonios ~ Nacimientos + Mujeres está mal especificado. Esto sugiere que podría faltar algún término no lineal o interacción en el modelo, o que se deben incluir variables adicionales para capturar correctamente la relación entre Matrimonios, Nacimientos y Mujeres.\nPor ejemplo, se podría modificar el modelo para incluir términos no lineales y de interacción:\n\nmodelo_mejorado &lt;- lm(Matrimonios ~ Nacimientos + I(Nacimientos^2) + Mujeres + I(Mujeres^2) + Nacimientos:Mujeres, data = data_train)\nsummary(modelo_mejorado)\n\n\nCall:\nlm(formula = Matrimonios ~ Nacimientos + I(Nacimientos^2) + Mujeres + \n    I(Mujeres^2) + Nacimientos:Mujeres, data = data_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-115.010  -36.734   -1.545   31.532  143.208 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -4.111e+01  2.595e+01  -1.584   0.1162    \nNacimientos          1.905e-01  9.239e-02   2.062   0.0417 *  \nI(Nacimientos^2)    -2.063e-04  1.276e-04  -1.617   0.1089    \nMujeres              6.140e-03  1.392e-03   4.411 2.54e-05 ***\nI(Mujeres^2)        -1.760e-08  1.799e-08  -0.978   0.3302    \nNacimientos:Mujeres  2.964e-06  2.945e-06   1.007   0.3165    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.42 on 103 degrees of freedom\nMultiple R-squared:  0.944, Adjusted R-squared:  0.9412 \nF-statistic: 346.9 on 5 and 103 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#gráfico-de-influencia-del-modelo-porpuesto",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#gráfico-de-influencia-del-modelo-porpuesto",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Gráfico de influencia del modelo porpuesto",
    "text": "Gráfico de influencia del modelo porpuesto\nEn el siguiente gráfico se muestran los residuos “studentizados”, es decir, los residuos transformados a una \\(N(0,1)\\). Es por ello, que debido a el cuantil \\(z_{\\alpha/2}=-1.96\\; con \\; \\alpha=0.05\\) de una normal, sabemos que el \\(95\\%\\) de elementos deben estar contenidos en \\((-1,96,1.96)\\) que son las rayas horizontales del gráfico.\n\ninfluencePlot(lm1, id = list(method = \"noteworth\", n = 2))\n\n\n\n\n\n\n\n\n\nLos residuos bajo hipótesis de RLM siguen una N(0,sigma) y los “studentizados” un N(0,1), es decir el \\(95\\%\\) de datos están entre \\((-1.96,1.96)\\), las líneas horizontales. Tenemos varios datos fuera de la línea lo que a priori podría ser un problema\nLas líneas verticales indican los datos con apalancamiento en el modelo.\nEl área de las burbujas es proporcional a la Distancia de Cook (mide cómo cambian los parámetros del modelo cuando se excluye una observación específica). Vemos que hay uno con una gran distancia de cook (tienen residuo grande), luego esto nos indica que podría considerarse en cierto modo atípico.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#colinealidad",
    "href": "notebooks/Linear Regression/matrimionios_reg/matrimonios_reg.html#colinealidad",
    "title": "Regresión Lineal: matrimonios_reg",
    "section": "Colinealidad",
    "text": "Colinealidad\nCuando los regresores no tienen una relación lineal, se consideran ortogonales. Sin embargo, en la mayoría de las aplicaciones de regresión, los regresores no cumplen con esta condición. En algunos casos, la falta de ortogonalidad no representa un problema significativo, pero en otros, los regresores pueden estar tan estrechamente relacionados linealmente de manera que las predicciones del modelo de regresión se vuelven poco fiables o incorrectas.\nEste fenómeno, en el que los regresores presentan dependencias lineales casi perfectas, se conoce como el problema de colinealidad. Se realiza la matriz de correlaciones para ver la dependencia lineal entre las distintas variables.\n\nmat_cor &lt;- cor(data[, 3:5]) %&gt;% round(digits = 2)\ncorrplot(mat_cor, type = \"upper\", order = \"hclust\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nCombinando los resultados de la matriz anterior y el conocimiento de las variables es posible identificar variables que podrían generar el problema de multicolinealidad. Es por ello que se puede optar por suprimir uno de los dos predictores.\nPor otro lado, el factor de inflación de la varianza (FIV) detecta si una variable independiente es colineal con el resto. Es decir, mira cuanto se infla la varianza de los estimadores por culpa de la colinealidad de unas variables respecto a otras. Decisión: Un Valor del FIV mayor de 10 requiere actuación.\n\n# Inflación de la varianza\nvif(lm1)\n\nNacimientos     Mujeres \n   7.239639    7.239639 \n\n\nVemos que puede haber problema de multicolinealidad.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Matrimionios Reg",
      "Regresión Lineal: matrimonios_reg"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/salud_reg/Dataset_cleaning.html",
    "href": "notebooks/Linear Regression/salud_reg/Dataset_cleaning.html",
    "title": "Variables de Interés",
    "section": "",
    "text": "En este cuaderno vamos a procesar un conjunto de datos para posteriormente discriminar el sexo de una persona a partir de los microdatos de la Encuesta Nacional de Salud. Resultados Concretamente, se han tomado los datos relativos a 2017.\nLos microdatos de la Encuesta 2017 pueden descargarse en el siguiente link: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176783&menu=resultados&secc=1254736195295&idp=1254735573175#!tabs-1254736195295. Tomamos el fichero relativo a Adultos (15 años y más). De dichos microdatos leemos el fichero en formato .RData y vamos a tomar las siguientes variables:\n\nVariables de Interés\nAquí se muestran las variables que vamos a escoger para el estudio de una regresión lineal. En ella se pretende predecir el peso de una persona en función de su edad, altura, sexo y IMC. Si se conociera el valor exacto del IMC, entonces se podría saber el peso de manera exacta, ya que IMC = Peso/Altura2 , por lo que no sería necesario hacer un modelo para estimar la altura.\nPor el contrario, no tenemos el valor del IMC exacto, sino una clasificación del IMC entorno a 4 niveles. Este estudio demostrará como a patir de esos niveles, ya se puede estimar el peso con bastante exactitud, sin conocer el valor exacto del IMC.\n\nEDADa: Identificación del adulto seleccionado: Edad.\nSEXOa: Identificación del adulto seleccionado: Sexo.\nS109: Altura en cm.\nS110: Peso en kg.\nIMC: Índice de masa corporal (IMC) del adulto. Toma los siguientes valores\n\n1: Peso insuficiente\n2: Normopeso\n3: Sobrepeso\n4: Obesidad\n\n\n\n\nProcesamos el dataset para adaptarlo a lo de arriba\n# Microdatos\nlibrary(readxl)\ndatos &lt;- read_excel(\"/Users/davpero/Downloads/BECA/datos_ensalud17_xlsx/MICRODAT.CA.xlsx\")\ndatos &lt;- datos[, c(\"EDADa\", \"SEXOa\", \"S109\", \"S110\", \"IMCa\")]\nlibrary(dplyr)\ndatos &lt;- na.omit(datos)\n\n\n\n\n# Conevrtimos a factor variable sexo y la renombramos\ndatos$SEXO &lt;- as.factor(ifelse(datos$SEXOa == 1, 1, 0))\n\n# Numéricas las demas y las renombramos\ndatos$EDAD &lt;- as.numeric(datos$EDADa)\ndatos$Altura &lt;- as.numeric(datos$S109)\ndatos$Peso &lt;- as.numeric(datos$S110)\ndatos$IMC &lt;- as.factor(datos$IMCa)\n\n\n# Los valores atípicos los quitamos, los que tienen un peso y una altura que no tiene sentido\ndatos$Peso[datos$Peso &gt; 220] &lt;- NA\ndatos$Altura[datos$Altura &gt; 220] &lt;- NA\ndatos &lt;- na.omit(datos)\n\n\ndatos &lt;- datos[, c(\"EDAD\", \"SEXO\", \"Altura\", \"Peso\", \"IMC\")]\n\n# Creamos excel con datos\nlibrary(\"writexl\")\nwrite_xlsx(datos, \"../../../files/salud_reg.xlsx\")\nEste dataset será el que se proporcione para el estudiante para hacer sus análisis. Se puede encontrar en salud_reg.xlsx.\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Salud Reg",
      "Variables de Interés"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html",
    "title": "Regresión Lineal: Sucidios",
    "section": "",
    "text": "En este notebook se expondrá como llevar a cabo una Regresión Lineal a partir de un conjunto de datos, explicando las hipótesis que se deben satisfacer y un posterior análisis de bondad del modelo. Por último se explicará como utilizar el modelo creado para predecir nuevas observaciones.\n\n\nEn el siguiente dataset presenta la tasa de suicidios que ocurre en España con distintas variables demográficas y económicas que pueden llegar a influir o no, por Comunidades Autónomas.Concretamente tenemos las siguientes variables:\n\nanalf: Población de 16 años o más analfabeta.\nccaa: Comunidades Autónomas de España.\ndef: Número de defunciones.\ndefpc: Número de defunciones per cápita.\ndeu_nec: Deudores concursados necesarios\ngastoid : Gastos internos totales y personal en I+D\ngastomed: Gasto Medio por Persona.\ngastoprotec: Gasto en protección ambiental.\ngastoprotecpc: Gasto en protección ambiental per cápita.\nhabitantes: Número de habitantes por CCAA.\ninac: Inactivos.\nipc: Índice de Precios de Consumo.\nm_cond: Menores condenados.\nocu: Ocupados.\nparo: Número de personas en paro.\nparo_ld: Parados que llevan 2 años o más buscando empleo (larga duración).\nparopc: Número de parados per cápita.\npibpc: PIB per cápita.\nres_urb: Cantidad de residuos urbanos recogidos\nres_urbpc: Número de residuos urbanos per cápita.\nsoc_cons: Sociedades constituidas\nsoc_conspc: Número de sociedades constituidas per cápita.\nsuic: Suicidios.\nsuicpc: Número de suicidios per cápita.\ntur: Número de Turistas.\nturpc: Número de turistas per cápita.\nv_genero: Víctimas mortales por violencia de género.\nindice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\nsuic : Indicador sobre las condiciones materiales de vida.\nhabitantes : Indicador sobre el trabajo.\ndeu_nec : Indicador sobre la salud.\nipc : Indicador sobre la educación.\ndefpc : Indicador sobre el ocio y relaciones sociales.\ngastoprotecpc : Indicador sobre la seguridad física y personal.\npibpc : Indicador sobre la gobernanza y los derechos básicos.\nparopc : Indicador sobre el entorno y el medioambiente.\nCCAA: Comunidades Autónomas.\n\nEn este caso de uso, vamos a intentar explicar qué variables $(x1,x2,…,xk) $ pueden afectar al número de suicidios \\((y)\\) que ocurren en España.\nLos datos relativos a este estudio corresponden a una serie de estadísticas relacionadas con el suicidio y la población como pueden ser la Estadística de Defunciones según la Causa de Muerte o la Encuesta de Población Activa (EPA). Aunque el dataset presente muchas variables, sólo se usaran las relativas a términos per cápita puesto que las demás tienen una correlación bastante alta y esto puede causar problemas.\n\n\n\nSe pretende hacer una regresión lineal que explique el número de suicpc en función del resto de variables que sean pertinentes.\n\nHacer un análisis exploratorio. Ver que las variables que no son per cápita tienen una correlación muy altas y por tanto, no considerarlas.\nPlantear las hipótesis de una regresión.\nAnalizar el modelo planteado y su ajuste de bondad.\nComprobar hipótesis de regresión.\nSelección de variables.\nHacer una conclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#dataset",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#dataset",
    "title": "Regresión Lineal: Sucidios",
    "section": "",
    "text": "En el siguiente dataset presenta la tasa de suicidios que ocurre en España con distintas variables demográficas y económicas que pueden llegar a influir o no, por Comunidades Autónomas.Concretamente tenemos las siguientes variables:\n\nanalf: Población de 16 años o más analfabeta.\nccaa: Comunidades Autónomas de España.\ndef: Número de defunciones.\ndefpc: Número de defunciones per cápita.\ndeu_nec: Deudores concursados necesarios\ngastoid : Gastos internos totales y personal en I+D\ngastomed: Gasto Medio por Persona.\ngastoprotec: Gasto en protección ambiental.\ngastoprotecpc: Gasto en protección ambiental per cápita.\nhabitantes: Número de habitantes por CCAA.\ninac: Inactivos.\nipc: Índice de Precios de Consumo.\nm_cond: Menores condenados.\nocu: Ocupados.\nparo: Número de personas en paro.\nparo_ld: Parados que llevan 2 años o más buscando empleo (larga duración).\nparopc: Número de parados per cápita.\npibpc: PIB per cápita.\nres_urb: Cantidad de residuos urbanos recogidos\nres_urbpc: Número de residuos urbanos per cápita.\nsoc_cons: Sociedades constituidas\nsoc_conspc: Número de sociedades constituidas per cápita.\nsuic: Suicidios.\nsuicpc: Número de suicidios per cápita.\ntur: Número de Turistas.\nturpc: Número de turistas per cápita.\nv_genero: Víctimas mortales por violencia de género.\nindice_total: Índice multidimensional de calidad de vida teniendo en cuenta las nueve dimensiones.\nsuic : Indicador sobre las condiciones materiales de vida.\nhabitantes : Indicador sobre el trabajo.\ndeu_nec : Indicador sobre la salud.\nipc : Indicador sobre la educación.\ndefpc : Indicador sobre el ocio y relaciones sociales.\ngastoprotecpc : Indicador sobre la seguridad física y personal.\npibpc : Indicador sobre la gobernanza y los derechos básicos.\nparopc : Indicador sobre el entorno y el medioambiente.\nCCAA: Comunidades Autónomas.\n\nEn este caso de uso, vamos a intentar explicar qué variables $(x1,x2,…,xk) $ pueden afectar al número de suicidios \\((y)\\) que ocurren en España.\nLos datos relativos a este estudio corresponden a una serie de estadísticas relacionadas con el suicidio y la población como pueden ser la Estadística de Defunciones según la Causa de Muerte o la Encuesta de Población Activa (EPA). Aunque el dataset presente muchas variables, sólo se usaran las relativas a términos per cápita puesto que las demás tienen una correlación bastante alta y esto puede causar problemas.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#descripción-del-trabajo-a-realizar",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#descripción-del-trabajo-a-realizar",
    "title": "Regresión Lineal: Sucidios",
    "section": "",
    "text": "Se pretende hacer una regresión lineal que explique el número de suicpc en función del resto de variables que sean pertinentes.\n\nHacer un análisis exploratorio. Ver que las variables que no son per cápita tienen una correlación muy altas y por tanto, no considerarlas.\nPlantear las hipótesis de una regresión.\nAnalizar el modelo planteado y su ajuste de bondad.\nComprobar hipótesis de regresión.\nSelección de variables.\nHacer una conclusión.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#cargar-librerías",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#cargar-librerías",
    "title": "Regresión Lineal: Sucidios",
    "section": "Cargar Librerías",
    "text": "Cargar Librerías\nLo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:\n\nlibrary(readxl) # Para leer los excels\nlibrary(kableExtra) # Para dar formato a las tablas html\nlibrary(knitr)\nlibrary(gridExtra) # Para cargar bien las tab\nlibrary(car) # for bonfferroni test\nlibrary(reshape2) # Para manejar los datos\nlibrary(dplyr) # Para manejar los datos\nlibrary(ggplot2)\nlibrary(lmtest) # Test Homocedasticidad\nlibrary(corrplot) # Para el gráfico de correlaciones\n\nAhora leemos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#lectura-datos",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#lectura-datos",
    "title": "Regresión Lineal: Sucidios",
    "section": "Lectura datos",
    "text": "Lectura datos\nAhora cargamos los datos del excel correspondientes a la pestaña “Datos” y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.\n\nsuicidios &lt;- read_excel(\"../../../files/suicidios2019CCAA.xlsx\", sheet = \"Datos\")\n\n\nanyNA(suicidios) # Any missing data\n\n[1] FALSE",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#análisis",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#análisis",
    "title": "Regresión Lineal: Sucidios",
    "section": "Análisis",
    "text": "Análisis\n\n\n\nTabla mostrando las medidas de interés para cada variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nMin\nQ1\nMean\nMedian\nQ3\nMax\n\n\n\n\nsuic\nsuic\n29.0\n92.0\n230.0\n179.0\n305.0\n793.0\n\n\nhabitantes\nhabitantes\n319796.0\n1059501.0\n2777372.0\n2049562.0\n2695645.0\n8472407.0\n\n\nsuicpc\nsuicpc\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000121\n\n\ndeu_nec\ndeu_nec\n0.0\n1.0\n6.0\n4.0\n8.0\n20.0\n\n\nipc\nipc\n109.0\n110.0\n110.0\n110.0\n111.0\n112.291\n\n\ndefpc\ndefpc\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01321\n\n\ndef\ndef\n3409.0\n12333.0\n25766.0\n17178.0\n32841.0\n79498.0\n\n\ngastoprotec\ngastoprotec\n3000599.0\n11428710.0\n43314589.0\n34075312.0\n56016655.0\n148771215.0\n\n\ngastoprotecpc\ngastoprotecpc\n1.0\n9.0\n19.0\n14.0\n22.0\n55.3638\n\n\ngastoid\ngastoid\n64412.0\n193658.0\n927305.0\n341157.0\n1236012.0\n4252947.0\n\n\npibpc\npibpc\n19454.0\n21642.0\n25946.0\n24383.0\n28727.0\n35913.0\n\n\nparopc\nparopc\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000471\n\n\nparo\nparo\n140.0\n396.0\n1160.0\n847.0\n1085.0\n3442.1\n\n\nsoc_cons\nsoc_cons\n351.0\n1035.0\n4650.0\n2263.0\n3212.0\n17965.0\n\n\nsoc_conspc\nsoc_conspc\n0.0\n0.0\n0.0\n0.0\n0.0\n0.002661\n\n\nv_genero\nv_genero\n0.0\n1.0\n3.0\n1.0\n3.0\n9.0\n\n\nres_urbpc\nres_urbpc\n0.0\n0.0\n81.0\n1.0\n1.0\n1368.1\n\n\nres_urb\nres_urb\n133764.0\n507580.0\n1334382.0\n883808.0\n1282486.0\n4310649.0\n\n\ntur\ntur\n47913.0\n161462.0\n1833408.0\n499370.0\n4019766.0\n6324711.0\n\n\nturpc\nturpc\n0.0\n0.0\n1.0\n0.0\n1.0\n5.391874\n\n\nocu\nocu\n140.0\n396.0\n1160.0\n847.0\n1085.0\n3442.1\n\n\ninac\ninac\n106.0\n400.0\n965.0\n693.0\n1113.0\n3031.5\n\n\nanalf\nanalf\n0.0\n1.0\n1.0\n1.0\n2.0\n2.4\n\n\nparo_ld\nparo_ld\n4.0\n17.0\n54.0\n31.0\n75.0\n271.4\n\n\nm_cond\nm_cond\n77.0\n214.0\n652.0\n440.0\n750.0\n2382.0\n\n\ng_med\ng_med\n9587.0\n10480.0\n11607.0\n11806.0\n12022.0\n13981.58\n\n\n\n\n\nLo primero vamos a ver correlaciones para ver si podemos reducir la dimensionalidad ya que tenemos un número muy alto de variables. En el siguiente gráfico se muestra la correlación entre distintas variables, que puede ser positiva o negativa. Concretamente, la matriz de coeficientes de correlación, expresada con un número que va desde \\(-1\\) a \\(1\\). La diagonal es siempre \\(1\\) pues es trivial que una variable esta \\(100\\%\\) correlada con si misma. Además, se dice que dos variables están altamente correlacionadas si su valor \\(\\geq 0,7\\).\n\n\n\n\n\nCorrelaciones entre variables\n\n\n\n\nPodemos observar que las variables que no son per cápita presentan una correlación bastante alta. Esto es debido al “efecto tamaño”1 que produce dependientes del tamaño de la población considerada. Es por ello que se van a usar medidas per cápita que eliminan esta correlación, siendo la variable a predecir suicpc.\n\n\n\n\n\nCorrelaciones entre variables per cápita\n\n\n\n\n\n\n\n\n\nCorrelaciones entre variables que no son per cápita\n\n\n\n\nComo se comentaba, debido a que las medidas per cápita tienen una correlación mucho menor, serán estas las que intentemos usar para la regresión lineal con objetivo de explicar el número de suicidios per cápita. En las siguientes Figuras podemos observar que no parece haber datos atípicos en cuanto a esta magnitud, aunque para ser precisos se tienen un número muy bajo de observaciones.\n\n\n\n\n\nSuicidios per cápita.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#hipótesis-y-indicadores-de-bondad",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#hipótesis-y-indicadores-de-bondad",
    "title": "Regresión Lineal: Sucidios",
    "section": "Hipótesis y indicadores de bondad",
    "text": "Hipótesis y indicadores de bondad\nPara que una regresión lineal proporcione un buen ajuste a los datos debe cumplir una serie de requisitos que por tanto deben ser verificados al llevar a cabo el estudio. Recordar que la regresión lineal se expresa como: \\[\n\\mathbf{Y}=\\mathbf{X} \\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\n\\] donde \\(\\mathbf{Y}\\) es la variable respuesta, \\(\\mathbf{X}\\) los predictores (hay \\(k\\) variables predictoras), \\(\\boldsymbol{\\beta}\\) los coeficientes de la regresión y \\(\\boldsymbol{\\varepsilon}\\) el error. \\[\n\\mathbf{Y}=\\left[\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array}\\right] \\quad \\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1 k} \\\\\n1 & x_{21} & \\ldots & x_{2 k} \\\\\n\\vdots & \\ddots & \\vdots & \\\\\n1 & x_{n 1} & \\ldots & x_{n k}\n\\end{array}\\right] \\quad \\boldsymbol{\\beta}=\\left[\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{array}\\right] \\quad \\boldsymbol{\\varepsilon}=\\left[\\begin{array}{c}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{array}\\right]\n\\] Las hipótesis que se deben cumplir son:\n\nLinealidad: La media de la respuesta es función lineal de los predictores. En términos matemáticos: \\[E\\left[\\mathbf{Y} \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k \\]\nIndependencia de errores: Los errores \\(\\varepsilon_i\\) deben ser independientes, es decir, \\(Cov[\\varepsilon_i,\\varepsilon_j] =0, \\; \\forall i\\neq j\\).\nHomocedasticidad: La varianza del error debe ser constante.\n\n\\[Var\\left[\\varepsilon_i \\mid \\mathbf{X}_1=x_1, \\ldots, \\mathbf{X}_k=x_k\\right]=\\sigma^2 \\quad \\forall \\;i \\]\n\nNormalidad : Los errores deben estar distribuidos normalmente, es decir, \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\; \\forall i\\).\n\nPara analizar la bondad, hay algunos indicadores como el Coeficiente de Determinación o \\(R^2\\) que representa el porcentaje de variabilidad de la variable respuesta que es capaz de explicar el modelo. Es decir, si toma valor 1 hay una dependencia lineal exacta entre los predictores y la variable respuesta y por tanto las predicciones serán perfectas. Por el contrario, si toma valor 0 habrá que desechar el modelo puesto que no es capaz de predecir con nada de exactitud.\nEn caso de que haya más de un predictor (\\(k &gt;1\\), Regresión Lineal Múltiple), es más recomendable usar el Coeficiente de Determinación Ajustado \\(R^2\\_adj\\) como indicador de bondad, pues el \\(R^2\\) puede inflarse artificialmente debido a la presencia de varios predictores. Su interpretación es similar.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#modelo",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#modelo",
    "title": "Regresión Lineal: Sucidios",
    "section": "Modelo",
    "text": "Modelo\nEn este caso nos encontramos ante una Regresión Lineal Múltiple puesto que tenemos más de una variable predictora. Inicialmente vamos a considerar un modelo con todas variables predictoras para intentar predecir el \\(suicpc\\) y veremos si este modelo cumple las hipótesis necesarias y cuan bueno es.\n\n# Modelo inicial\nlm1 &lt;- lm(suicpc ~ defpc + gastoprotecpc + pibpc + paropc + soc_conspc + turpc, suicidios)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = suicpc ~ defpc + gastoprotecpc + pibpc + paropc + \n    soc_conspc + turpc, data = suicidios)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.042e-05 -2.210e-06  8.922e-07  1.754e-06  1.243e-05 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    1.234e-04  5.045e-05   2.445   0.0345 *\ndefpc          3.908e-03  1.753e-03   2.230   0.0498 *\ngastoprotecpc  3.982e-07  1.526e-07   2.610   0.0260 *\npibpc         -9.398e-10  6.414e-10  -1.465   0.1735  \nparopc        -1.556e-01  1.322e-01  -1.177   0.2664  \nsoc_conspc     2.118e-03  5.549e-03   0.382   0.7106  \nturpc          3.602e-06  1.850e-06   1.947   0.0802 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.63e-06 on 10 degrees of freedom\nMultiple R-squared:  0.8569,    Adjusted R-squared:  0.7711 \nF-statistic: 9.984 on 6 and 10 DF,  p-value: 0.0009766\n\n\nA primera vista vemos un valor de Múltiple R-squared: 0.8569,, lo cual es bastante alto y por tanto nuestro modelo parece capturar bien la variabilidad de la variable respuesta, concretamente un \\(85\\%\\). Sin embargo, en los sucesivos modelos que planteemos no podemos usar como criterio de comparación el \\(R-squared\\) pues aumenta a la vez que lo hace el número de variables, y por tanto para comparar modelos entre si se debe usar el Adjusted R-squared (que tiene en cuenta el número de variables) que toma un valor de 77%.\nEn la línea de los residuos no parece haber contraindicaciones a que estos sigan una distribución normal centrada en cero puesto que tenemos unas medidas de dispersión bastante simétricas. No obstante, más adelante se procederán a hacer los test pertinentes.\nEn la última linea se lleva a cabo un Test de Significación Global \\(F-Test\\) lo que considera es la hipótesis nula de\n\\[H_0: \\beta_i =0\\; \\forall i\n\\\\\nH_1: al\\; menos \\; un \\; \\beta_i \\neq 0\\].\nPara un nivel de confianza de \\(0.95\\) podemos rechazar la hipótesis nula (puesto que p_val&lt; 0.05) y por tanto aceptar la alternativa, lo cual es buena señal.\nNo obstante es necesario analizar que se cumplan las hipótesis iniciales para poder asegurar que estamos ante un buen modelo.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-de-bonferroni-datos-atípicos",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-de-bonferroni-datos-atípicos",
    "title": "Regresión Lineal: Sucidios",
    "section": "Test de Bonferroni (datos atípicos)",
    "text": "Test de Bonferroni (datos atípicos)\nLa idea principal es verificar si los residuos de las observaciones son significativamente diferentes de cero. Si un residuo tiene un valor “studentizado” grande en comparación con una distribución t, puede considerarse como un posible valor atípico. Esto se debe a que teóricamente se demuestra que los residuos “studentizados” \\(r^* _i \\sim t_{n-k-1}\\) con k el número de predictores. En este caso parece haber un posible valor atípico.\n\noutlierTest(lm1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n  rstudent unadjusted p-value Bonferroni p\n5 2.595927           0.028931      0.49182",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-homocedasticidad",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-homocedasticidad",
    "title": "Regresión Lineal: Sucidios",
    "section": "Test homocedasticidad",
    "text": "Test homocedasticidad\nEn términos sencillos, la Prueba de Breusch-Pagan evalúa si la varianza de los errores en un modelo de regresión es constante o si varía a lo largo de los valores de las variables predictoras. Una violación de la homocedasticidad puede afectar la validez de las inferencias realizadas a partir del modelo.\nEl test funciona de la siguiente manera: se obtienen los residuos al cuadrado y se realiza una regresión auxiliar para determinar si hay una relación significativa entre los residuos al cuadrado y las variables predictoras. Si se encuentra evidencia significativa, puede indicar la presencia de heterocedasticidad.\n\nbptest(suicpc ~ defpc + gastoprotecpc + pibpc + paropc + soc_conspc + turpc, suicidios, varformula = ~ fitted.values(lm1), studentize = FALSE)\n\n\n    Breusch-Pagan test\n\ndata:  suicpc ~ defpc + gastoprotecpc + pibpc + paropc + soc_conspc +     turpc\nBP = 0.058831, df = 1, p-value = 0.8084\n\n\n\\[H_0: Var[\\varepsilon_i ] =\\sigma^2 \\; \\forall \\;i\n\\\\\nH_1: Var[\\varepsilon_i ] \\neq \\sigma^2 \\; \\forall \\;i\\].\nSi el valor p obtenido de la prueba de Breusch-Pagan es \\(0.8\\), interpretaríamos esto como evidencia insuficiente para rechazar la hipótesis nula de homocedasticidad (a nivel de significancia de \\(0.05\\)). En otras palabras, no tendríamos suficiente evidencia estadística para decir que hay heterocedasticidad en los errores del modelo de regresión.\nEn términos prácticos, esto sugiere que la varianza de los errores parece ser constante a lo largo de los valores de las variables predictoras.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#normalidad-de-residuos",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#normalidad-de-residuos",
    "title": "Regresión Lineal: Sucidios",
    "section": "Normalidad de residuos",
    "text": "Normalidad de residuos\nEl Test de Shapiro es una prueba de normalidad que se utiliza para evaluar si una muestra proviene de una población con una distribución normal. La hipótesis nula del test es que la muestra sigue una distribución normal. Si el valor p obtenido en la prueba es menor que el nivel de significancia (comúnmente establecido en \\(0.05\\)), se rechaza la hipótesis nula, indicando que la muestra no sigue una distribución normal.\n\\[H_0: \\varepsilon_i  \\sim N(\\;,\\;) \\; \\forall\n\\\\\nH_1: \\varepsilon_i  \\nsim N(\\;,\\;) \\; \\forall \\].\n\nshapiro.test(lm1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm1$residuals\nW = 0.93194, p-value = 0.2345\n\n\nAceptamos la normalidad de los residuos puesto que el \\(p-value&gt;0.05\\).",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-linealidad",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#test-linealidad",
    "title": "Regresión Lineal: Sucidios",
    "section": "Test linealidad",
    "text": "Test linealidad\nLa hipótesis alternativa analiza si la inclusión de términos cuadráticos (potencia 2) de las variables predictoras mejora significativamente el modelo en comparación con un modelo que solo incluye términos lineales.\n\nresettest(suicpc ~ defpc + gastoprotecpc + pibpc + paropc + soc_conspc + turpc, suicidios, power = 2, type = \"regressor\")\n\n\n    RESET test\n\ndata:  suicpc ~ defpc + gastoprotecpc + pibpc + paropc + soc_conspc +     turpc\nRESET = 0.72339, df1 = 6, df2 = 4, p-value = 0.6563\n\n\nAceptamos la linealidad puesto que el \\(p-value&gt;0.05\\), a un nivel de significancia de \\(\\alpha =0.05\\), luego no es necesario incluir términos cuadráticos.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#gráfico-de-influencia-del-modelo-porpuesto",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#gráfico-de-influencia-del-modelo-porpuesto",
    "title": "Regresión Lineal: Sucidios",
    "section": "Gráfico de influencia del modelo porpuesto",
    "text": "Gráfico de influencia del modelo porpuesto\nEn el siguiente gráfico se muestran los residuos “studentizados”, es decir, los residuos transformados a una \\(N(0,1)\\). Es por ello, que debido a el cuantil \\(z_{\\alpha/2}=-1.96\\; con \\; \\alpha=0.05\\) de una normal, sabemos que el \\(95\\%\\) de elementos deben estar contenidos en \\((-1,96,1.96)\\) que son las rayas horizontales del gráfico.\n\ninfluencePlot(lm1, id = list(method = \"noteworth\", n = 2))\n\n\n\n\n\n\n\n\n\nLos residuos bajo hipótesis de RLM siguen una N(0,sigma) y los “studentizados” un N(0,1), es decir el \\(95\\%\\) de datos están entre \\((-1.96,1.96)\\), las líneas horizontales. Tenemos 20 observaciones y 2 o 3 datos fuera de la línea lo que a priori podría ser correcto.\nLas líneas verticales indican los datos con apalancamiento en el modelo. Es decir los datos fuera de la línea vertical derecha No vemos ni siquiera las lineas entonces no parece haber apalancamiento.\nEl área de las burbujas es proporcional a la dist. de cook (mide cómo cambian los parámetros del modelo cuando se excluye una observación específica). Vemos que hay uno con una gran distancia de cook (tienen residuo grande), luego esto nos indica que podría considerarse en cierto modo atípico. Como no hemos encontrado más evidencias de que fuera atípico lo vamos a dejar así.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#colinealidad",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#colinealidad",
    "title": "Regresión Lineal: Sucidios",
    "section": "Colinealidad",
    "text": "Colinealidad\nCuando los regresores no tienen una relación lineal, se consideran ortogonales. Sin embargo, en la mayoría de las aplicaciones de regresión, los regresores no cumplen con esta condición. En algunos casos, la falta de ortogonalidad no representa un problema significativo, pero en otros, los regresores pueden estar tan estrechamente relacionados linealmente de manera que las predicciones del modelo de regresión se vuelven poco fiables o incorrectas.\nEste fenómeno, en el que los regresores presentan dependencias lineales casi perfectas, se conoce como el problema de colinealidad. Se realiza la matriz de correlaciones para ver la dependencia lineal entre las distintas variables.\n\nmat_cor &lt;- cor(suicidios[, c(\"defpc\", \"gastoprotecpc\", \"pibpc\", \"paropc\", \"soc_conspc\", \"turpc\")]) %&gt;% round(digits = 2)\ncorrplot(mat_cor, type = \"upper\", order = \"hclust\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nNo parece haber ningún rastro de colinealidad (al menos directa), entre los predictores. Para estar más seguros vamos a realizar un análisis del VIF.\nEl factor de inflación de la varianza (FIV) detecta si una variable independiente es colineal con el resto. Es decir, mira cuanto se infla la varianza de los estimadores por culpa de la colinealidad de unas variables respecto a otras. Decisión: Un Valor del FIV mayor de 10 requiere actuación.\n\n# Inflación de la varianza\nvif(lm1)\n\n        defpc gastoprotecpc         pibpc        paropc    soc_conspc \n     2.767328      1.568304      2.961025      3.757949      2.151474 \n        turpc \n     1.641132 \n\n\nVemos que la primera dimensión podría presentar problemas de colinealidad.Posibles soluciones:\n\nEliminar una de las variables, p.ej. la de mayor FIV.\nUtilizar regresión sobre componentes principales (esto es una transformación lineal de las variables independientes de forma que las nuevas variables sean incorreladas entre sí y de varianza decreciente) o bien\nDisponer de algún método de selección de las variables dentro del modelo.\n\nEn nuestro caso no hay ninguna grande lo cual no tenemos ningún indicio para alarmarnos.\n\nSelección de variables\nA la hora de realizar un modelo de acuerdo a una lista de variables estadísticas, suele ser frecuente estudiar la relevancia de todas ellas, o si por el contrario con un número reducido de predictores es suficiente. Además del \\(R^2_{adj}\\), el Criterio de Información de Akaike (AIC), es una medida para comparar diferentes modelos. Comparando dos modelos, el que tenga un AIC más bajo se puede considerar mejor modelo.\nVamos a realizar un método de Stepwise que lo que hace es incluyendo/sacando variables sobre el modelo inicial hasta encontrar el modelo con el mejor AIC(el más bajo). No se ejecuta en el notebook dicha función puesto que tiene una salida muy larga, aunque es recomendable su uso.\n\nlibrary(Rcmdr)\nstepwise(lm1, direction = \"backward/forward\", criterion = \"AIC\")\n\nNos ha quitado una variable pero el AIC tampoco ha bajado tanto por lo que podríamos considerar ambos modelos como buenos.",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  },
  {
    "objectID": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#footnotes",
    "href": "notebooks/Linear Regression/Suicidios_2019CCAA/Suicidios_2019.html#footnotes",
    "title": "Regresión Lineal: Sucidios",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEjemplo: La Comunidad de Madrid que tiene muchos más habitantes que Navarra, es trivial que va a tener número de muertes, de suicidios, de ipc,.. más alto que Navarra. Es por ello que nos interesa tener una medida invariante ante la estructura poblacional y para ello dividimos los valores obtenidos entre la población total.↩︎",
    "crumbs": [
      "Notebooks",
      "Linear Regression",
      "Suicidios 2019CCAA",
      "Regresión Lineal: Sucidios"
    ]
  }
]