---
title: 'Reducción Dimensionalidad - PCA: provincias_variado'
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", warning = FALSE, message = FALSE)


# fig.align= 'center para que nos centre todas figuras que se muestran
# warning = FALSE para que no muestre mensajes de warning
# message = FALSE para que no muestre mensajes automáticos cuando carga paquetes y demás en el output
```


# Introducción

A continuación se expondrá como llevar a cabo una Reducción de Dimensionalidad utilizando el método de Componentes Principales. Para ello se utilizará un dataset sobre el que se irán explicando los sucesivos pasos a llevar a cabo.



## dataset

En este cuaderno vamos a analizar el dataset llamado
[*variado_provincias.xlsx*](../../../../files/variado_provincias.xlsx).
Este dataset presenta datos para las provincias, abordando información diversa sobre variables socioeconómicas. Contiene datos como número de explotaciones agrícolas, superficies agrícolas, indicadores de empleo, ejecuciones hipotecarias, empresas por sector, PIB per cápita y datos demográficos. 


Concretamente en este dataset tenemos las siguientes variables:

  - **prov**:	Nombre de la provincia o ciudad autónoma.
  - **neag**:	Número de explotaciones agrícolas (2009).
  - **sagt**:	Superficie agrícola total  (2009).
  - **sagu**:	Superficie agrícola utilizada  (2009).
  - **benf**:	Beneficiarios de prestaciones por desempleo (2019).
  - **acem**:	Importe de todos los programas de Apoyo a la Creación de Empleo (2019).
  - **acti**:	Personas en situación laboral activa (2019).
  - **ocup**:	Personas en situación laboral de ocupación (2019).
  - **para**:	Personas en situación laboral de desempleo (2019).
  - **ejec**:	Ejecuciones hipotecarias de fincas urbanas de tipo vivienda (2019).
  - **inds**:	Número de empresas dentro del sector industrial (2020).
  - **cnst**:	Número de empresas dentro del sector de la construcción (2020).
  - **ctrh**:	Número de empresas dentro del sector del Comercio, el transporte y la hostelería (2020).
  - **serv**:	Número de empresas dentro del sector servicios (2020).
  - **pibc**:	Producto Interior Bruto per Cápita (2018).
  - **ipib**:	Indice o ratio de PIB per cápita siendo el valor para el conjunto del Estado Español 1.
  - **inmi**:	Personas nacidas fuera de España y nacionalidad distinta a la española que inmigran instalándose en la provincia.
  - **pobl**:	Población total independientemente de la nacionalidad.
  - **espa**:	Población de nacionalidad española.
  - **d_migr**:	Provincia de alta recepción de inmigración.
  - **d_prod**:	Provincia con un PIB per cápita superior al del conjunto del Estado.
  - **d_cons**:	Provincia con alta dedicación a la construcción.
  - **m_ieag**:	Intensividad de explotación agrícola.


## Descripción del trabajo a realizar

Se pretende hacer un Análisis de
Reducción de la Dimensionalidad empleando el procedimiento de
Componentes Principales. El objetivo es conocer que variables
independientes son de interés para estudiar las variables socioeconómicas de las provincias y ver cuáles son las más parecidas y las más diferentes.

-   Hacer un análisis exploratorio explorando matriz de correlaciones.
-   Ver si es necesario escalar/centrar los datos antes de aplicar pca y
    decidir si hacerlo con matriz de correlaciones o covarianzas.
-   Seleccionar un determinado número de componentes y ver como influyen
    las variables en estas.
-   Interpretar componentes y resultados.


Si tomamos demasiadas variables es difícil visualizar relaciones entre
ellas. Otro problema que se presenta es la fuerte correlación. Se hace
necesario, pues, reducir el número de variables sin perder información.
Es importante resaltar el hecho de que el concepto de mayor información
se relaciona con el de mayor variabilidad o varianza.






# Análisis Exploratorio (EDA)



EDA viene del Inglés *Exploratory Data Analysis* y son los pasos relativos en los que se exploran las variables para tener una idea de que forma toma el dataset.



## Cargar Librerías
Lo primero de todo vamos a cargar las librerías necesarias para ejecutar el resto del código del trabajo:

```{r}
#| label: librerias
#| message: false
#| warning: false
# Librerías
library(readxl) # Para leer los excels
library(dplyr) # Para tratamiento de dataframes
library(ggplot2) # Nice plots
library(factoextra) # fviz_cluster function
library(ggcorrplot) # Para funcion ggcorrplot
library(corrplot) # Para corrplot
```

## Lectura datos

Ahora cargamos los datos del excel correspondientes a la pestaña *"Datos"* y vemos si hay algún NA o algún valor igual a 0 en nuestro dataset. Vemos que no han ningún NA (missing value) en el dataset luego no será necesario realizar ninguna técnica para imputar los missing values o borrar observaciones.

Cargamos entonces el conjunto de datos:

```{r}
#| label: cargar_datos
datos <- read_excel("../../../../files/provincias_variado.xlsx", sheet = "Datos")
```



En primer lugar, cargamos los datos que vamos a utilizar. En este caso,
se trata de un conjunto de datos compuesto por 52 filas y 23 columnas.
Las filas corresponden a las 50 provincias de España, más Ceuta y Melilla.

Antes de comenzar a aplicar la técnica, comprobamos si hay valores
faltantes, por si fuera necesario realizar algún preproceso. En este
caso, y como vemos a continuación, no hay ningún NA en los datos que
vamos a utilizar.

```{r}
sum(is.na(datos))
```

Por otra parte, para tener una noción general que nos permita describir
el conjunto con el que vamos a trabajar, podemos extraer su dimensión,
el tipo de variables que contiene o qué valores toma cada una.

```{r}
# Dimensión del conjunto de datos
dim(datos)

# Tipo de variables que contiene
str(datos)

# Descripción de las variables
summary(datos)
```

Vemos que hay alguna observación como Ceuta y Melilla con datos faltantes, luego los eliminamos y posteriormente convertimos a variable numérica las que han sido consideradas como carácter(debido a los fallos para Ceuta y Melilla) y deberían ser variables numéricas.

```{r}
# Ceuta y Melilla tienen datos faltantes luego eliminamos observaciones
datos <- datos[datos$prov != "Ceuta", ]
datos <- datos[datos$prov != "Melilla", ]

# Convertimos a variables numéricas
datos$acem <- as.numeric(datos$acem)
datos$neag <- as.numeric(datos$neag)
datos$sagt <- as.numeric(datos$sagt)
datos$sagu <- as.numeric(datos$sagu)
```



**Correlación:** El que existan correlaciones muy elevadas en el
conjunto de datos nos permitirá resumir la información en un menor
número de componentes principales, pues éstas explicarán una mayor
cantidad de información.

```{r}
ggcorrplot(cor(datos[, 2:23]), type = "lower", lab = T, show.legend = T, lab_size = 1.9)
```


En este caso, se ha generado un gráfico entre variables, sin tener en
cuenta la correlación de la variable consigo misma, pues siempre será
del 100%. En términos absolutos, vemos lo siguiente:


-   Las variables *sagt* y *sagu* presentan una correlación alta lo que parece razonable. puesto que ambas están relacionadas con superficies agrícolas. 

- Por otro lado, se encuentran también bastante correlacionadas entre ellas *benf*, *acem*, *acti*, *ocup*, *para*, *ejec*, *inds*, *cnst* y *ctrh*, las cuales hablan sobre la actividad económica, el mercado laboral y la situación financiera.

- Además, las variables *inmi*, *pobl*, *espa* que hablan sobre población se encuentran correlacionadas con las variables socioeconómicas citadas antes.


En general es un dataset que presenta variables con bastante correlación luego esto, probablemente, nos permitirá reducir bastante el numero de variables finales.

# Reducción Dimensionalidad: Componentes Principales

## Introducción

El **Análisis de Componentes Principales (PCA)** es una técnica para
reducir la complejidad de conjuntos de datos con múltiples variables. Su
objetivo es transformar variables correlacionadas en un conjunto menor
de dimensiones sin perder la mayor parte de la información original.

Se busca encontrar **nuevas variables (componentes)** que estén
incorrelacionadas y que capturen la máxima variabilidad de los datos.
Esto se logra mediante combinaciones lineales de las variables
originales. PCA es útil para entender relaciones, reducir dimensiones y
manejar la alta correlación entre variables.

Para aplicar PCA, se necesitan **datos cuantitativos** y es crucial
*escalar las variables* (estandarizar = media cero y varianza uno). Esto
garantiza que ninguna variable domine el análisis. Además, se puede
trabajar con la matriz de correlaciones para abordar fuertes
correlaciones entre variables, manteniendo así la información más
relevante del conjunto de datos.

Los pasos generales son:

1.  **Estandarización de las variables**: Es importante estandarizar las
    variables numéricas para que tengan media cero y desviación estándar
    uno. Esto es crucial para que ninguna variable domine el análisis
    debido a su escala.

2.  **Cálculo de la matriz de correlaciones o covarianzas**: Dependiendo
    del enfoque, se puede trabajar con la matriz de correlaciones si se
    busca abordar fuertes correlaciones entre variables, o con la matriz
    de covarianzas si se busca la varianza total de las variables.

-   **NOTA**: Aconsejable trabajar siempre con la matriz de
    correlaciones (a no ser que todas variables estén en las mismas
    unidades, que se podrá usar la matriz de covarianzas). De no seguir
    esta nota y usar la matriz de covarianzas, las variables que tienen
    mayores unidades dominarán la estructura de covarianza, lo que
    llevará a una representación inexacta de la variabilidad real de los
    datos.

3.  **Descomposición de la matriz**: Se descompone la matriz de
    correlaciones en sus vectores y valores propios. Los valores propios
    representan la cantidad de varianza explicada por cada componente
    principal, mientras que los vectores propios (autovectores)
    determinan la dirección de cada componente en el espacio
    multidimensional original.

4.  **Selección de componentes**: Los componentes se ordenan de manera
    descendente según la cantidad de varianza que explican. Los primeros
    componentes capturan la mayor variabilidad de los datos y se
    seleccionan para reducir la dimensionalidad manteniendo la
    información más relevante.

5.  **Transformación de datos**: Proyectar los datos originales en el
    espacio de los componentes principales para obtener las nuevas
    variables. Estas son combinaciones lineales de las variables
    originales y son ortogonales entre sí. Esta transformación lineal
    **conserva la mayor parte de la información en un espacio de menor
    dimensión, lo que facilita el análisis y la visualización de los
    datos**.

6.  **Interpretación y visualización**: Explorar la importancia de cada
    componente en términos de la variabilidad explicada. Se pueden
    interpretar los componentes para comprender qué aspectos de los
    datos capturan. Si es posible, representar gráficamente los datos en
    el espacio reducido de los componentes principales para obtener una
    mejor comprensión de las relaciones entre las observaciones.

## Modelo

En las siguientes lineas haremos que la variable `prov` se ponga como
nombre de filas y posteriormente eliminaremos esa variable ya que ya la
tendremos como nombre de filas.

```{r}
prov <- datos$prov
datos <- datos[, -1] # Eliminamos ahora
rownames(datos) <- prov # Como nombres de filas las provincias
```

Escalamos los datos y calculamos la matriz de varianzas covarianzas,
mostramos solo la diagonal (debería ser 1).

```{r}
datos2 <- scale(datos)
summary(datos2)
diag(var(datos2))
```

Aplicamos funcion PCA, notar que en este caso no haría falta los
argumentos `SCALE=TRUE` y `CENTER=TRUE` puesto que ya hemos escalado dos
datos en un paso previo. Por defecto en la función viene el valor de
`SCALE=FALSE` y `CENTER=TRUE`.

```{r}
pca <- prcomp(datos2, center = TRUE, scale = TRUE) # Scale=T
```

**Calculamos los coeficientes de la ecuación para cada componente principal** (Autovectores)

```{r}
pca$rotation
```

Podemos observar aquí nuestras variables en el nuevo sistema de
cordenadas (las componentes principales), dando lugar a ecuaciones de
cada eje como combinación lineal del total de variables. Analizar el
vector que crea cada componente y cuáles son los pesos que tienen las
variables en cada componente, ayuda a interpretar qué tipo de
información recoge cada una de ellas.



**Extraemos las nuevas coordenadas de los individuos (puntuaciones)**

Además, podemos ver las puntuaciones, que son las coordenadas de cada
observación original (provincia) sobre los nuevos ejes
construidos (componentes principales). Esto corresponde a un cambio de
coordenadas bajo el paradigma del Álgebra Lineal.

```{r}
pca$x
```

Dando una interpretación a cada eje,
podremos determinar qué perfil tiene cada provincia dentro del estudio.

**Varianza explicada por cada componente principal**

Una vez calculadas las componentes principales, es de interés conocer la
varianza explicada por cada una, ya que el principal objetivo que se
sigue con PCA es maximizar la cantidad de información explicada por las
componentes.

```{r}
summary(pca)
```

-   **Standard deviation**: muestra las desviaciones estándar de cada
    componente principal. Si elevamos al cuadrado estas desviaciones,
    tenemos la varianza (**el autovalor correspondiente**). Es decir, la
    varianza explicada por cada componente corresponde con los
    autovalores de la matriz de covarianzas de los datos estandarizados.

-   **Proportion of Variance**: es la proporción de la varianza total
    que explica cada componente principal y quizá, es la fila más
    importante de nuestros resultados. Como los autovalores están
    ordenados de mayor a menor y así son construidas las componentes
    principales, la primera componente principal es la que mayor
    porcentaje de variabilidad explica, un 56%. Así, la varianza
    explicada por la componentes van en orden decreciente, teniendo que
    la segunda componente explica un 16% y la tercera, un 7%.

-   **Cumulative proportion**: es la varianza acumulada y se calcula
    progresivamente sumando la Proportion of Variance anterior. En vista
    de estos resultados, vemos que la primera componente agrupa el 56%
    de la variación, y que necesitamos 3 componentes para alcanzar el
    81%.

Si elevamos al cuadrado estas desviaciones, tenemos la varianza (el
**autovalor correspondiente**). Es decir, la varianza explicada por cada
componente corresponde con los autovalores de la matriz de covarianzas
de los datos estandarizados.

```{r}
# Autovalues
pca$sdev^2 # varianza de cada componente
```

## Selección de componentes

Graficando el valor de la varianza de cada componente principal, podemos
observar los resultados comentados anteriormente, que las primeras
componentes son las que más varianza explican y que a medida que se
añaden más, la varianza explicada por cada una es menor.

```{r}
fviz_eig(pca, main = "Varianza de cada componente", choice = "eigenvalue", addlabels = T)
```

o como el porcentaje de varianza explicada por cada componente sobre el
total.

```{r}
fviz_screeplot(pca, addlabels = TRUE, main = "Porcentaje de varianza explicada por cada componente (%)")
```

A continuación, representamos las varianzas acumuladas:

```{r}
plot(summary(pca)$importance[3, ], type = "o", col = "darkblue", lwd = 3, main = "Porcentaje de varianza acumulada", xlab = "Componente Principal", ylab = "Porcentaje de varianza acumulada")
```

Determinar el número de componentes que elegir para continuar con el
análisis no tiene unas normas determinadas a seguir. Respecto a ello,
existen varios criterios con sus respectivas propuestas.

1.  Una opción para determinar el número de componentes principales que
    seleccionar, es coger aquellas tal que expliquemos un %
    **determinado de la variabilidad de los datos que nosotros prefijemos**. Generalmente se pone como umbral mínimo un 80%,
    entonces necesitaríamos elegir 3 componentes.

2.  Otra posibilidad es seguir el **criterio de Kaisser**, que escoge
    aquellas componentes cuyo autovalor sea superior a 1 (cuando las
    variables han sido generadas a partir de la matriz de
    correlaciones). Según este criterio y mirando el gráfico anterior de
    la varianza (igual a eigenvalues), elegiríamos las 4 primeras
    componentes.

3.  Para relajar el criterio de Kaisser, existe la **modificación de Jollife**, que elige aquellas componentes cuyo autovalor sea
    superior a 0.7. Esta modificación, nos permite elegir 6 componentes.

En este caso, nos podríamos quedar con las 3 primeras componentes
principales, ya que un 80% de variabilidad es bastante ilustrativo para este ejemplo que queremos hacer. Por tanto, en lugar de trabajar con las 22 variables
originales, trabajaremos con 3 variables nuevas, que son combinaciones
de ellas.

## Interpretación

Hemos decidido quedarnos con 3 componentes principales, que explican el
81% de la variabilidad total. Para realizar su interpretación, volvemos
a ver los coeficientes de las ecuaciones de los componentes, observando
cuáles son los valores más altos (en valor absoluto), para así poder dar
una interpretación a cada eje.

```{r}
pca$rotation[, 1:3]

corr_var <- pca$rotation %*% diag(pca$sdev)
colnames(corr_var) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10", "PC11", "PC12", "PC13", "PC14", "PC15", "PC16", "PC17", "PC18", "PC19", "PC20", "PC21", "PC22")
corrplot(corr_var)
```

Si nos fijamos en los pesos más altos, podemos darle una interpretación
a cada eje. Por ejemplo:

-   La **primera componente** explica un 56.9% de la variación. Hay
    valores absolutos bastante similares y elevados (en rojo), que son los
    relativos a las variables socio-demográficas que hemos comentado antes, y son los que esencialmente está capturando la Componente 1.
Teniendo en cuenta los signos podemos concluir que todas ellas influyen negativamente en la componente 1 ya que la correlación es negativa.

-   La **segunda componente** explica un 16.9% de variación adicional, los pesos más elevados corresponden con las variables *pibpc* e *ibipc* y puesto que tienen signo positivo, estás están correlacionadas positivamente con la Componente 2. Vemos que esta componente describe principalmente esas dos variables, que hablan del comportamiento económico del país.



-   La **tercera componente** explica un 7.8% de variación adicional, los pesos más elevados corresponden con las variables *sagt* e *sagu* (correlacionada positivamente) y que describe principalmente las características agrícolas del país.


## Representación gráfica

**Gráfico de las variables**

Representamos sobre las dos primeras componentes principales las
variables originales. En el eje de abscisas se representa la PC1 y en el
eje de ordenadas, la PC2. Para interpretar correctamente las variables
tenemos que fijarnos en la longitud de la flecha y en el ángulo que
forman respecto a los ejes y entre ellos mismos.

-   **Ángulo vector - eje**: cuanto más paralelo es un vector al eje,
    más ha contribuido a dicha componente principal.

-   **Ángulo entre dos vectores**: si es pequeño representa una alta
    correlación entre las variables implicadas (y por tanto,
    observaciones con valores altos en una variable, tendrá valores
    altos en la otra). Si el ángulo es cercano a 90º indica que las
    variables están incorreladas y los ángulos opuestos indican
    correlación negativa entre ellas.

-   **Longitud**: cuanto mayor es la longitud de un vector, mayor
    varianza de la variable está contenida en el biplot, es decir, mejor
    representada está en el gráfico.

En el gráfico, diferenciamos por colores las variables según su calidad
de representación en las dos primeras componentes. Cuanto más cerca esté
una variable del círculo de correlaciones, mejor será su representación,
por lo que las variables que estén muy cerca del centro de la gráfica
son las menos importantes para las dos primeras componentes.

```{r}
fviz_pca_var(pca, axes = c(1, 2), col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

fviz_pca_var(pca, axes = c(1, 3), col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
```

  - Si nos fijamos en el eje de abscisas, todas variables que hemos visto correlacionadas con la componente 1 de manera alta se situan con menor ángulo respecto a él, indicando que han contribuido a la formación de la PC1. Además hay muy poco ángulo entre ellas puesto que se encuentran altamente correlacionadas entre sí. 
  
  
  - En cuanto al eje de ordenadas, vemos que las variables que forman un menor ángulo respecto a él, son las que más contribuían a la formación de la PC2, tanto de forma positiva como negativa.
  


**RESUMEN DE RESULTADOS**

Los gráficos obtenidos muestran una visualización de las variables en
función de las componentes principales 1 y 2, y 1 y 3. Las variables que
tienen una correlación alta con la primera componente principal están
más cerca del eje horizontal y las variables que tienen una alta
correlación con la segunda componente principal están más cerca del eje
vertical. Las variables que se encuentran cerca del centro del gráfico
tienen una correlación baja con ambas componentes principales, aún así
nos guiaremos con la tabla para decidir qué variables están mejor
explicadas por cada una de las componentes. En resumen, estos gráficos
proporcionan una representación visual de las relaciones entre las
variables en función de las dos primeras componentes principales y las
dos segundas, lo que puede ayudar a identificar patrones y tendencias en
los datos.

En el siguiente gráfico podemos ver las correlaciones de dichas
variables con las componentes principales, como ya hemos comentado.

```{r}
corr_var <- pca$rotation %*% diag(pca$sdev)
colnames(corr_var) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10", "PC11", "PC12", "PC13", "PC14", "PC15", "PC16", "PC17", "PC18", "PC19", "PC20", "PC21", "PC22")
corrplot(corr_var)
```

En cuanto a este gráfico, es llamativo como las dos o tres primeras
componentes son las más importantes en el PCA, sobre todo la PC1 que es la que mas variables resume.

En resumen, las nuevas componentes han permitido identificar patrones y
características de las provincias en términos de la situación
sanitaria y indicadores relativos a esta.

**Gráfico de los individuos**

Tras observar la representación de las variables, en este apartado vemos la representación de los individuos sobre los nuevos ejes, con la idea de que aquellos con características similares, se agrupan cerca al tener puntuaciones parecidas. Las provincias con valores cercanos a la media se situarán cerca del centro del gráfico (0,0).

Vemos que los más lejanos son Madrid y Barcelona en el gráfico de las primeras dos componentes.

```{r}
# Sobre PC1 y PC2
fviz_pca_ind(pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE, axes = c(1, 2))
```

```{r}
fviz_pca_ind(pca, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE, axes = c(2, 3))
```


Las provincias que se encuentren cerca indican similitud para las componentes que se están representando en el gráfico.






# Conclusión

El objetivo de este estudio era, partiendo de un conjunto de datos demográfico-económicos sobre la población en provincias en España, extraer en qué situación se encuentra cada provincia y así identificar cuáles son las provincias con un perfil más parecido entre sí.

Tras haber aplicado la técnica de PCA, hemos concluido que:

  -   La **primera componente** resume para cada provincia los datos
    relativos a las variables socio-demográficas que hemos comentado antes.
Teniendo en cuenta los signos podemos concluir que todas ellas influyen negativamente en la componente 1 ya que la correlación es negativa.

-   La **segunda componente** resume para cada provincia los datos que corresponden con las variables *pibpc* e *ibipc* , que hablan del comportamiento económico del país.



-   La **tercera componente**resume para cada provincia los datos que corresponden con las variables *sagt* e *sagu* (correlacionada positivamente) y que describe principalmente las características agrícolas del país.








# Bibliografía

  - <https://rpubs.com/Joaquin_AR/287787>
  - <http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/>
  - <https://www.datacamp.com/es/tutorial/pca-analysis-r>
